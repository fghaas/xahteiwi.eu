<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>xahteiwi.eu</title><link href="fghaas.github.io/xahteiwi.eu/" rel="alternate"></link><link href="fghaas.github.io/xahteiwi.eu/feeds/all.atom.xml" rel="self"></link><id>fghaas.github.io/xahteiwi.eu/</id><updated>2018-12-08T00:00:00+00:00</updated><entry><title>1,000 routers per tenant? Think again!</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/1000-routers-per-tenant-think-again/index.html" rel="alternate"></link><published>2018-12-08T00:00:00+00:00</published><updated>2018-12-08T00:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2018-12-08:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/1000-routers-per-tenant-think-again/index.html</id><summary type="html">&lt;p&gt;When you allow one of your OpenStack tenants a large number of routers, they may not be getting as many as you think they will.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Neutron quotas&lt;/h2&gt;
&lt;p&gt;As with all other OpenStack services, Neutron uses a fairly extensive
quota system. As an OpenStack admin, you can give a tenant — I’m very
sorry, I still can’t force myself to call it a “project”, as I find
that term profoundly illogical — a quota limit on networks, routers,
port, subnets, IPv6 subnetpools, and many other object types. &lt;/p&gt;
&lt;p&gt;Most OpenStack deployments set the default per-tenant quota at 10
routers. However, nothing stops an admin from setting a much higher
quota, including one above 255. When you do that, you’re in for a
surprise.&lt;/p&gt;
&lt;h2&gt;HA routers&lt;/h2&gt;
&lt;p&gt;Way back in the OpenStack Juno release, we got high-availability
support for Neutron routers. This means that, assuming you have more
than one network gateway node that can host them, your virtual routers
will work in an automated active/backup configuration. &lt;/p&gt;
&lt;p&gt;In effect, what Neutron does for you is that for every subnet that is
plugged into the router — and for which it therefore acts as the
default gateway — the gateway address binds to a keepalived-backed
VRRP interface. On one of the network nodes that interface is active,
and on the others it’s in standby. If your network node goes down,
keepalived makes sure that the subnets’ default gateway IPs come up on
the other node. The keepalived configuration is completely abstracted
away from the user; the Neutron L3 agent happily takes care of all of
it.&lt;/p&gt;
&lt;p&gt;In addition, in case a network node is up but has lost upstream
network connectivity itself, whereas another is still available that
retains it, HA routers also fail over in order to ensure connectivity
for your VMs.&lt;/p&gt;
&lt;h2&gt;The catch: one HA-router network per tenant&lt;/h2&gt;
&lt;p&gt;In order to enable HA routers, Neutron creates &lt;em&gt;one&lt;/em&gt; administrative
network per tenant, over which it runs VRRP traffic. In order to tell
apart all the keepalived instances that it manages on that network, it
assigns each an individual Virtual Router ID or VRID.&lt;/p&gt;
&lt;p&gt;And here’s the problem: &lt;a href="https://tools.ietf.org/html/rfc5798"&gt;RFC
5798&lt;/a&gt; defines the VRID to be an
8-bit integer.&lt;/p&gt;
&lt;p&gt;That means that if you use HA routers, then setting a router quota
over 255 is useless — Neutron will run out of VRIDs in the
administrative network, before your tenant can ever hit the quota.&lt;/p&gt;
&lt;p&gt;And this is a hard limit; there’s really not much that Neutron can do
about this. If you want more than 255 &lt;em&gt;highly-available&lt;/em&gt; routers,
you’ll have to spread them across multiple tenants.&lt;/p&gt;
&lt;h2&gt;Wait, what if I really don’t &lt;em&gt;need&lt;/em&gt; HA routers?&lt;/h2&gt;
&lt;p&gt;Well, yes you probably do, really. But that aside, let’s assume for a
moment that you actually don’t. Or rather, that it’s more important
for you to have more than 255 routers in a single tenant, than for any
of them to be highly available. So you create routers with the &lt;code&gt;ha&lt;/code&gt;
flag set to &lt;code&gt;False&lt;/code&gt;, simple, right?&lt;/p&gt;
&lt;p&gt;It turns out that you probably won’t be able to do that. That’s
because the default Neutron policy restricts setting the &lt;code&gt;ha&lt;/code&gt; flag on
a router to admins only.&lt;/p&gt;
&lt;p&gt;So &lt;em&gt;if&lt;/em&gt; you want to be able to disable a router’s HA capability,
you’ll first need to convince your cloud service provider to override
the following default entries in Neutron’s &lt;code&gt;policy.json&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;"create_router:ha"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"rule:admin_only"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nt"&gt;"get_router:ha"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"rule:admin_only"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nt"&gt;"update_router:ha"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"rule:admin_only"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;... and instead set them as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;"create_router:ha"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"rule:admin_or_owner"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nt"&gt;"get_router:ha"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"rule:admin_or_owner"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nt"&gt;"update_router:ha"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"rule:admin_or_owner"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If your cloud service provider deploys Neutron with
&lt;a href="https://docs.openstack.org/openstack-ansible/latest/"&gt;OpenStack-Ansible&lt;/a&gt;,
they can define this in the &lt;a href="https://docs.openstack.org/openstack-ansible-os_neutron/latest/"&gt;following
variable&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;neutron_policy_overrides&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
    &lt;span class="s"&gt;"create_router:ha"&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"rule:admin_or_owner"&lt;/span&gt;
    &lt;span class="s"&gt;"get_router:ha"&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"rule:admin_or_owner"&lt;/span&gt;
    &lt;span class="s"&gt;"update_router:ha"&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"rule:admin_or_owner"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Subsequently, you should be able to create a new router with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;openstack router create --no-ha &amp;lt;name&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And modify an existing router’s high-availability flag with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;openstack router &lt;span class="nb"&gt;set&lt;/span&gt; --disable &amp;lt;name&amp;gt;
openstack router &lt;span class="nb"&gt;set&lt;/span&gt; --no-ha &amp;lt;name&amp;gt;
openstack router &lt;span class="nb"&gt;set&lt;/span&gt; --enable &amp;lt;name&amp;gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="OpenStack"></category></entry><entry><title>Working from home, with little kids in the house</title><link href="fghaas.github.io/xahteiwi.eu/blog/2018/02/18/working-from-home-with-little-kids/index.html" rel="alternate"></link><published>2018-02-18T00:00:00+00:00</published><updated>2018-02-18T00:00:00+00:00</updated><author><name>florian</name></author><id>tag:None,2018-02-18:fghaas.github.io/xahteiwi.eu/blog/2018/02/18/working-from-home-with-little-kids/index.html</id><summary type="html">&lt;p&gt;I very much prefer working from home to working from an office. But with little kids in the house, that’s a nontrivial endeavor.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I’ve worked exclusively from home&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt; for the last 6½ years, and
when I first started, my older kids were 7 and 6 years old. For the
last 3 years after the arrival of our two younger children, though,
I’ve worked from home &lt;strong&gt;with infants/toddlers in the house,&lt;/strong&gt; and
that’s a wholly different ball game.&lt;/p&gt;
&lt;h2&gt;The basic rules of working from home&lt;/h2&gt;
&lt;p&gt;If you’ve dealt with the idea of working from home, or have
done it before, then you’re surely familiar with a few basic rules to
follow:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Make sure that you have a &lt;em&gt;home office,&lt;/em&gt; that is a working space to
  yourself that allows you to close a door and be undisturbed when you
  need it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set aside some time to go outside. Office workers have a commute
  that ensures they get out of the house; as a home worker you’re
  running a certain risk of turning into a hermit. Make sure you run
  errands, take walks, go for runs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Keep tabs on your working hours. Home workers are frequently at risk
  of working too much or too long, which puts a strain on yourself,
  your significant other, and your family.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Maintain relationships with others who work from home. Chat with
  them, call them, invite them over for dinner if they live close
  by. Working from home is still not a common thing to do in general
  (however common it might be in the tech industry), so exchanging
  thoughts with people in the same boat is a good thing.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Be ready to try out working-from-home strategies that have worked
  for others. Fortunately for all of us, there are quite a few people
  who talk about the experience publicly — for example,
  &lt;a href="https://twitter.com/johndalton"&gt;John Dalton&lt;/a&gt; did so in a
  &lt;a href="https://youtu.be/qFWkDPTjjEM"&gt;45-minute talk&lt;/a&gt; at linux.conf.au
  2018.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Now, the challenging part: &lt;em&gt;little&lt;/em&gt; kids&lt;/h2&gt;
&lt;p&gt;If anyone reading this wants to pre-emptively freak out and berate me
for being an irresponsible or unloving parent, you are politely asked
to chill and if incapable of doing so, to GTFO. I absolutely love
being around my children, and I wouldn’t even &lt;em&gt;think&lt;/em&gt; to trade my job
for an office-dwelling one. Being able to work from home is generally
a wonderful privilege, and so is being around my kids.&lt;/p&gt;
&lt;p&gt;You need to be aware of two things: one, working from home with a
family is different from working from home when you’re alone or living
with an adult partner or roomie. Two, working from home with a family
with little children is &lt;em&gt;very&lt;/em&gt; different from a family with school-age
kids. School-age children don’t permanently need an adult in the room,
they spend a significant amount of their time outside the house,
they’re usually not particularly prone to tantrums, and in the event
that you really, really need some quiet, they can be asked to give you
that for a limited amount of time. Kids under two?  Good luck with
that.&lt;/p&gt;
&lt;p&gt;If you’re not striking a balance here, here’s what’s likely to
happen: you’ll think that you are a completely inadequate parent, or a
terrible employee, or both. So, never lose sight of the fact that
you’re probably a wonderful parent and a highly productive employee —
but that you can do &lt;em&gt;anything,&lt;/em&gt; but not &lt;em&gt;everything.&lt;/em&gt;&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;My most important piece of advice here is that you &lt;strong&gt;should not attempt
to &lt;em&gt;simultaneously&lt;/em&gt; be a primary parent and an employee.&lt;/strong&gt; (Please
note the emphasis on “simultaneously.”)&lt;/p&gt;
&lt;p&gt;I think it’s fundamentally impossible to be a mindful parent of your
kids, and &lt;em&gt;at the same time&lt;/em&gt; a productive knowledge worker. And the
repeated attempt will lead to frustration and burnout. Of course, you
can split your day in half with your significant other, if for example
you happen to be a night owl and they are not, and you can put in a
full day of work between 1pm and 9pm (but do ensure you get enough
sleep, which is probably a topic for another post). Or you could rise
early and start working at 6 while your other half drops your kids off
at daycare, from whence you pick them up at 2. But whatever it is that
you decide, please don’t fool yourself into thinking that you can work
productively, and be a good parent for your kids at the very same
time.&lt;/p&gt;
&lt;h2&gt;Productivity tips during work hours&lt;/h2&gt;
&lt;p&gt;So, we’ve established that work time and parenting time shouldn’t
overlap. So what can you do while you’re at work, and your kids are in
the house (under the supervision of your spouse or partner, or other
trusted guardian)? Spoiler: “please, kids, be quiet for a bit” isn’t
going to work. Not with toddlers. And sound-proofing your room is
quite expensive, and also remarkably ineffective for the deep bass
&lt;em&gt;whooomp&lt;/em&gt; of an exuberant kid jumping off a couch upstairs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;I find myself being able to focus extremely well if I &lt;strong&gt;play
&lt;a href="https://en.wikipedia.org/wiki/Pink_noise"&gt;pink noise&lt;/a&gt; through my
headphones.&lt;/strong&gt;&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="#fn:3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt; It drowns out practically all background noise, and
is uniform enough that my brain eventually drops it as an input, and I
barely notice it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I also habitually &lt;strong&gt;close my door&lt;/strong&gt; when I need to work. Obviously, my
family knows that they can call on me when something urgent comes up,
but little kids rightfully take an open door as an invitation. And as
much as I would like to be able to tolerate interruptions of my
thinking flow to play with a little one for a few minutes, and then
immediately pick up where I left off,
&lt;a href="http://heeris.id.au/2013/this-is-why-you-shouldnt-interrupt-a-programmer/"&gt;I am not.&lt;/a&gt;&lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="#fn:4" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;
So, this is a classic trade-off: I can either enjoy the interaction
with my kids and then deal with the frustration that comes with not
getting things done, or I can get things done and regret that I didn’t
spend more time with my kids. There’s no right answer. The wrong
answer that I choose is that my door stays closed.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Striking a balance (or trying to)&lt;/h2&gt;
&lt;p&gt;There are a few things I do to try and balance my in-house family
absence (I’m dead serious, that’s what working from home is):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;When I’m home, &lt;strong&gt;I don’t miss lunch.&lt;/strong&gt;&lt;sup id="fnref:5"&gt;&lt;a class="footnote-ref" href="#fn:5" rel="footnote"&gt;5&lt;/a&gt;&lt;/sup&gt; I can’t say that I’ve &lt;em&gt;never&lt;/em&gt;
  missed lunch because I have done so when a customer’s or colleague’s
  environment had just reached the
  &lt;a href="https://en.wikipedia.org/wiki/Halt_and_Catch_Fire"&gt;HCF instruction&lt;/a&gt;,
  but I won’t miss lunch for a meeting that can also be scheduled to
  an hour earlier or later.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Also, &lt;strong&gt;tucking the little ones in&lt;/strong&gt; is my job every night.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;I don’t travel when it would mean being absent for a child’s
  birthday.&lt;/strong&gt; Which is actually not that easy to do when you’re on the
  conference circuit, travel about 100 days per year, and you have a
  big family — thus far, if my math is right I’ve successfully
  navigated 28 kids’ birthdays.&lt;sup id="fnref:6"&gt;&lt;a class="footnote-ref" href="#fn:6" rel="footnote"&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Am I succeeding?&lt;/h2&gt;
&lt;p&gt;Let’s be real: &lt;strong&gt;I have no way of knowing.&lt;/strong&gt; Whether I’m being a
decent parent will ultimately be judged by my children when they’ve
grown up. Whether I’ve had a successful career is something I’ll
decide upon my retirement.&lt;/p&gt;
&lt;p&gt;But hey, such is life. And thus far it looks like I don’t
suck at it. I hope. 🙂 &lt;/p&gt;
&lt;hr/&gt;
&lt;h3&gt;Footnotes&lt;/h3&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Well, not &lt;em&gt;just&lt;/em&gt; from home. Throughout the last few years I’ve
traveled rather extensively, so I’ve also worked from planes,
airport lounges, trains, hotel rooms, customer offices, cafes and
parks. What I mean is that I haven’t had to go to an office on a
daily basis, since 2011. &lt;a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;Hat tip to
&lt;a href="https://en.wikipedia.org/wiki/David_Allen_(author)"&gt;David Allen&lt;/a&gt;,
whose &lt;a href="http://a.co/iZhglcP"&gt;Getting Things Done&lt;/a&gt; is not the
ultimate fount of all self-management wisdom, but definitely a
good source to have read at least once. &lt;a class="footnote-backref" href="#fnref:2" rev="footnote" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;On a Linux box with &lt;a href="https://en.wikipedia.org/wiki/SoX"&gt;SoX&lt;/a&gt;,
you can generate an Ogg Vorbis file containing 25 minutes of pink
noise with
&lt;code&gt;sox -n pinknoise.ogg synth 25:00 pinknoise&lt;/code&gt;
— the 25-minute length may come in handy if you’re using
&lt;a href="https://en.wikipedia.org/wiki/Pomodoro_Technique"&gt;Pomodoro&lt;/a&gt;. As
with music, I play my pink noise from
&lt;a href="https://audacious-media-player.org/"&gt;Audacious&lt;/a&gt; with the
&lt;a href="https://specifications.freedesktop.org/mpris-spec/latest/"&gt;MPRIS&lt;/a&gt;-2
plugin enabled, which allows me to pause and play from
&lt;a href="https://wiki.gnome.org/Projects/GnomeShell"&gt;gnome-shell&lt;/a&gt; with the
&lt;a href="https://extensions.gnome.org/extension/55/media-player-indicator/"&gt;Media Player Indicator&lt;/a&gt;
extension. &lt;a class="footnote-backref" href="#fnref:3" rev="footnote" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;I do not consider myself a programmer. But the issue described
in the cartoon applies to any knowledge worker. In fact, it
probably applies to &lt;em&gt;any&lt;/em&gt; worker, it’s just that I’ve been doing
knowledge work for the entirety of my career, and can’t comment
authoritatively on anything else. &lt;a class="footnote-backref" href="#fnref:4" rev="footnote" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:5"&gt;
&lt;p&gt;For those of you who grew up outside central Europe: over here,
lunch (not dinner) is traditionally considered the “big” meal of
the day. &lt;a class="footnote-backref" href="#fnref:5" rev="footnote" title="Jump back to footnote 5 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:6"&gt;
&lt;p&gt;I also took uninterrupted travel breaks of 6 and 10 weeks,
respectively, when our younger kids were born. (When the older
ones arrived, I was traveling way less overall, so those travel
schedule adjustments were easy at the time.) &lt;a class="footnote-backref" href="#fnref:6" rev="footnote" title="Jump back to footnote 6 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Philosophy"></category></entry><entry><title>More recommendations for Ceph and OpenStack</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/more-recommendations-ceph-openstack/index.html" rel="alternate"></link><published>2017-08-03T00:00:00+00:00</published><updated>2017-08-03T00:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2017-08-03:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/more-recommendations-ceph-openstack/index.html</id><summary type="html">&lt;p&gt;Our series on best practices for Ceph and OpenStack continues.&lt;/p&gt;</summary><content type="html">&lt;p&gt;A few months ago, we
&lt;a href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/dos-donts-ceph-openstack/index.html"&gt;shared our Dos and Don'ts&lt;/a&gt;,
as they relate to Ceph and OpenStack. Since that post has proved quite
popular, here are a few additional considerations for your Ceph-backed
OpenStack cluster.&lt;/p&gt;
&lt;h2&gt;Do configure your images for VirtIO-SCSI&lt;/h2&gt;
&lt;p&gt;By default, RBD-backed Nova instances use the &lt;code&gt;virtio-blk&lt;/code&gt; driver to
expose RBD images to the guest -- either as ephemeral drives, or as
persistent volumes. In this default configuration, VirtIO presents a
virtual PCI device to the guest that represents the paravirtual I/O
bus, and devices are named &lt;code&gt;/dev/vda&lt;/code&gt;, &lt;code&gt;/dev/vdb&lt;/code&gt;, and so
forth. VirtIO block devices are lightweight and efficient, but they
come with a drawback: they don't support the &lt;code&gt;discard&lt;/code&gt; operation.&lt;/p&gt;
&lt;p&gt;Not being able to use &lt;code&gt;discard&lt;/code&gt; means the guest cannot mount a
filesystem with &lt;code&gt;mount -o discard&lt;/code&gt;, and it also cannot clean up freed
blocks on a filesystem with &lt;code&gt;fstrim&lt;/code&gt;. This can be a security concern
for your users, who might want to be able to really, actually &lt;em&gt;delete&lt;/em&gt;
data from within the guest (after overwriting it, presumably). It can
also be an operational concern for you as the cluster operator.&lt;/p&gt;
&lt;p&gt;This is because not supporting &lt;code&gt;discard&lt;/code&gt; also means that RADOS objects
owned by the corresponding RBD image and never &lt;em&gt;removed&lt;/em&gt; during the
image's lifetime -- they persist until the whole image is deleted. So
your cluster may carry the overhead of perhaps tens of thousands of
RADOS objects that no-one actually cares about.&lt;/p&gt;
&lt;p&gt;Thankfully, there is an alternative VirtIO disk driver that &lt;em&gt;does&lt;/em&gt;
support &lt;code&gt;discard&lt;/code&gt;: the paravirtualized VirtIO SCSI controller,
&lt;code&gt;virtio-scsi&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Enabling the VirtIO SCSI controller is something you do by setting a
couple of Glance &lt;strong&gt;image properties,&lt;/strong&gt; namely &lt;code&gt;hw_scsi_model&lt;/code&gt; and
&lt;code&gt;hw_disk_bus&lt;/code&gt;. You do so by running the following &lt;code&gt;openstack&lt;/code&gt; commands
on your image:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;openstack image &lt;span class="nb"&gt;set&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
  --property &lt;span class="nv"&gt;hw_scsi_model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;virtio-scsi &lt;span class="se"&gt;\&lt;/span&gt;
  --property &lt;span class="nv"&gt;hw_disk_bus&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;scsi &lt;span class="se"&gt;\&lt;/span&gt;
  &amp;lt;name or ID of your image&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then, if you boot an instance from this image, you'll see that its
block device names switch from &lt;code&gt;/dev/vdX&lt;/code&gt; to &lt;code&gt;/dev/sdX&lt;/code&gt;, and you also
get everything else you expect from a SCSI stack. For example, there's
&lt;code&gt;/proc/scsi/scsi&lt;/code&gt;, you can extract information about your bus,
controller, and LUs with &lt;code&gt;lsscsi&lt;/code&gt; command, and so on.&lt;/p&gt;
&lt;p&gt;It's important to note that this &lt;em&gt;image&lt;/em&gt; property is inherited by the
&lt;em&gt;instance&lt;/em&gt; booted from that image, which also passes it on to all
&lt;em&gt;volumes&lt;/em&gt; that you may subsequently attach to that instance. Thus,
&lt;code&gt;openstack server add volume&lt;/code&gt; will now add &lt;code&gt;/dev/sdb&lt;/code&gt;, not &lt;code&gt;/dev/vdb&lt;/code&gt;,
and you will automatically get the benefits of &lt;code&gt;discard&lt;/code&gt; on your
volumes, as well.&lt;/p&gt;
&lt;h2&gt;Do set disk I/O limits on your Nova flavors&lt;/h2&gt;
&lt;p&gt;In a Ceph cluster that acts as backing storage for OpenStack,
naturally many OpenStack VMs share the bandwidth and IOPS of your
whole cluster. When that happens, occasionally you may have a VM
that’s very busy (meaning it produces a lot of I/O), which the Ceph
cluster will attempt to process to the best of its abilities. In doing
so, since RBD has no built-in QoS guarantees
(&lt;a href="http://tracker.ceph.com/projects/ceph/wiki/Add_QoS_capacity_to_librbd"&gt;yet&lt;/a&gt;),
it might cause &lt;em&gt;other&lt;/em&gt; VMs to suffer from reduced throughput,
increased latency, or both.&lt;/p&gt;
&lt;p&gt;The trouble with this is that it’s almost impossible for your users to
calculate and reckon with. They’ll see a VM that sustains, say, 10,000
IOPS at times, and then drop to 2,000 with no warning or
explanation. It is much smarter to pre-emptively &lt;em&gt;limit&lt;/em&gt; Ceph RBD
performance from the hypervisor, and luckily, OpenStack Nova
absolutely allows you to do that. This concept is known as &lt;strong&gt;instance
resource quotas&lt;/strong&gt;, and you set them via flavor properties. For
example, an you may want to limit a specific flavor to 1,500 IOPS and
a maximum throughput of 100 MB/s:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;openstack flavor &lt;span class="nb"&gt;set&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
  --property quota:disk_total_bytes_sec&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$((&lt;/span&gt;&lt;span class="m"&gt;100&lt;/span&gt;&amp;lt;&amp;lt;&lt;span class="m"&gt;20&lt;/span&gt;&lt;span class="k"&gt;))&lt;/span&gt;
  --property quota:disk_total_iops_sec&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1500&lt;/span&gt;
  m1.medium
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In the background, these settings are handed through to libvirt and
ultimately fed into cgroup limitations for Qemu/KVM, when a VM with
this flavor spins up. So these limits aren’t specific to RBD, but they
come in particularly handy when dealing with RBD.&lt;/p&gt;
&lt;p&gt;Obviously, since flavors can be public, but can also be limited to
specific tenants, you can set relatively low instance resource quotas
in public flavors, and then make flavors with higher resource quotas
available to select tenants only.&lt;/p&gt;
&lt;h2&gt;Do differentiate Cinder volume types by disk I/O limits&lt;/h2&gt;
&lt;p&gt;In addition to setting I/O limits on flavors for VMs, you can also
influence the I/O characteristics of volumes. You do so by specifying
distinct Cinder volume &lt;em&gt;types&lt;/em&gt;. Volume types are frequently used to
enable users to select a specific Cinder backend — say, to stick
volumes either on a NetApp box or on RBD, but it’s perfectly OK if you
define multiple volume types using the same backend. You can then set
characteristics like maximum IOPS or maximum throughput via Cinder QoS
specifications. A QoS specification akin to the Nova flavor decribed
above — limiting throughput to 100 MB/s and 1,500 IOPS would be
created like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;openstack volume qos create &lt;span class="se"&gt;\&lt;/span&gt;
  --consumer front-end
  --property &lt;span class="nv"&gt;total_bytes_sec&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$((&lt;/span&gt;&lt;span class="m"&gt;100&lt;/span&gt;&amp;lt;&amp;lt;&lt;span class="m"&gt;20&lt;/span&gt;&lt;span class="k"&gt;))&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
  --property &lt;span class="nv"&gt;total_iops_sec&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1500&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
  &lt;span class="s2"&gt;"100MB/s-1500iops"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You would then create a corresponding volume type, and associate the
QoS spec with it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;openstack volume &lt;span class="nb"&gt;type&lt;/span&gt; create &lt;span class="se"&gt;\&lt;/span&gt;
  --public &lt;span class="se"&gt;\&lt;/span&gt;
  &lt;span class="s2"&gt;"100MB/s-1500iops"&lt;/span&gt;
openstack volume qos associate &lt;span class="se"&gt;\&lt;/span&gt;
  &lt;span class="s2"&gt;"100MB/s-1500iops"&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
  &lt;span class="s2"&gt;"100MB/s-1500iops"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Again, as with Nova flavors, you can make volume types public, but you
can also limit them to specific tenants.&lt;/p&gt;
&lt;h2&gt;Don't forget about suspend files&lt;/h2&gt;
&lt;p&gt;When you &lt;strong&gt;suspend&lt;/strong&gt; a Nova/libvirt/KVM instance, what really happens
is what libvirt calls a &lt;strong&gt;managed save&lt;/strong&gt;: the instance's entire memory
is written to a file, and then KVM process shuts down. This is
actually quite neat because it means that the VM does not consume any
CPU cycles nor memory until it restarts, and it will continue right
where it left off, even if the compute node is rebooted in the
interim.&lt;/p&gt;
&lt;p&gt;You should understand that these savefiles are not compressed in any
way: if your instance has 16GB of RAM, that's a 16GB file that
instance suspension drops into &lt;code&gt;/var/lib/nova/save&lt;/code&gt;. This can add up
pretty quickly: if a single compute node hosts something like 10
suspended instances, their combined save file size can easily exceed 
100 GB. Obviously, this can put you in a really bad spot if this fills
up your &lt;code&gt;/var&lt;/code&gt; (or worse, &lt;code&gt;/&lt;/code&gt;) filesystem.&lt;/p&gt;
&lt;p&gt;Of course, if you already have a Ceph cluster, you can put it to good
use here too: just deep-mount a CephFS file system into that
spot. Here's an Ansible playbook snippet that you may use as
inspiration:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nn"&gt;---&lt;/span&gt;
&lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;hosts&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
  &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;compute-nodes&lt;/span&gt;

  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;vars&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
    &lt;span class="l l-Scalar l-Scalar-Plain"&gt;ceph_mons&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
      &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;ceph-mon01&lt;/span&gt;
      &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;ceph-mon02&lt;/span&gt;
      &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;ceph-mon03&lt;/span&gt;
    &lt;span class="l l-Scalar l-Scalar-Plain"&gt;cephfs_client&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;cephfs&lt;/span&gt;
    &lt;span class="l l-Scalar l-Scalar-Plain"&gt;cephfs_secret&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"{{&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;vaulted_cephfs_secret&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;}}"&lt;/span&gt;

  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;tasks&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;

  &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;name&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"install&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ceph-fs-common&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;package"&lt;/span&gt;
    &lt;span class="l l-Scalar l-Scalar-Plain"&gt;apt&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;name&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;ceph-fs-common&lt;/span&gt;
      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;state&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;installed&lt;/span&gt;

  &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;name&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"create&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ceph&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;directory"&lt;/span&gt;
    &lt;span class="l l-Scalar l-Scalar-Plain"&gt;file&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;dest&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;/etc/ceph&lt;/span&gt;
      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;owner&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;root&lt;/span&gt;
      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;group&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;root&lt;/span&gt;
      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;mode&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;'0755'&lt;/span&gt;
      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;state&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;directory&lt;/span&gt;

  &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;name&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"create&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;cephfs&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;secretfile"&lt;/span&gt;
    &lt;span class="l l-Scalar l-Scalar-Plain"&gt;copy&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;dest&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;/etc/ceph/cephfs.secret&lt;/span&gt;
      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;owner&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;root&lt;/span&gt;
      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;group&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;root&lt;/span&gt;
      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;mode&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;'0600'&lt;/span&gt;
      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;content&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;'{{&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;cephfs_secret&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;}}'&lt;/span&gt;

  &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;name&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"mount&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;savefile&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;directory"&lt;/span&gt;
    &lt;span class="l l-Scalar l-Scalar-Plain"&gt;mount&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;fstype&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;ceph&lt;/span&gt;
      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;path&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;/var/lib/nova/save&lt;/span&gt;
      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;src&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"{{&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ceph_mons&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;|&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;join(',')&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;}}:/nova/save/{{&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ansible_hostname&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;}}"&lt;/span&gt;
      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;opts&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"name={{&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;cephfs_client&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;}},secretfile=/etc/ceph/cephfs.secret"&lt;/span&gt;
      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;state&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;mounted&lt;/span&gt;

  &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;name&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"fix&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;savefile&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;directory&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ownership"&lt;/span&gt;
    &lt;span class="l l-Scalar l-Scalar-Plain"&gt;file&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;path&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;/var/lib/nova/save&lt;/span&gt;
      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;owner&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;libvirt-qemu&lt;/span&gt;
      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;group&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;kvm&lt;/span&gt;
      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;state&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;directory&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;hr/&gt;
&lt;h2&gt;Got more?&lt;/h2&gt;
&lt;p&gt;Do you have Ceph/OpenStack hints of your own? Leave them in the
comments below and we’ll include them in the next installment.&lt;/p&gt;</content><category term="OpenStack"></category><category term="Ceph"></category></entry><entry><title>Importing an existing Ceph RBD image into Glance</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/importing-rbd-into-glance/index.html" rel="alternate"></link><published>2017-02-17T00:00:00+00:00</published><updated>2017-02-17T00:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2017-02-17:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/importing-rbd-into-glance/index.html</id><summary type="html">&lt;p&gt;As an OpenStack/Ceph operator, you may sometimes want to forgo uploading a new image using the Glance API, because the process can be inefficient and time-consuming. Here's a faster way.&lt;/p&gt;</summary><content type="html">&lt;p&gt;The normal process of uploading an image into Glance is
straightforward: you use &lt;code&gt;glance image-create&lt;/code&gt; or &lt;code&gt;openstack image
create&lt;/code&gt;, or the Horizon dashboard. Whichever process you choose, you
select a local file, which you upload into the Glance image store.&lt;/p&gt;
&lt;p&gt;This process can be unpleasantly time-consuming when your Glance
service is backed with Ceph RBD, for a practical reason. When using
the &lt;code&gt;rbd&lt;/code&gt; image store, you're expected to use &lt;code&gt;raw&lt;/code&gt; images, which have
interesting characteristics.&lt;/p&gt;
&lt;h2&gt;Raw images and sparse files&lt;/h2&gt;
&lt;p&gt;Most people will take an existing vendor cloud image, which is
typically available in the &lt;code&gt;qcow2&lt;/code&gt; format, and convert it using the
&lt;code&gt;qemu-img&lt;/code&gt; utility, like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ wget -O ubuntu-xenial.qcow2 &lt;span class="se"&gt;\&lt;/span&gt;
  https://cloud-images.ubuntu.com/xenial/current/xenial-server-cloudimg-amd64-disk1.img
$ qemu-img convert -p -f qcow2 -O raw ubuntu-xenial.qcow2 ubuntu-xenial.raw
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On face value, the result looks innocuous enough:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ qemu-img info ubuntu-xenial.qcow2 
image: ubuntu-xenial.qcow2
file format: qcow2
virtual size: &lt;span class="m"&gt;2&lt;/span&gt;.2G &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;2361393152&lt;/span&gt; bytes&lt;span class="o"&gt;)&lt;/span&gt;
disk size: 308M
cluster_size: &lt;span class="m"&gt;65536&lt;/span&gt;
Format specific information:
    compat: &lt;span class="m"&gt;0&lt;/span&gt;.10
    refcount bits: &lt;span class="m"&gt;16&lt;/span&gt;

$ qemu-img info ubuntu-xenial.raw
image: ubuntu-xenial.raw
file format: raw
virtual size: &lt;span class="m"&gt;2&lt;/span&gt;.2G &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;2361393152&lt;/span&gt; bytes&lt;span class="o"&gt;)&lt;/span&gt;
disk size: 1000M
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As you can see, in both cases the virtual image size differs starkly
from the actual file size. In &lt;code&gt;qcow2&lt;/code&gt;, this is due to the
copy-on-write nature of the file format and zlib compression; for the
&lt;code&gt;raw&lt;/code&gt; image, we're dealing with a sparse file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ls -lh ubuntu-xenial.qcow2
-rw-rw-r-- &lt;span class="m"&gt;1&lt;/span&gt; florian florian 308M Feb &lt;span class="m"&gt;17&lt;/span&gt; &lt;span class="m"&gt;10&lt;/span&gt;:05 ubuntu-xenial.qcow2
$ du -h  ubuntu-xenial.qcow2
308M    ubuntu-xenial.qcow2
$ ls -lh info ubuntu-xenial.raw
-rw-r--r-- &lt;span class="m"&gt;1&lt;/span&gt; florian florian &lt;span class="m"&gt;2&lt;/span&gt;.2G Feb &lt;span class="m"&gt;17&lt;/span&gt; &lt;span class="m"&gt;10&lt;/span&gt;:16 ubuntu-xenial.raw
$ du -h  ubuntu-xenial.raw
1000M   ubuntu-xenial.raw
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So, while the &lt;code&gt;qcow2&lt;/code&gt; file's physical and logical sizes match, the
&lt;code&gt;raw&lt;/code&gt; file looks much larger in terms of filesystem metadata, as
opposed to its actual storage utilization. That's because in a sparse
file, "holes" (essentially, sequences of null bytes) aren't actually
written to the filesystem. Instead, the filesystems just records the
position and length of each "hole", and when we read from the "holes"
in the file, the read would just return null bytes again.&lt;/p&gt;
&lt;p&gt;The trouble with sparse files is that RESTful web services, like
Glance, don't know too much about them. So, if we were to import that
raw file with &lt;code&gt;openstack image-create --file my_cloud_image.raw&lt;/code&gt;, the
command line client would upload null bytes with happy abandon, which
would greatly lengthen the process.&lt;/p&gt;
&lt;h2&gt;Importing images into RBD with &lt;code&gt;qemu-img convert&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Luckily for us, &lt;code&gt;qemu-img&lt;/code&gt; also allows us to upload &lt;em&gt;directly&lt;/em&gt; into
RBD. All you need to do is make sure the image goes into the correct
pool, and is reasonably named. Glance names uploaded images by their
image ID, which is a universally unique identifier (UUID), so let's
follow Glance's precedent.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;IMAGE_ID&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;uuidgen&lt;span class="sb"&gt;`&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;POOL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"glance-images"&lt;/span&gt;  &lt;span class="c1"&gt;# replace with your Glance pool name&lt;/span&gt;

qemu-img convert &lt;span class="se"&gt;\&lt;/span&gt;
  -f qcow2 -O raw &lt;span class="se"&gt;\&lt;/span&gt;
  my_cloud_image.raw &lt;span class="se"&gt;\&lt;/span&gt;
  rbd:&lt;span class="nv"&gt;$POOL&lt;/span&gt;/&lt;span class="nv"&gt;$IMAGE_ID&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Creating the clone baseline snapshot&lt;/h2&gt;
&lt;p&gt;Glance expects a snapshot named &lt;code&gt;snap&lt;/code&gt; to exist on any image that is
subsequently cloned by Cinder or Nova, so let's create that as
well:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;rbd snap create &lt;span class="nv"&gt;$POOL&lt;/span&gt;/&lt;span class="nv"&gt;$IMAGE_ID&lt;/span&gt;@snap
rbd snap protect &lt;span class="nv"&gt;$POOL&lt;/span&gt;/&lt;span class="nv"&gt;$IMAGE_ID&lt;/span&gt;@snap
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Making Glance aware of the image&lt;/h2&gt;
&lt;p&gt;Finally, we can let Glance know about this image. Now, there's a catch
to this: this trick &lt;em&gt;only&lt;/em&gt; works with the Glance v1 API, and thus you
&lt;em&gt;must&lt;/em&gt; use the &lt;code&gt;glance&lt;/code&gt; client to do it. Your Glance is v2 only?
Sorry. Insist on using the &lt;code&gt;openstack&lt;/code&gt; client? Out of luck.&lt;/p&gt;
&lt;p&gt;What's special about this invocation of the &lt;code&gt;glance&lt;/code&gt; client are simply
the pre-populated &lt;code&gt;location&lt;/code&gt; and &lt;code&gt;id&lt;/code&gt; fields. The &lt;code&gt;location&lt;/code&gt; is composed of the following segments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the fixed string &lt;code&gt;rbd://&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;your Ceph cluster UUID (you get this from &lt;code&gt;ceph fsid&lt;/code&gt;),&lt;/li&gt;
&lt;li&gt;a forward slash (&lt;code&gt;/&lt;/code&gt;),&lt;/li&gt;
&lt;li&gt;the name of the pool that the image is stored in,&lt;/li&gt;
&lt;li&gt;the name of your image (which you previously created with &lt;code&gt;uuidgen&lt;/code&gt;),&lt;/li&gt;
&lt;li&gt;another forward slash (&lt;code&gt;/&lt;/code&gt;, not &lt;code&gt;@&lt;/code&gt; as you might expect),&lt;/li&gt;
&lt;li&gt;and finally, the name of your snapshot (&lt;code&gt;snap&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other than that, the &lt;code&gt;glance&lt;/code&gt; client invocation is pretty
straightforward for a v1 API call:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;CLUSTER_ID&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;ceph fsid&lt;span class="sb"&gt;`&lt;/span&gt;
glance --os-image-api-version &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
  image-create &lt;span class="se"&gt;\&lt;/span&gt;
  --disk-format raw &lt;span class="se"&gt;\&lt;/span&gt;
  --id &lt;span class="nv"&gt;$IMAGE_ID&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
  --location rbd://&lt;span class="nv"&gt;$CLUSTER_ID&lt;/span&gt;/&lt;span class="nv"&gt;$POOL&lt;/span&gt;/&lt;span class="nv"&gt;$IMAGE_ID&lt;/span&gt;/snap
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Of course, you might add other options, like &lt;code&gt;--private&lt;/code&gt; or
&lt;code&gt;--protected&lt;/code&gt; or &lt;code&gt;--name&lt;/code&gt;, but the above options are the bare minimum.&lt;/p&gt;
&lt;h2&gt;And that's it!&lt;/h2&gt;
&lt;p&gt;Now you can happily fire up VMs, or clone your image into a volume and
fire a VM up from that.&lt;/p&gt;</content><category term="Ceph"></category><category term="OpenStack"></category></entry><entry><title>Replacing the built-in Open edX forum with a suitable alternative</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/replace-edx-forum/index.html" rel="alternate"></link><published>2017-02-02T00:00:00+00:00</published><updated>2017-02-02T00:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2017-02-02:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/replace-edx-forum/index.html</id><summary type="html">&lt;p&gt;Open edX comes with a built-in
&lt;a href="http://edx.readthedocs.io/projects/open-edx-building-and-running-a-course/en/latest/manage_live_course/discussions.html"&gt;discussion forum&lt;/a&gt;
service. Many Open edX users find this service less than optimal: it
is the only edX service to require Ruby, it depends on a Ruby version
that is outdated and
&lt;a href="https://github.com/edx/configuration/issues/3589"&gt;no longer receives security updates (although a fix for that is on …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Open edX comes with a built-in
&lt;a href="http://edx.readthedocs.io/projects/open-edx-building-and-running-a-course/en/latest/manage_live_course/discussions.html"&gt;discussion forum&lt;/a&gt;
service. Many Open edX users find this service less than optimal: it
is the only edX service to require Ruby, it depends on a Ruby version
that is outdated and
&lt;a href="https://github.com/edx/configuration/issues/3589"&gt;no longer receives security updates (although a fix for that is on the way),&lt;/a&gt;
it and generally feels like overkill to many users.&lt;/p&gt;
&lt;p&gt;Thankfully, since the Open edX Eucalyptus release it's been quite easy
to replace the course forum with an alternative. Here at hastexo,
we're fans of &lt;a href="//www.disqus.com"&gt;Disqus&lt;/a&gt; (you may have noticed we also
use it around out web site), so let's see what we can do to drop the
Open edX Forum and replace it with Disqus.&lt;/p&gt;
&lt;h2&gt;Step 1: Locate your course's &lt;code&gt;policy.json&lt;/code&gt; file&lt;/h2&gt;
&lt;p&gt;If you keep your course materials in Git or some other
version-controlled repository, you'll already be familiar with the
&lt;a href="http://edx.readthedocs.io/projects/edx-open-learning-xml/en/latest/directory-structure.html#olx-and-directory-file-structures"&gt;directory structure of an OLX course tree.&lt;/a&gt;
If you're not,
&lt;a href="http://help.appsembler.com/article/157-how-to-export-and-import-a-course"&gt;just use edX Studio&lt;/a&gt;
to export your course into a compressed archive, download it, and
extract it on your local machine.&lt;/p&gt;
&lt;p&gt;Locate the &lt;code&gt;policies/_base&lt;/code&gt; directory. Find the &lt;code&gt;policy.json&lt;/code&gt; file
located therein. It might look like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="s2"&gt;"course/201702"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;"language"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"en"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"invitation_only"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"start"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"2017-02-01T00:00:00Z"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"advertised_start"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"2017-02-01T00:00:00Z"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"end"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"2017-02-28T23:59:59Z"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"is_new"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"catalog_visibility"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"both"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"max_student_enrollments_allowed"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;null&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"due"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;null&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"giturl"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;null&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"course_image"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"images_course_image.jpg"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"advanced_modules"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"hastexo"&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="s2"&gt;"hide_from_toc"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"ispublic"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"rerandomize"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"never"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"show_calculator"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"showanswer"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"attempted"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"days_early_for_beta"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;null&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"discussion_topics"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="s2"&gt;"General"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;"id"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"i4x-hastexo-hx212-course-201702"&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="s2"&gt;"tabs"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
      &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"Courseware"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"courseware"&lt;/span&gt;
      &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"Course Info"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"course_info"&lt;/span&gt;
      &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"Textbooks"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"textbooks"&lt;/span&gt;
      &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"Discussion"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"discussion"&lt;/span&gt;
      &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"Wiki"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"wiki"&lt;/span&gt;
      &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"Progress"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"progress"&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note the &lt;code&gt;tabs&lt;/code&gt; list. It contains the list of course tabs
(&lt;a href="http://edx.readthedocs.io/projects/edx-partner-course-staff/en/latest/course_assets/pages.html"&gt;which edX Studio, confusingly, calls "pages"&lt;/a&gt;).&lt;/p&gt;
&lt;h2&gt;Step 2: Remove the default Discussion tab&lt;/h2&gt;
&lt;p&gt;You can now edit &lt;code&gt;policy.json&lt;/code&gt;, and drop the &lt;code&gt;Discussion&lt;/code&gt; entry from
the &lt;code&gt;tabs&lt;/code&gt; list, to make it look like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="s2"&gt;"tabs"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
      &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"Courseware"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"courseware"&lt;/span&gt;
      &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"Course Info"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"course_info"&lt;/span&gt;
      &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"Textbooks"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"textbooks"&lt;/span&gt;
      &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"Wiki"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"wiki"&lt;/span&gt;
      &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"Progress"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"progress"&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Maybe you also want to remove the course wiki. Just keep whichever
tabs you'd like to keep.&lt;/p&gt;
&lt;h2&gt;Step 3: Add a "static" tab&lt;/h2&gt;
&lt;p&gt;In place of the old &lt;code&gt;Discussion&lt;/code&gt; tab (which, you may have noticed, was
of a special type conspicuously named &lt;code&gt;discussion&lt;/code&gt;), you can now put a
tab of different, simpler type: &lt;code&gt;static_tab&lt;/code&gt;. Like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="s2"&gt;"tabs"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
      &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"Courseware"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"courseware"&lt;/span&gt;
      &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"Course Info"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"course_info"&lt;/span&gt;
      &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"Textbooks"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"textbooks"&lt;/span&gt;
      &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"Discussion"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"static_tab"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"url_slug"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"discussion"&lt;/span&gt;
      &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"Wiki"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"wiki"&lt;/span&gt;
      &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"Progress"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"progress"&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that a &lt;code&gt;static_tab&lt;/code&gt; type tab also requires a value
&lt;code&gt;url_slug&lt;/code&gt;. What's that one about, you ask?&lt;/p&gt;
&lt;h2&gt;Step 4: add static content&lt;/h2&gt;
&lt;p&gt;Whatever you put into &lt;code&gt;url_slug&lt;/code&gt; tells Open edX to go look into the
&lt;code&gt;tabs&lt;/code&gt; subdirectory of your course root, and find a properly named
file there. In our case, that file needs to be named
&lt;code&gt;discussion.html&lt;/code&gt;, because we defined &lt;code&gt;"url_slug": "discussion"&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;So, head over to Disqus and grab the generated code from there, and
then stick it into &lt;code&gt;tabs/discussion.html&lt;/code&gt;. Something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;div&lt;/span&gt; &lt;span class="na"&gt;id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"disqus_thread"&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;div&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;script&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;&lt;span class="c1"&gt;// &amp;lt;![CDATA[&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;// DON'T EDIT BELOW THIS LINE&lt;/span&gt;
&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;document&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;d&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;createElement&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'script'&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="nx"&gt;s&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;src&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'//&amp;lt;your Disqus site domain name&amp;gt;/embed.js'&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="nx"&gt;s&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;setAttribute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'data-timestamp'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nb"&gt;Date&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;d&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;head&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="nx"&gt;d&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;body&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;appendChild&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;s&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;})();&lt;/span&gt;
&lt;span class="err"&gt;// ]]&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;script&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;noscript&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;Please enable JavaScript to view the &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;a&lt;/span&gt; &lt;span class="na"&gt;href&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"https://disqus.com/?ref_noscript"&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;comments powered by Disqus.&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;a&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;noscript&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;p&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;p&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Step 5: deploy&lt;/h2&gt;
&lt;p&gt;Re-compress your tarball,
&lt;a href="http://edx.readthedocs.io/projects/edx-partner-course-staff/en/latest/course_assets/pages.html"&gt;upload to Studio&lt;/a&gt;
or run
&lt;a href="https://openedx.atlassian.net/wiki/display/OpenOPS/Managing+OpenEdX+Tips+and+Tricks#ManagingOpenEdXTipsandTricks-manage.pycommands"&gt;&lt;code&gt;manage.py import&lt;/code&gt;,&lt;/a&gt;
and you're done!&lt;/p&gt;</content><category term="Open edX"></category></entry><entry><title>The Dos and Don'ts for Ceph for OpenStack</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/dos-donts-ceph-openstack/index.html" rel="alternate"></link><published>2016-11-28T00:00:00+00:00</published><updated>2016-11-28T00:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2016-11-28:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/dos-donts-ceph-openstack/index.html</id><summary type="html">&lt;p&gt;Ceph and OpenStack are an extremely useful and
&lt;a href="https://www.openstack.org/assets/survey/April-2016-User-Survey-Report.pdf"&gt;highly popular&lt;/a&gt;
combination. Still, new Ceph/OpenStack deployments frequently come
with easily avoided shortcomings — we'll help you fix them!&lt;/p&gt;
&lt;h2&gt;Do use &lt;code&gt;show_image_direct_url&lt;/code&gt; and the Glance v2 API&lt;/h2&gt;
&lt;p&gt;With Ceph RBD (RADOS Block Device), you have the ability to create
&lt;strong&gt;clones.&lt;/strong&gt; You can …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Ceph and OpenStack are an extremely useful and
&lt;a href="https://www.openstack.org/assets/survey/April-2016-User-Survey-Report.pdf"&gt;highly popular&lt;/a&gt;
combination. Still, new Ceph/OpenStack deployments frequently come
with easily avoided shortcomings — we'll help you fix them!&lt;/p&gt;
&lt;h2&gt;Do use &lt;code&gt;show_image_direct_url&lt;/code&gt; and the Glance v2 API&lt;/h2&gt;
&lt;p&gt;With Ceph RBD (RADOS Block Device), you have the ability to create
&lt;strong&gt;clones.&lt;/strong&gt; You can think of clones as the writable siblings of
&lt;em&gt;snapshots&lt;/em&gt; (which are read-only). A clone creates RADOS objects only
for those parts of your block device which have been modified relative
to its parent snapshot, and this means two things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You save space. That's a no-brainer, but in and of itself it's not
   a very compelling argument as storage space is one of the cheapest
   things in a distributed system.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What's &lt;em&gt;not&lt;/em&gt; been modified in the clone can be served from the
   original volume. This is important because, of course, it means you
   are effectively hitting the same RADOS objects — and thus, the
   same OSDs — no matter which clone you're talking to. And that, in
   turn, means, those objects are likely to be served from the
   respective OSD's page caches, in other words, from RAM. RAM is way
   faster to access than any persistent storage device, so being able
   to serve lots of reads from the page cache is good. That, in turn,
   means, that serving data from a clone will be faster than serving
   the same data from a full copy of a volume.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Both Cinder (when creating a volume from an image) and Nova (when
serving ephemeral disks from Ceph) will make use of cloning RBD images
in the Ceph backend, and will do so automatically. But they will do so
only if &lt;code&gt;show_image_direct_url=true&lt;/code&gt; is set in &lt;code&gt;glance‑api.conf&lt;/code&gt;, and
they are configured to connect to Glance using the Glance v2
API. &lt;a href="http://docs.ceph.com/docs/jewel/rbd/rbd-openstack/#any-openstack-version"&gt;So do both.&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Do set &lt;code&gt;libvirt/images_type = rbd&lt;/code&gt; on Nova compute nodes&lt;/h2&gt;
&lt;p&gt;In Nova (using the libvirt compute driver with KVM), you have several
options of storing ephemeral disk images, that is, storage for any VM
that is &lt;em&gt;not&lt;/em&gt; booted from a Cinder volume. You do so by setting the
&lt;code&gt;images_type&lt;/code&gt; option in the &lt;code&gt;[libvirt]&lt;/code&gt; section in
&lt;code&gt;nova‑compute.conf&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;[libvirt]&lt;/span&gt;
&lt;span class="na"&gt;images_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;lt;type&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The default type is &lt;code&gt;disk&lt;/code&gt;, which means that when you fire up a new
VM, the following events occur:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;nova‑compute&lt;/code&gt; on your hypervisor node connects to the Glance API,
  looks up the desired image, and downloads the image to your compute
  node (into the &lt;code&gt;/var/lib/nova/instances/_base&lt;/code&gt; directory by
  default).&lt;/li&gt;
&lt;li&gt;It then creates a new qcow2 file which uses the downloaded image as
  its backing file.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This process uses up a fair amount of space on your compute nodes,
and can quite seriously delay spawning a new VM if it has been
scheduled to a host that hasn't downloaded the desired image
before. It also makes it impossible for such a VM to be live-migrated
to another host without downtime.&lt;/p&gt;
&lt;p&gt;Flipping &lt;code&gt;images_type&lt;/code&gt; to &lt;code&gt;rbd&lt;/code&gt; means the disk lives in the RBD
backend, as an RBD clone of the original image, and can be created
instantaneously. No delay on boot, no wasting space, all the benefits
of
clones. &lt;a href="http://docs.ceph.com/docs/jewel/rbd/rbd-openstack/#id2"&gt;Use it.&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Do enable RBD caching on Nova compute nodes&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;librbd&lt;/code&gt;, the library that underpins the Qemu/KVM RBD storage driver,
can enable a disk cache that uses the hypervisor host's RAM for
caching purposes. You should use this.&lt;/p&gt;
&lt;p&gt;Yes, it's a cache that is safe to use. On the one hand, the
combination of &lt;code&gt;virtio-blk&lt;/code&gt; with the Qemu RBD storage driver &lt;strong&gt;will&lt;/strong&gt;
properly honor disk flushes. That is to say, when an application
inside your VM says "I want this data on disk now," then &lt;code&gt;virtio‑blk&lt;/code&gt;,
Qemu, and Ceph will all work together to only report the write as
complete when it has been&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;written to the primary OSD,&lt;/li&gt;
&lt;li&gt;replicated to the available replica OSDs,&lt;/li&gt;
&lt;li&gt;acknowledged to have hit at least the persistent journal on all OSDs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition, Ceph RBD has an intelligent safeguard in place: even if
it is configured to cache in write-back mode, &lt;em&gt;it will refuse to do
so&lt;/em&gt; (meaning, it will operate in write-through mode) until it has
received the first flush request from its user. Thus, if you run a VM
that just never does that — because it has been misconfigured or its
guest OS is just ages old — then RBD will stubbornly refuse to cache
any writes. The corresponding RBD option is called
&lt;a href="http://docs.ceph.com/docs/jewel/rbd/rbd-config-ref/#cache-settings"&gt;&lt;code&gt;rbd cache writethrough until flush&lt;/code&gt;&lt;/a&gt;,
it defaults to &lt;code&gt;true&lt;/code&gt; and you should never disable it.&lt;/p&gt;
&lt;p&gt;You can enable writeback caching for Ceph by setting the following
&lt;code&gt;nova-compute&lt;/code&gt; configuration option:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;[libvirt]&lt;/span&gt;
&lt;span class="na"&gt;images_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;rbd&lt;/span&gt;
&lt;span class="na"&gt;...&lt;/span&gt;
&lt;span class="na"&gt;disk_cachemodes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"network=writeback"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And you just should.&lt;/p&gt;
&lt;h2&gt;Do use separate pools for images, volumes, and ephemeral disks&lt;/h2&gt;
&lt;p&gt;Now that you have enabled &lt;code&gt;show_image_direct_url=true&lt;/code&gt; in Glance,
configured Cinder and &lt;code&gt;nova-compute&lt;/code&gt; to talk to Glance using the v2
API, and configured &lt;code&gt;nova-compute&lt;/code&gt; with &lt;code&gt;libvirt/images_type=rbd&lt;/code&gt;, all
your VMs and volumes will be using RBD clones. Clones can span
multiple RADOS pools, meaning you can have an RBD image (and its
snapshots) in one pool, and its clones in another.&lt;/p&gt;
&lt;p&gt;You should do exactly that, for several reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Separate pools means you can lock down access to those pools
   separately. This is just a standard threat mitigation approach: if
   your &lt;code&gt;nova-compute&lt;/code&gt; node gets compromised and the attacker can
   corrupt or delete ephemeral disks, then that's bad — but it would
   be &lt;em&gt;worse&lt;/em&gt; if they could also corrupt your Glance images.&lt;/li&gt;
&lt;li&gt;Separate pools also means that you can have different pool
   settings, such as the settings for &lt;code&gt;size&lt;/code&gt; or &lt;code&gt;pg_num&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Most importantly, separate pools can use separate &lt;code&gt;crush_ruleset&lt;/code&gt;
   settings. We'll get back to this in a second, it'll come in handy
   shortly.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It's common to have three different pools: one for your Glance images
(usually named &lt;code&gt;glance&lt;/code&gt; or &lt;code&gt;images&lt;/code&gt;), one for your Cinder volumes
(&lt;code&gt;cinder&lt;/code&gt; or &lt;code&gt;volumes&lt;/code&gt;), and one for your VMs (&lt;code&gt;nova-compute&lt;/code&gt; or
&lt;code&gt;vms&lt;/code&gt;).&lt;/p&gt;
&lt;h2&gt;Don't necessarily use SSDs for your Ceph OSD journals&lt;/h2&gt;
&lt;p&gt;Of the recommendations in this article, this one will probably be the
one that surprises the most people. Of course, conventional wisdom
holds that you should &lt;em&gt;always&lt;/em&gt; put your OSD journals on fast OSDs, and
that you should deploy SSDs and spinners in a 1:4 to 1:6 ratio, right?&lt;/p&gt;
&lt;p&gt;Let's take a look. Suppose you're following the 1:6 approach, and your
SATA spinners are capable of writing at 100 MB/s. 6 spinners make 6
OSDs, and each OSD uses a journal device that's on a partition on an
enterprise SSD. Suppose further that the SSD is capable of writing at
500 MB/s.&lt;/p&gt;
&lt;p&gt;Congratulations, in that scenario you've just made your SSD your
bottleneck. While you would be able to hit your OSDs at 600 MB/s on
aggregate, your SSD limits you to about 83% of that.&lt;/p&gt;
&lt;p&gt;In that scenario you &lt;em&gt;would&lt;/em&gt; actually be fine with a 1:4 ratio, but
make your spindles just a little faster and the SSD advantage goes out
the window again.&lt;/p&gt;
&lt;p&gt;Now, of course, do consider the alternative: if you're putting your
journals on the same drive as your OSD filestores, then you
effectively get only half the nominal bandwidth of your drive, on
average, because you write everything twice, to the same device. So
that means that &lt;em&gt;without&lt;/em&gt; SSDs, your effective spinner bandwidth is
only about 50 MB/s, so the &lt;em&gt;total&lt;/em&gt; bandwidth you get out of 6 drives
that way is more like 300 MB/s, against which 500 MB/s is still a
substantial improvement.&lt;/p&gt;
&lt;p&gt;So you will need to plug your own numbers into this, and make your own
evaluation for price &lt;em&gt;and&lt;/em&gt; performance. Just don't assume that journal
SSD will be a panacea, or that it's always a good idea to use them.&lt;/p&gt;
&lt;h2&gt;Do create all-flash OSDs&lt;/h2&gt;
&lt;p&gt;One thing your journal SSDs don't help with are reads. So, what can you
do to take advantage of SSDs on reads, too?&lt;/p&gt;
&lt;p&gt;Make them OSDs. That is, not OSD &lt;em&gt;journals,&lt;/em&gt; but actual OSDs with a
filestore &lt;em&gt;and&lt;/em&gt; journal. What this will create are OSDs that don't
just write fast, but read fast, too.&lt;/p&gt;
&lt;h2&gt;Do put your all-flash OSDs into a separate CRUSH root&lt;/h2&gt;
&lt;p&gt;Assuming you don't run on all-flash hardware, but operate a
cost-effective mixed cluster where some OSDs are spinners and others
are SSDs (or NVMe devices or whatever), you obviously want to treat
those OSDs separately. The simplest and easiest way to do that is to
create a separate CRUSH &lt;code&gt;root&lt;/code&gt; in addition to the normally configured
&lt;code&gt;default&lt;/code&gt; root.&lt;/p&gt;
&lt;p&gt;For example, you could set up your CRUSH hierarchy as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ID WEIGHT  TYPE NAME         UP/DOWN REWEIGHT PRIMARY-AFFINITY
- 
-1 4.85994 root default
-2 1.61998     host elk
 0 0.53999         osd.0          up  1.00000          1.00000 
 1 0.53999         osd.1          up  1.00000          1.00000 
 2 0.53999         osd.2          up  1.00000          1.00000 
-3 1.61998     host moose
 3 0.53999         osd.3          up  1.00000          1.00000 
 4 0.53999         osd.4          up  1.00000          1.00000 
 5 0.53999         osd.5          up  1.00000          1.00000 
-4 1.61998     host reindeer
 6 0.53999         osd.6          up  1.00000          1.00000 
 7 0.53999         osd.7          up  1.00000          1.00000 
 8 0.53999         osd.8          up  1.00000          1.00000
-5 4.85994 root highperf
-6 1.61998     host elk-ssd
 9 0.53999         osd.9          up  1.00000          1.00000 
10 0.53999         osd.10         up  1.00000          1.00000 
11 0.53999         osd.11         up  1.00000          1.00000 
-7 1.61998     host moose-ssd
12 0.53999         osd.12         up  1.00000          1.00000 
13 0.53999         osd.13         up  1.00000          1.00000 
14 0.53999         osd.14         up  1.00000          1.00000 
-8 1.61998     host reindeer-ssd
15 0.53999         osd.15         up  1.00000          1.00000 
16 0.53999         osd.16         up  1.00000          1.00000 
17 0.53999         osd.17         up  1.00000          1.00000
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In the example above, OSDs 0-8 are assigned to the &lt;code&gt;default&lt;/code&gt; root,
whereas OSDs 9-17 (our SSDs) belong to the root &lt;code&gt;highperf&lt;/code&gt;. We can now
create two separate CRUSH rulesets:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;rule replicated_ruleset {
    ruleset 0
    type replicated
    min_size 1
    max_size 10
    step take default
    step chooseleaf firstn 0 type host
    step emit
}

rule highperf_ruleset {
    ruleset 1
    type replicated
    min_size 1
    max_size 10
    step take highperf
    step chooseleaf firstn 0 type host
    step emit
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The default ruleset, &lt;code&gt;replicated_ruleset&lt;/code&gt;, picks OSDs from the
&lt;code&gt;default&lt;/code&gt; root, whereas &lt;code&gt;step take highperf&lt;/code&gt; in &lt;code&gt;highperf_ruleset&lt;/code&gt;
means it covers only OSDs in the &lt;code&gt;highperf&lt;/code&gt; root.&lt;/p&gt;
&lt;h2&gt;Do assign individual pools to your all-flash ruleset&lt;/h2&gt;
&lt;p&gt;Assigning individual pools to a new CRUSH ruleset (and hence, to a
whole different set of OSDs) is a matter of issuing a single command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ceph osd pool set &amp;lt;name&amp;gt; crush_ruleset &amp;lt;number&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;... where &lt;code&gt;&amp;lt;name&amp;gt;&lt;/code&gt; name of your pool and &lt;code&gt;&amp;lt;number&amp;gt;&lt;/code&gt; is the numerical
ID of your ruleset as per your CRUSH map. You can do this while the
pool is online, and while clients are accessing its data — although
of course, there will be a lot of remapping and backfilling so your
overall performance may be affected somewhat.&lt;/p&gt;
&lt;p&gt;Now, the assumption is that you will have more spinner storage than
SSD storage. Thus, you will want to select individual pools for your
all-flash OSDs. Here are a handful of pools that might come in handy
as first candidates to migrate to all-flash. You can interpret the
list below as a priority list: as you add more SSD capacity to your
cluster, you can move pools over to all-flash storage one by one.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Nova ephemeral RBD pools (&lt;code&gt;vms&lt;/code&gt;, &lt;code&gt;nova-compute&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;radosgw bucket indexes (&lt;code&gt;.rgw.buckets.index&lt;/code&gt; and friends)
   — if you're using radosgw as your drop-in OpenStack Swift
   replacement&lt;/li&gt;
&lt;li&gt;Cinder volume pools (&lt;code&gt;cinder&lt;/code&gt;, &lt;code&gt;volumes&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;radosgw data pools (&lt;code&gt;.rgw.buckets&lt;/code&gt; and friends) — if you need
   low-latency reads and writes on Swift storage&lt;/li&gt;
&lt;li&gt;Glance image pools (&lt;code&gt;glance&lt;/code&gt;, &lt;code&gt;images&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Cinder backup pools (&lt;code&gt;cinder-backup&lt;/code&gt;) — usually the last pool to
   convert to all-flash OSDs.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Do designate some non-Ceph compute hosts with low-latency local storage&lt;/h2&gt;
&lt;p&gt;Now, there will undoubtedly be some applications where Ceph does not
produce the latency you desire. Or, for that matter, &lt;em&gt;any&lt;/em&gt;
network-based storage. That's just a direct consequence of recent
developments in storage and network technology.&lt;/p&gt;
&lt;p&gt;Just a few years ago, the average latency of a single-sector uncached
write to a block device was on the order of a millisecond, or 1,000
microseconds (µs). In contrast, the latency incurred on a TCP packet
carrying a 512-byte (1-sector) payload was about 50 µs, which makes
for a 100-µs round trip. All in all, the &lt;em&gt;additional&lt;/em&gt; latency incurred
from writing to a device over the network, as opposed to locally, was
approximately 10%.&lt;/p&gt;
&lt;p&gt;In the interim, a single-sector write for a device of the same price
is itself about 100 µs, tops, with some reasonably-priced devices down
to about 40 µs. Network latency, in contrast, hasn't changed all that
much — going down about 20% from Gigabit Ethernet to 10 GbE.&lt;/p&gt;
&lt;p&gt;So even going to a single, un-replicated SSD device over the network
will now be 40 + 80 = 120 µs latency, vs. just 40 µs locally. That's
not a 10% overhead anymore, that's a whopping &lt;em&gt;factor&lt;/em&gt; of 3.&lt;/p&gt;
&lt;p&gt;With Ceph, that gets worse. Ceph writes data multiple times, first to
the primary OSD, then (in parallel) to all replicas. So in contrast to
a single-sector write at 40 µs, we now incur a latency of at least two
writes, &lt;em&gt;plus&lt;/em&gt; two network round-trips, to that's 40 x 2 + 80 x 2 =
240 µs, &lt;em&gt;six times&lt;/em&gt; the local write latency.&lt;/p&gt;
&lt;p&gt;The good news is, &lt;em&gt;most&lt;/em&gt; applications don't care about this sort of
latency overhead, because they're not latency-critical at all. The bad
news is, &lt;em&gt;some&lt;/em&gt; will.&lt;/p&gt;
&lt;p&gt;So, should you ditch Ceph because of that? Nope. But do consider
adding a handful of compute nodes that are &lt;em&gt;not&lt;/em&gt; configured with
&lt;code&gt;libvirt/images_type=rbd&lt;/code&gt;, but that use local disk images instead. Roll
those hosts into a
&lt;a href="http://docs.openstack.org/admin-guide/dashboard-manage-host-aggregates.html"&gt;host aggregate,&lt;/a&gt;
and map them to a specific flavor. Recommend to your users that they
use that flavor for low-latency applications.&lt;/p&gt;</content><category term="OpenStack"></category><category term="Ceph"></category></entry><entry><title>High Availability and Disaster Recovery in OpenStack</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/high-availability-and-disaster-recovery-in-openstack/index.html" rel="alternate"></link><published>2016-11-07T00:00:00+00:00</published><updated>2016-11-07T00:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2016-11-07:fghaas.github.io/xahteiwi.eu/resources/presentations/high-availability-and-disaster-recovery-in-openstack/index.html</id><summary type="html">&lt;p&gt;From the 2016 International Industry-Academia Workshop on Cloud
Reliability and Resilience in Berlin. An OpenStack primer followed by
a closer focus on OpenStack's HA &amp;amp; DR feature set.&lt;/p&gt;
&lt;p&gt;About 35 minutes.&lt;/p&gt;
&lt;!--break--&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//hastexo.github.io/cloud-reliability-workshop/"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;Use the arrow keys to navigate through the presentation, hit &lt;code&gt;Esc&lt;/code&gt; to
zoom out for an overview, or just advance …&lt;/p&gt;</summary><content type="html">&lt;p&gt;From the 2016 International Industry-Academia Workshop on Cloud
Reliability and Resilience in Berlin. An OpenStack primer followed by
a closer focus on OpenStack's HA &amp;amp; DR feature set.&lt;/p&gt;
&lt;p&gt;About 35 minutes.&lt;/p&gt;
&lt;!--break--&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//hastexo.github.io/cloud-reliability-workshop/"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;Use the arrow keys to navigate through the presentation, hit &lt;code&gt;Esc&lt;/code&gt; to
zoom out for an overview, or just advance by hitting the spacebar.&lt;/p&gt;</content><category term="OpenStack"></category><category term="High Availability"></category></entry><entry><title>Heat and its Alternatives: Application Deployment in OpenStack</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/heat-and-its-alternatives-application-deployment-in-openstack/index.html" rel="alternate"></link><published>2016-10-27T00:00:00+00:00</published><updated>2016-10-27T00:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2016-10-27:fghaas.github.io/xahteiwi.eu/resources/presentations/heat-and-its-alternatives-application-deployment-in-openstack/index.html</id><summary type="html">&lt;p&gt;From the 2016 OpenStack Summit in Barcelona. A comparison of tools for
virtual systems orchestration in OpenStack.
&lt;!--break--&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Heat&lt;/li&gt;
&lt;li&gt;Juju&lt;/li&gt;
&lt;li&gt;Ansible&lt;/li&gt;
&lt;li&gt;Cloudify&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;About 45 minutes.&lt;/p&gt;
&lt;!--break--&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//www.youtube.com/embed/wtXVd09qHoo"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//hastexo.github.io/openstacksummit2016-barcelona/"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;Use the arrow keys to navigate through the presentation, hit &lt;code&gt;Esc&lt;/code&gt; to
zoom out for an overview, or just advance by hitting the spacebar.&lt;/p&gt;</summary><content type="html">&lt;p&gt;From the 2016 OpenStack Summit in Barcelona. A comparison of tools for
virtual systems orchestration in OpenStack.
&lt;!--break--&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Heat&lt;/li&gt;
&lt;li&gt;Juju&lt;/li&gt;
&lt;li&gt;Ansible&lt;/li&gt;
&lt;li&gt;Cloudify&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;About 45 minutes.&lt;/p&gt;
&lt;!--break--&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//www.youtube.com/embed/wtXVd09qHoo"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//hastexo.github.io/openstacksummit2016-barcelona/"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;Use the arrow keys to navigate through the presentation, hit &lt;code&gt;Esc&lt;/code&gt; to
zoom out for an overview, or just advance by hitting the spacebar.&lt;/p&gt;</content><category term="OpenStack"></category><category term="Heat"></category><category term="Ansible"></category><category term="Juju"></category><category term="Cloudify"></category></entry><entry><title>CephFS and LXC: Container High Availability and Scalability, Redefined</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/cephfs-and-lxc-container-high-availability-and-scalability-redefined/index.html" rel="alternate"></link><published>2016-10-06T00:00:00+00:00</published><updated>2016-10-06T00:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2016-10-06:fghaas.github.io/xahteiwi.eu/resources/presentations/cephfs-and-lxc-container-high-availability-and-scalability-redefined/index.html</id><summary type="html">&lt;p&gt;An overview of applying CephFS to LXC containers.&lt;/p&gt;
&lt;!--break--&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//hastexo.github.io/lceu2016-cephlxc/"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;Use the arrow keys to navigate through the presentation, hit &lt;code&gt;Esc&lt;/code&gt; to
zoom out for an overview, or just advance by hitting the spacebar.&lt;/p&gt;</summary><content type="html">&lt;p&gt;An overview of applying CephFS to LXC containers.&lt;/p&gt;
&lt;!--break--&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//hastexo.github.io/lceu2016-cephlxc/"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;Use the arrow keys to navigate through the presentation, hit &lt;code&gt;Esc&lt;/code&gt; to
zoom out for an overview, or just advance by hitting the spacebar.&lt;/p&gt;</content><category term="Ceph"></category><category term="LXC"></category></entry><entry><title>Fragile Development: Scrum is terrible, and you should ditch it</title><link href="fghaas.github.io/xahteiwi.eu/blog/2016/07/05/fragile-development/index.html" rel="alternate"></link><published>2016-07-05T00:00:00+00:00</published><updated>2016-07-05T00:00:00+00:00</updated><author><name>florian</name></author><id>tag:None,2016-07-05:fghaas.github.io/xahteiwi.eu/blog/2016/07/05/fragile-development/index.html</id><summary type="html">&lt;p&gt;Scrum is irrational, impractical, and outright dangerous for software development. It is time to stop considering it a viable method for building software.&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is a writeup of an
&lt;a href="https://en.wikipedia.org/wiki/Ignite_(event)"&gt;Ignite&lt;/a&gt; talk I gave at
&lt;a href="http://www.openstack-israel.org"&gt;OpenStack Israel 2016&lt;/a&gt;. The
paragraph headings below approximately correspond to the content of my
talk slides; the paragraphs themselves are an approximation of what I
said. If you're interested in the exact slide content, you can find
that &lt;a href="//fghaas.github.io/openstackisrael2016-ignite"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;hr/&gt;
&lt;h2&gt;Zero flexibility&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;_____&lt;/code&gt;'s roles, artifacts, events, and rules are &lt;em&gt;immutable&lt;/em&gt; and
although implementing only parts of &lt;code&gt;_____&lt;/code&gt; is possible, &lt;em&gt;the result
is not&lt;/em&gt; &lt;code&gt;_____&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;When you see a statement like this and wonder what should be filled in
for the blanks, it's rather quite likely that you would guess either a
radical political ideology, a very strict religious sect or cult, or
something to that effect. You couldn't be further from the truth.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;Scrum&lt;/code&gt;'s roles, artifacts, events, and rules are &lt;em&gt;immutable&lt;/em&gt; and
although implementing only parts of &lt;code&gt;Scrum&lt;/code&gt; is possible, &lt;em&gt;the result
is not&lt;/em&gt; &lt;code&gt;Scrum&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Yes, that's a
&lt;a href="http://www.scrumguides.org/scrum-guide.html#endnote"&gt;direct quote from the Scrum guide.&lt;/a&gt;
Scrum, by its own definition, can either be implemented completely —
that is, with all its roles, artifacts, events, and rules &lt;em&gt;unchanged&lt;/em&gt;
— or not at all. This sounds ludicrous enough as it is, and any sane,
thinking person should reject or at least resent &lt;em&gt;any&lt;/em&gt; such statement
outright. But let's give Scrum the benefit of doubt, and let's
actually start examining some of its postulates.&lt;/p&gt;
&lt;h2&gt;Teams are self-organizing&lt;/h2&gt;
&lt;p&gt;Scrum hinges on the idea that teams are comprised of capable
individuals forming teams, which then self-organize. Now I'm sure
nobody would argue that self-organizing teams cannot exist, so this
postulate does not invalidate itself outright.&lt;/p&gt;
&lt;p&gt;However, it is missing an important prerequisite: teams can
self-organize &lt;strong&gt;if they are stable.&lt;/strong&gt; And team stability is a
precondition that almost never exists in the software industry: our
industry is &lt;em&gt;growth-oriented,&lt;/em&gt; and driven by quickly-growing startups,
so in a successful organization having a new colleague every other
month is not unheard of. It is also highly &lt;em&gt;competitive&lt;/em&gt; for talent,
so having a colleague leave every few months isn't unusual either. The
moment a new person joins or leaves, you have a new team. Team
stability goes out the window, and with it any reasonable expectation
of self-organization.&lt;/p&gt;
&lt;h2&gt;Sprint after sprint after sprint&lt;/h2&gt;
&lt;p&gt;The Scrum Guide explicitly states that every &lt;em&gt;sprint&lt;/em&gt; (a time frame of
one month or less, in which the team completes objectives agreed to
for the sprint backlog) is &lt;em&gt;immediately followed by the next sprint.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This is mind-bogglingly ludicrous and outright dangerous to your
team's mental health. Software development is a marathon, and running
a marathon as an unbroken series of sprints leads to collapse or
death. In software development, it's likely to cause burnout.&lt;/p&gt;
&lt;h2&gt;The Daily Scrum&lt;/h2&gt;
&lt;p&gt;One of Scrum's immutable events is the Daily Scrum. The Scrum Guide
defines this event as a specific, daily occurrence, time-boxed to 15
minutes and involving the entirety of the development team.&lt;/p&gt;
&lt;p&gt;This is staggeringly out of place in the modern development team,
which may well be spread out over multiple offices and timezones, and
may not even physically be in one place more than a handful of times a
year. Even in the unlikely event that everyone &lt;em&gt;can&lt;/em&gt; get together in
one room for precisely fifteen minutes each day, have you ever been in
a meeting involving more than 3 people that got anything accomplished
in 15 minutes?&lt;/p&gt;
&lt;p&gt;And remember, 15 minutes. Time-boxed, immutable. If you think &lt;em&gt;your&lt;/em&gt;
Daily Scrum can be 30 or 45 minutes, or you can do it just every other
day or maybe thrice a week, recall: if you do that, you're no longer
doing Scrum.&lt;/p&gt;
&lt;h2&gt;No planning beyond the current sprint&lt;/h2&gt;
&lt;p&gt;Scrum is quite emphatic that the only thing developers should be
really concerned about in terms of planning is the next 24 hours (the
plan for which is ostensibly being laid out in the Daily Scrum), and
beyond that, the current sprint at a maximum. Now, while the idea of
freeing people's minds and allowing them to focus on a single task at
hand is certainly laudable, the practical implications of having no
medium to long-term planning is insane.&lt;/p&gt;
&lt;p&gt;I'd venture a guess that an approach where no planning is for more
than a month out is viable, under one condition: having exactly zero
users and/or customers for the product you are developing. I leave it
to you to decide how valuable it is, then, to develop the product in
the first place.&lt;/p&gt;
&lt;h2&gt;Permanent emergency mode&lt;/h2&gt;
&lt;p&gt;Arguably, some of the methods proposed in Scrum are quite suitable for
emergency situations. In a situation where you need to come up with a
solution that requires creativity, hustle, and speed, you may well sit
down, put down a requirements list, elect a coordinator and
spokesperson for your team, and just start hacking. I'd fully agree
that such situations can be extremely challenging, and quite
satisfying to come out of with flying colors.&lt;/p&gt;
&lt;p&gt;But if your organization is permanently operating in this mode,
&lt;strong&gt;quit.&lt;/strong&gt; It doesn't matter which role you're in: as a developer,
you're headed for burnout. As a manager, you're herding your team into
burnout. Either way, you shouldn't be doing this job, either in your
own interest or in that of others.&lt;/p&gt;
&lt;h2&gt;Novelty?&lt;/h2&gt;
&lt;p&gt;Scrum proponents frequently argue in its favor as the antithesis of
the obsolete waterfall model, where all deliverables are defined from
the outset and there is no room for deviation, leading to products
that are either broken, or outdated, or both the moment they are
completed. If you think we only found out recently that waterfall is
bad, you've been asleep at the switch for over 30 years. In his
seminal
&lt;a href="https://en.wikipedia.org/wiki/The_Mythical_Man-Month"&gt;Mythical Man-Month&lt;/a&gt;
essay collection from 1975, Fred Brooks pointed out some weaknesses of
this model, and in his 1986 follow-up
&lt;a href="https://en.wikipedia.org/wiki/No_Silver_Bullet"&gt;No Silver Bullet,&lt;/a&gt; he
proposes organic, incremental software development as an alternative.&lt;/p&gt;
&lt;h2&gt;Your team can't work with Scrum?&lt;/h2&gt;
&lt;p&gt;Scrum advocates frequently argue that if Scrum doesn't work with your
team, chances are that your team is the problem. This means that you
should either replace them, or at least educate them in the ways and
means of Scrum, so they can become a better-performing team.&lt;/p&gt;
&lt;p&gt;At this point, it should be fairly obvious that if Scrum doesn't work
for your team, the problem is not your team. The problem is Scrum.&lt;/p&gt;
&lt;h2&gt;What if Scrum doesn't deliver?&lt;/h2&gt;
&lt;p&gt;And finally, Scrum proponents usually argue that if Scrum fails to
deliver adequate results in your organization, it's likely because you
aren't applying its central tenets correctly. In other words, you must
come to your senses, and implement Scrum as designed, and which point
results with magically appear, and your team will be in a constant
state of flow.&lt;/p&gt;
&lt;p&gt;This is nonsense. &lt;strong&gt;If&lt;/strong&gt; you were able to actually do Scrum (meaning
in its pure, immutable, One True Way), it would surely lead to
disaster. But, it's impossible to do so anyway, so go ahead and ditch
it — stop being a scrumbag.&lt;/p&gt;
&lt;hr/&gt;
&lt;h3&gt;Update: upcoming FrOSCon talk&lt;/h3&gt;
&lt;p&gt;The article above, as the 5-minute Ignite talk at OpenStack Israel, is
a discussion merely of Scrum's shortcomings, while not offering
workable alternatives for software development. I have a longer
version (50 minutes)
&lt;a href="https://programm.froscon.de/2016/events/1722.html"&gt;scheduled&lt;/a&gt; for
this year's &lt;a href="https://www.froscon.de/en/home/"&gt;FrOSCon,&lt;/a&gt; which does
talk about alternatives. Once completed, the presentation slides will
be available here, &lt;a href="/resources/presentations/"&gt;as usual&lt;/a&gt;.&lt;/p&gt;</content><category term="Philosophy"></category><category term="Development"></category></entry><entry><title>Wiping and resetting your SUSE OpenStack Cloud Crowbar configuration</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/wipe-suse-openstack-cloud-config/index.html" rel="alternate"></link><published>2016-07-05T00:00:00+00:00</published><updated>2016-07-05T00:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2016-07-05:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/wipe-suse-openstack-cloud-config/index.html</id><summary type="html">&lt;p&gt;If you're using
&lt;a href="https://www.suse.com/products/suse-openstack-cloud"&gt;SUSE OpenStack Cloud&lt;/a&gt;
6, you may want to erase and reinstall your cloud deployment a few
times during the testing or proof-of-concept phase. You may also want
to experiment with a few permutations of Crowbar network
configurations. SUSE's (otherwise excellent)
&lt;a href="https://www.suse.com/documentation/suse-openstack-cloud-6/book_cloud_deploy/data/book_cloud_deploy.html"&gt;Deployment Guide&lt;/a&gt;
suggests that the only way …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you're using
&lt;a href="https://www.suse.com/products/suse-openstack-cloud"&gt;SUSE OpenStack Cloud&lt;/a&gt;
6, you may want to erase and reinstall your cloud deployment a few
times during the testing or proof-of-concept phase. You may also want
to experiment with a few permutations of Crowbar network
configurations. SUSE's (otherwise excellent)
&lt;a href="https://www.suse.com/documentation/suse-openstack-cloud-6/book_cloud_deploy/data/book_cloud_deploy.html"&gt;Deployment Guide&lt;/a&gt;
suggests that the only way to change your Crowbar settings, after
&lt;code&gt;install-suse-cloud&lt;/code&gt; has been run,
&lt;a href="https://www.suse.com/documentation/suse-openstack-cloud-6/book_cloud_deploy/data/sec_depl_adm_inst_crowbar_network.html"&gt;is to reinstall your entire admin node&lt;/a&gt;.
That isn't really true if you know what you're doing.&lt;/p&gt;
&lt;p&gt;You may be thinking that you could just use
&lt;a href="http://snapper.io/"&gt;&lt;code&gt;snapper&lt;/code&gt;&lt;/a&gt; to
&lt;a href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/sec_snapper_auto.html"&gt;revert to your last Btrfs snapshot&lt;/a&gt;
created before you ran &lt;code&gt;install-suse-cloud&lt;/code&gt;. After all, running &lt;code&gt;yast2
crowbar&lt;/code&gt;, like any other YaST module, automatically creates a
before-and-after Btrfs snapshot of your root filesystem and all its
subvolumes. So, reboot machine, select pre-&lt;code&gt;install-suse-cloud&lt;/code&gt;
snapshot, complete boot, run &lt;code&gt;snapper rollback&lt;/code&gt;, done. Right?&lt;/p&gt;
&lt;p&gt;Well, not quite. If you
&lt;a href="https://www.suse.com/documentation/suse-openstack-cloud-6/book_cloud_deploy/data/sec_depl_adm_inst_partition.html"&gt;followed the Deployment Guide closely,&lt;/a&gt;
you will have removed your Btrfs subvolume for the &lt;code&gt;/srv&lt;/code&gt; directory,
and replaced it with a separate, XFS-formatted partition. That means
it is excluded from all &lt;code&gt;snapper&lt;/code&gt; Btrfs snapshots, and thus, no
rollback for you for that directory. Which, of course, Crowbar uses
rather extensively.&lt;/p&gt;
&lt;p&gt;So, here is your checklist for resetting your admin node to a
pre-&lt;code&gt;install-suse-cloud&lt;/code&gt; state:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Reboot your admin node.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the SLES boot menu, select an appropriate snapshot taken
  immediately prior to running &lt;code&gt;install-suse-cloud&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Boot into your snapshot.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run &lt;code&gt;snapper rollback&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reboot again.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;After rebooting, delete the following and directories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/srv/tftpboot/authorized_keys&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/srv/tftpboot/validation.pem&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;all subdirectories under &lt;code&gt;/srv/tftpboot/nodes/&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then, you can reconfigure Crowbar (&lt;code&gt;yast2 crowbar&lt;/code&gt;), run
&lt;code&gt;install-suse-cloud&lt;/code&gt;, and reboot your OpenStack nodes. They should be
discovered anew, and you're then able to redeploy your OpenStack
barclamps to them.&lt;/p&gt;</content><category term="OpenStack"></category><category term="SUSE"></category></entry><entry><title>Containers: Just Because Everyone Else is Doing Them Wrong, Doesn't Mean You Have To</title><link href="fghaas.github.io/xahteiwi.eu/blog/2016/02/21/containers-just-because-everyone-else/index.html" rel="alternate"></link><published>2016-02-21T00:00:00+00:00</published><updated>2016-02-21T00:00:00+00:00</updated><author><name>florian</name></author><id>tag:None,2016-02-21:fghaas.github.io/xahteiwi.eu/blog/2016/02/21/containers-just-because-everyone-else/index.html</id><summary type="html">&lt;p&gt;The recent CVE-2015-7547 vulnerability in glibc exposed a common antipattern in container management. Here's what you can do to avoid it, and instead adopt a container management pattern that will preserve your sanity and enable you to react to critical issues in minutes.&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is a writeup of &lt;a href="http://sched.co/3xVu"&gt;a presentation&lt;/a&gt; I did at
LinuxCon Europe in Dublin last year. Since Linux Foundation Events
&lt;em&gt;still&lt;/em&gt; don't come with video recording for all talks (all they do
record and publish are keynotes), I can't point you to a YouTube link,
though you're certainly welcome to
&lt;a href="fghaas.github.io/xahteiwi.eu/resources/presentations/manageable-application-containers/index.html"&gt;peruse my slides&lt;/a&gt;
from that talk.&lt;/p&gt;
&lt;h2&gt;The problem&lt;/h2&gt;
&lt;p&gt;Suppose you're an operator who, in a massively scaled-out and highly
automated deployment, is responsible for keeping a few hundred or a
few thousand containers up and running. Your developers put those
together and then basically throw them over the wall for you to
manage. It's your job just to keep them alive, available, and secure;
what's &lt;em&gt;in&lt;/em&gt; them is your developers' domain. Sure, you have Git repos
you build your containers from, and a Docker registry, so you can
always check what's in which container. You don't get to call the
shots, though.&lt;/p&gt;
&lt;p&gt;Suppose further that all most of your containers run some form of web
service. And let's assume, just for the sake of this discussion, that
they're all running Apache, because that's your reference
platform. Your developers may be writing applications in Python or
Ruby or (shudder) PHP, but what all your apps have in common is that
you've settled on Apache as your reference platform. Your developers
can assume that with Apache, you, the ops person, know the boldface
cold, and you can give them an extremely stable, well-tuned platform
to build on.&lt;/p&gt;
&lt;p&gt;And then Apache is affected by some disturbing security vulnerability
that you must now fix in record time. Say, something affecting your
core SSL library or maybe even your C library. Sound familiar? Thought
so.&lt;/p&gt;
&lt;h3&gt;The fix in a non-containerized world&lt;/h3&gt;
&lt;p&gt;OK, so you must now fix OpenSSL or libc on all your systems in record
time before the anticipated exploit barrage rolls in. In a world
without containers, you'd rely on your trusted software source
(normally, your distro vendor) to provide you with a fixed package or
packages for the affected libraries. You would then roll those out via
your preferred software management utility, or system automation
facility, or unattended upgrade scheme.&lt;/p&gt;
&lt;p&gt;In short, you'd have a tense time until updated packages are
available, but once they are, things get fixed in a matter of minutes.&lt;/p&gt;
&lt;h3&gt;But what now?&lt;/h3&gt;
&lt;p&gt;With the deployment of containers comes, frequently, the notion that
packaging, package management, or dependency tracking is somehow a
terrible idea. Instead, you put everything you need into one container
image, deploy one container per service, and not worry about what a
&lt;em&gt;different&lt;/em&gt; service running on the same physical hardware might need.&lt;/p&gt;
&lt;p&gt;At first glance, that simplifies things. Your developer needs MySQL
configured a certain way, and some other app needs it differently?
Fine, they can put everything in their own separate containers,
binaries, libraries and all, problem solved. Storage is dirt cheap,
containers are efficient and produce little overhead. If they ever
need to change anything, say go from one MySQL point release to
another, then they just rebuild the container, you replace the old
build with the new one, fine.&lt;/p&gt;
&lt;p&gt;But now it's not your developer who wants to change things, it's &lt;em&gt;you&lt;/em&gt;
who needs to deploy a critical fix.&lt;sup id="fnref:twitter"&gt;&lt;a class="footnote-ref" href="#fn:twitter" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;blockquote class="twitter-tweet" data-lang="en"&gt;&lt;p dir="ltr" lang="en"&gt;so.. using GlibC? &lt;br/&gt;&lt;br/&gt;How’s re-imaging all of your &lt;a href="https://twitter.com/docker"&gt;@Docker&lt;/a&gt; images going?&lt;/p&gt;—
Josh Long (龙之春) (@starbuxman) &lt;a href="https://twitter.com/starbuxman/status/700591322177019904"&gt;February
19, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;So you set out to rebuild a few hundred containers, or maybe a couple
of thousand, to get the issue fixed. In a perfect environment, you
have access to every build chain, know about every version of every
container in your area of responsibility, can pinpoint exactly which
are affected by the vulnerability, have an automated toolchain to
build and deploy them, have perfect documentation so you don't need to
check back with any of your developers, so it doesn't matter whether
any one is out sick, on vacation, or has left the company since they
deployed one of their, now potentially affected, services.&lt;/p&gt;
&lt;p&gt;And of course, everyone works in such a perfect environment. Right?&lt;/p&gt;
&lt;p&gt;So now, even &lt;em&gt;after&lt;/em&gt; a fix to your issue is already available, you
&lt;em&gt;still&lt;/em&gt; need to scramble to get it deployed, and deploying is &lt;em&gt;a lot&lt;/em&gt;
more complicated than in a world without containers.&lt;/p&gt;
&lt;h2&gt;Is this an inherent problem with containers?&lt;/h2&gt;
&lt;p&gt;Of course not. The problem isn't with the fact that you're using
containers, or with the specific container technology. &lt;strong&gt;The problem
is that everyone is telling you to use containers a certain way, and
from an operational perspective that way is wrong.&lt;/strong&gt; And it's not even
"wrong but still better than all other options", it's just wrong. I
guess you could call it the Docker Fallacy.&lt;/p&gt;
&lt;p&gt;That's the bad news. The good news is that there is a way that is
better, saner, and cleaner, and will make your life as an operator
&lt;em&gt;much&lt;/em&gt; easier, while not being too hard on your developer friends.&lt;/p&gt;
&lt;h2&gt;So what's a better way?&lt;/h2&gt;
&lt;p&gt;You can use containers in a simpler, less flashy, less exciting
— in short, &lt;em&gt;better&lt;/em&gt; way.&lt;/p&gt;
&lt;h3&gt;Define a core platform, or platforms&lt;/h3&gt;
&lt;p&gt;Any organization worth its salt will select a handful of distributions
to build products and services on. Maybe it's even just one, but let's
assume you have several, say the latest Ubuntu LTS,&lt;sup id="fnref:ubuntu"&gt;&lt;a class="footnote-ref" href="#fn:ubuntu" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt; the
latest CentOS, and the latest Debian. For each of these, you can
define an absolute bare-minimal list of packages. I can almost
guarantee you that none of your developers will care about a single
item on that list. A C library, a shell, an init system, coreutils,
NTP... chances are that you'll run up a list of well over 100 core
system components that &lt;em&gt;you&lt;/em&gt; will be expected to keep secure; your
developers will take them all for granted.&lt;/p&gt;
&lt;p&gt;What &lt;em&gt;you&lt;/em&gt; can take for granted, thanks to the tireless work of
packagers and distro vendors over years and years, is that you will
get timely security updates for all of those.&lt;/p&gt;
&lt;h3&gt;Deploy your core platforms as often as you need&lt;/h3&gt;
&lt;p&gt;Deploy these reference systems across your physical hardware. Deploy
as many as you need for all the containers you're expected to run on
each platform. Do so in an automated fashion, so that you never have
to log into any of these systems by hand.&lt;/p&gt;
&lt;h3&gt;Use OverlayFS for your containers&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/OverlayFS"&gt;OverlayFS&lt;/a&gt; is a union mount
filesystem that ships as part of the mainline kernel. With OverlayFS
you can do a few clever things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use a read-only base filesystem with a writable overlay to create a
  read/write union mount.&lt;/li&gt;
&lt;li&gt;Write to the union mount and only touch the overlay, leaving the
  base filesystem pristine.&lt;/li&gt;
&lt;li&gt;Hide selected content in the base filesystem from the union mount,
  through the use of
  &lt;a href="https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt"&gt;opaque directories&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Use one base filesystem with multiple overlays to create any number
  of separate read/write union mounts.&lt;/li&gt;
&lt;li&gt;Immediately make updates to the base filesystem known to &lt;em&gt;all&lt;/em&gt; union
  mounts, by simply remounting them.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This makes OverlayFS extremely powerful when used together with
LXC. You define a bunch of overlay directories — one for each of
your containers —, and they can all share one base filesystem:
your host root filesystem.&lt;sup id="fnref:automount"&gt;&lt;a class="footnote-ref" href="#fn:automount" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Then, the union mount becomes your LXC container's root. It
automatically has read access to everything that is available on the
host, unless specifically hidden, and whatever it writes goes to the
overlay. When you discard a container, you delete the overlay.&lt;/p&gt;
&lt;p&gt;Here is a minimal example configuration for a container like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# For additional config options, please look at lxc.container.conf(5)&lt;/span&gt;
&lt;span class="c1"&gt;# Common configuration&lt;/span&gt;
&lt;span class="na"&gt;lxc.include&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;/usr/share/lxc/config/ubuntu.common.conf&lt;/span&gt;
&lt;span class="c1"&gt;# Container specific configuration&lt;/span&gt;
&lt;span class="na"&gt;lxc.arch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;amd64&lt;/span&gt;
&lt;span class="c1"&gt;# Network configuration&lt;/span&gt;
&lt;span class="na"&gt;lxc.network.type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;veth&lt;/span&gt;
&lt;span class="na"&gt;lxc.network.link&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;lxcbr0&lt;/span&gt;
&lt;span class="na"&gt;lxc.network.flags&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;up&lt;/span&gt;
&lt;span class="na"&gt;lxc.network.hwaddr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;00:16:3e:76:59:10&lt;/span&gt;
&lt;span class="c1"&gt;# Automatic mounts&lt;/span&gt;
&lt;span class="na"&gt;lxc.mount.auto&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;proc sys cgroup&lt;/span&gt;

&lt;span class="na"&gt;lxc.rootfs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;overlayfs:/var/lib/lxc/host/rootfs:/var/lib/lxc/mytestcontainer/delta0&lt;/span&gt;
&lt;span class="na"&gt;lxc.utsname&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;mytestcontainer&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that the LXC userland presently enforces an OverlayFS base
directory to be in a subtree of &lt;code&gt;/var/lib/lxc&lt;/code&gt;. You can satisfy this
requirement by bind-mounting &lt;code&gt;/&lt;/code&gt; to &lt;code&gt;/var/lib/lxc/host/rootfs&lt;/code&gt;, as
shown in the example above.&lt;/p&gt;
&lt;p&gt;What this creates, among other things, is crystal-clear separation of
concerns: whatever is in the overlay is for your developers to
decide. They can pull in packages from PyPI, Ruby Gems, NPMs,
whatever. What's in the host root is your responsibility.&lt;/p&gt;
&lt;h3&gt;Automate, automate, automate&lt;/h3&gt;
&lt;p&gt;It's obvious and self-evident, but it doesn't hurt to reiterate: you
want to automate &lt;em&gt;all&lt;/em&gt; of this. You're certainly free to select your
own tools to do it, but Ansible specifically has very good LXC
container support so it makes this a breeze.&lt;/p&gt;
&lt;p&gt;Here's a simple Ansible playbook example that creates 100 containers,
all based off your host root.&lt;sup id="fnref:ansible"&gt;&lt;a class="footnote-ref" href="#fn:ansible" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;hosts&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;localhost&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;tasks&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
    &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;name&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;Create a local bind mount for the host root filesystem&lt;/span&gt;
      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;mount&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
        &lt;span class="l l-Scalar l-Scalar-Plain"&gt;name&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;/var/lib/lxc/host/rootfs&lt;/span&gt;
        &lt;span class="l l-Scalar l-Scalar-Plain"&gt;src&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;/&lt;/span&gt;
        &lt;span class="l l-Scalar l-Scalar-Plain"&gt;opts&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;bind&lt;/span&gt;
        &lt;span class="l l-Scalar l-Scalar-Plain"&gt;fstype&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;none&lt;/span&gt;
        &lt;span class="l l-Scalar l-Scalar-Plain"&gt;state&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;mounted&lt;/span&gt;
    &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;name&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;Create a template container using the host root&lt;/span&gt;
      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;lxc_container&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
        &lt;span class="l l-Scalar l-Scalar-Plain"&gt;name&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;host&lt;/span&gt;
        &lt;span class="l l-Scalar l-Scalar-Plain"&gt;state&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;stopped&lt;/span&gt;
        &lt;span class="l l-Scalar l-Scalar-Plain"&gt;directory&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;/var/lib/lxc/host/rootfs&lt;/span&gt;
        &lt;span class="l l-Scalar l-Scalar-Plain"&gt;config&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;/var/lib/lxc/host/config&lt;/span&gt;
        &lt;span class="l l-Scalar l-Scalar-Plain"&gt;container_config&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
          &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="s"&gt;"lxc.mount.auto&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;=&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;proc&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;sys&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;cgroup"&lt;/span&gt;
          &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="s"&gt;"lxc.include&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;=&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;/usr/share/lxc/config/ubuntu.common.conf"&lt;/span&gt;
    &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;name&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;Create 100 OverlayFS based containers&lt;/span&gt;
      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;lxc_container&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
        &lt;span class="l l-Scalar l-Scalar-Plain"&gt;name&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;host&lt;/span&gt;
        &lt;span class="l l-Scalar l-Scalar-Plain"&gt;backing_store&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;overlayfs&lt;/span&gt;
        &lt;span class="l l-Scalar l-Scalar-Plain"&gt;clone_snapshot&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;true&lt;/span&gt;
        &lt;span class="l l-Scalar l-Scalar-Plain"&gt;clone_name&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"mytestcontainer{{&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;item&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;}}"&lt;/span&gt;
        &lt;span class="l l-Scalar l-Scalar-Plain"&gt;state&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;started&lt;/span&gt;
      &lt;span class="l l-Scalar l-Scalar-Plain"&gt;with_sequence&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;count=100&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now of course, this will also mean that you'll need to get your
developers to define their container config in Ansible. However, that
is fundamentally a &lt;em&gt;good&lt;/em&gt; thing, because it means that developers and
operations people will be reading and writing the same language. Also,
if your developers can write a Dockerfile, they won't have a hard time
with Ansible YAML either.&lt;/p&gt;
&lt;h2&gt;How does this help?&lt;/h2&gt;
&lt;p&gt;With this approach, think of what you now have to do to make hundreds
of containers running on the same box get a new libc.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Update your host libc.&lt;/li&gt;
&lt;li&gt;Restart your containers.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That's it. That is literally all you have to do to update hundreds of
containers in one fell swoop. LXC will remount your OverlayFS on
container restart, and thus all changes to the host will be
immediately visible in the container's overlay filesystem.&lt;/p&gt;
&lt;p&gt;On an Ubuntu platform, you could even go so far as automating this in
conjunction with unattended upgrades:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# /etc/apt/apt.conf.d/50unattended-upgrades&lt;/span&gt;
&lt;span class="sr"&gt;//&lt;/span&gt; &lt;span class="n"&gt;Automatically&lt;/span&gt; &lt;span class="n"&gt;upgrade&lt;/span&gt; &lt;span class="n"&gt;packages&lt;/span&gt; &lt;span class="n"&gt;from&lt;/span&gt; &lt;span class="n"&gt;these&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;origin:archive&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;pairs&lt;/span&gt;
&lt;span class="n"&gt;Unattended&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nn"&gt;Upgrade::&lt;/span&gt;&lt;span class="n"&gt;Allowed&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Origins&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s"&gt;"${distro_id}:${distro_codename}-security"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;};&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# /etc/apt/apt.conf.d/05lxc&lt;/span&gt;
&lt;span class="nn"&gt;DPkg::&lt;/span&gt;&lt;span class="n"&gt;Post&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Invoke&lt;/span&gt;      &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s"&gt;"/sbin/service lxc restart"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="p"&gt;};&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So there you have it. Upgrade loads of containers in minutes. No
rebuild, no redeploy, nothing. Packaging actually does work and has
merit, regardless of what the hipster crowd is trying to sell you.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:twitter"&gt;
&lt;p&gt;Edit, 2016-02-22: Added Twitter quote from Josh Long. &lt;a class="footnote-backref" href="#fnref:twitter" rev="footnote" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:ubuntu"&gt;
&lt;p&gt;At the time of writing, the latest Ubuntu LTS is 14.04 "Trusty
Tahr", which is based on a Linux 3.13 kernel. This Ubuntu stock
kernel ships with a pre-release version of OverlayFS which
predates the 3.14 mainline merge. I would not recommend using that
kernel; instead you'll want to run your hosts with a more recent
kernel from the
&lt;a href="https://wiki.ubuntu.com/Kernel/LTSEnablementStack"&gt;LTS Enablement Stack&lt;/a&gt;. Again
at the time of writing, this is a Linux 4.2 kernel that ships with
the &lt;code&gt;linux-generic-lts-wily&lt;/code&gt; package. &lt;a class="footnote-backref" href="#fnref:ubuntu" rev="footnote" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:automount"&gt;
&lt;p&gt;LXC containers do present per-container specific content for some
directories by default, notably &lt;code&gt;/proc&lt;/code&gt;, &lt;code&gt;/dev&lt;/code&gt;, and &lt;code&gt;/sys&lt;/code&gt;. Other
host-filesystem content can be hidden by creating opaque
directories in the container overlay; this is what you would
commonly do for directories like &lt;code&gt;/root&lt;/code&gt;, &lt;code&gt;/home&lt;/code&gt;, &lt;code&gt;/tmp&lt;/code&gt; and
others. &lt;a class="footnote-backref" href="#fnref:automount" rev="footnote" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:ansible"&gt;
&lt;p&gt;Please note that it's not &lt;em&gt;quite&lt;/em&gt; as simple as shown in the
Ansible example. You will want to provide some additional tweaks,
such as added mounts or opaque directories. I've tried to keep the
example brief to illustrate the concept. &lt;a class="footnote-backref" href="#fnref:ansible" rev="footnote" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="LXC"></category><category term="Containers"></category><category term="Ubuntu"></category><category term="Ansible"></category></entry><entry><title>Dogfooding Dogwood</title><link href="fghaas.github.io/xahteiwi.eu/blog/2016/02/12/dogfooding-dogwood/index.html" rel="alternate"></link><published>2016-02-12T00:00:00+00:00</published><updated>2016-02-12T00:00:00+00:00</updated><author><name>florian</name></author><id>tag:None,2016-02-12:fghaas.github.io/xahteiwi.eu/blog/2016/02/12/dogfooding-dogwood/index.html</id><summary type="html">&lt;p&gt;The Open edX "Dogwood" release is out. We've been running its code base in production for several weeks, and can share some first-hand experience.&lt;/p&gt;</summary><content type="html">&lt;p&gt;This week, the &lt;a href="https://open.edx.org"&gt;Open edX&lt;/a&gt; community
&lt;a href="https://open.edx.org/blog/newest-open-edx-release-dogwood-now-available"&gt;announced&lt;/a&gt;
its latest release,
&lt;a href="http://edx.readthedocs.org/projects/open-edx-release-notes/en/latest/dogwood.html"&gt;Open edX Dogwood&lt;/a&gt;. (In
case you don't follow the Open edX community closely, its releases are
alphabetically named after trees, so on the heels of the Birch and
Cypress releases, we now have
&lt;a href="https://en.wikipedia.org/wiki/Cornus_(genus)"&gt;Dogwood&lt;/a&gt;, and
Eucalyptus will be next.)&lt;/p&gt;
&lt;p&gt;Our team got involved in Open edX around the Cypress release
timeframe, and we shifted
&lt;a href="fghaas.github.io/xahteiwi.eu/resources/presentations/openedxcon2015/index.html"&gt;our OpenStack integration work&lt;/a&gt;
to track the master branch in December, to ensure we would be ready in
time for Dogwood. &lt;a href="//academy.hastexo.com"&gt;hastexo Academy&lt;/a&gt; also tracks
master, so if you take one of our self-paced online courses, you'll be
running the latest and greatest from Open edX.&lt;/p&gt;
&lt;hr/&gt;
&lt;h2&gt;Checking out the new features&lt;/h2&gt;
&lt;p&gt;There are several new features in Open edX Dogwood, some of which we
tested and ran, with somewhat mixed (but generally positive) results.&lt;/p&gt;
&lt;h3&gt;Platform upgrades&lt;/h3&gt;
&lt;p&gt;Open edX now builds upon Django 1.8 and Python 2.7.10. It's great to
see some technical debt pay-down by moving beyond the now-unsupported
Django 1.4. We hope to see this continue by Eucalyptus
&lt;a href="https://openedx.slack.com/archives/general/p1455215550000885"&gt;hopefully moving to the next Ubuntu LTS, 16.04 "Xenial Xerus".&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It would also be great to see a move to Python 3, but we're not
holding our breath on that, for various reasons — including the fact
that Ansible, which Open edX uses for deployment, &lt;a href="https://lwn.net/Articles/661590/"&gt;also still requires
Python 2.&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Comprehensive theming&lt;/h3&gt;
&lt;p&gt;Comprehensive theming is a new and improved way to apply theming and
branding to Open edX platforms, which will eventually replace the
current "Stanford" theming engine (named after an Open edX theme
developed at Stanford University, which became a popular basis for
rebranding the Open edX LMS). In mid-January, we shifted
&lt;a href="https://github.com/hastexo/edx-theme"&gt;our own Stanford-style Open edX theme&lt;/a&gt;
to Comprehensive Theming and test-deployed on hastexo Academy, then
still in pre-launch. We ran into a critical bug
&lt;a href="https://github.com/edx/edx-platform/pull/11319"&gt;that has been fixed for the release,&lt;/a&gt;
and will come back to redeploying our new Comprehensive theme at a
later date.&lt;/p&gt;
&lt;p&gt;We're also waiting for a
&lt;a href="https://github.com/edx/configuration/pull/2676"&gt;patch to the &lt;code&gt;edx-configuration&lt;/code&gt; Ansible repository&lt;/a&gt;
to land, so we can properly deploy our Comprehensive theme to our Open
edX instance.&lt;/p&gt;
&lt;h3&gt;Otto&lt;/h3&gt;
&lt;p&gt;We also looked extensively at the new Open edX ecommerce framework,
"Otto", for buying and sellling course seats. Sadly, we found multiple
issues that prevented us from using it in our infrastructure for the
time being, and we pushed Otto off for our Eucalyptus respin.&lt;/p&gt;
&lt;p&gt;Otto has no support for tax assessment on course seats; this is a show
stopper for anyone who wants to sell courses to people in Europe, as
course seats are Digital Goods under EU VAT regulations and require
VAT assessment. We were admittedly a little dismayed to find that Otto
had made some design decisions that made this impossible to fix in the
way you would normally do this in the
&lt;a href="http://django-oscar.readthedocs.org/en/latest/"&gt;Oscar&lt;/a&gt; framework that
Otto builds on. Fixing Otto in-place would likely have delayed our
Academy launch by several months, so that was a delay we were
unwilling to accept. There are other issues with Otto, notably the
fact that it comes with its own PayPal integration (as if
&lt;a href="http://django-oscar-paypal.readthedocs.org/en/latest/"&gt;django-oscar-paypal&lt;/a&gt;
didn't exist), which made us rather uncomfortable.&lt;/p&gt;
&lt;p&gt;So we instead integrated hastexo Academy with our own, pure-Oscar web
store that makes use of upstream community supported features much
more extensively than Otto, and that also enables us to sell other
products and services besides hastexo Academy course seats.&lt;/p&gt;
&lt;h3&gt;LTI XBlock&lt;/h3&gt;
&lt;p&gt;With the Dogwood release, the LTI XModule has been refactored into the
&lt;a href="https://github.com/edx/xblock-lti-consumer"&gt;LTI Consumer XBlock&lt;/a&gt;. While
we do not currently use this XBlock in production, it comes in very
handy as a good reference for
&lt;a href="https://github.com/edx/xblock-lti-consumer/tree/master/lti_consumer/tests/unit"&gt;XBlock unit tests&lt;/a&gt;,
which we'll be using to improve the test coverage in our own XBlock.&lt;/p&gt;
&lt;hr/&gt;
&lt;h2&gt;Open edX integration with OpenStack&lt;/h2&gt;
&lt;p&gt;Our OpenStack integration work for Open edX is continuing at its
regular, steady pace.&lt;/p&gt;
&lt;h3&gt;Running Open edX Dogwood on OpenStack&lt;/h3&gt;
&lt;p&gt;You're of course still able to deploy Open edX on OpenStack, using the
Heat templates we've maintained since Cypress.&lt;/p&gt;
&lt;h3&gt;Running the hastexo XBlock on Open edX Dogwood&lt;/h3&gt;
&lt;p&gt;The hastexo XBlock, enabling course authors to define arbitrarily
complex lab environments for courses with OpenStack Heat, is of course
fully supported for Open edX Dogwood. That's exactly what you're using
when speeding through interactive labs on hastexo Academy.&lt;/p&gt;
&lt;hr/&gt;
&lt;h2&gt;Congrats, and thanks!&lt;/h2&gt;
&lt;p&gt;Congratulations are in order for the entire development community! Our
team at hastexo would like to extend a big thank-you to everyone who
made a contribution to this release.&lt;/p&gt;</content><category term="Open edX"></category><category term="OpenStack"></category></entry><entry><title>Pacemaker's best-kept secret: crm_report</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/pacemakers-best-kept-secret/index.html" rel="alternate"></link><published>2016-01-30T00:00:00+00:00</published><updated>2016-02-03T00:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2016-01-30:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/pacemakers-best-kept-secret/index.html</id><summary type="html">&lt;p&gt;Pacemaker has an excellent, but little-known, error reporting facility: crm_report.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Whenever things in Pacemaker go wrong (say, for example, resource
failover doesn't work as expected, or your cluster didn't properly
recover after a node shutdown), you'll want to find out just exactly
&lt;em&gt;why&lt;/em&gt; that happened. Of course, the actual reason for the malfunction
may be buried somewhere deep in your cluster configuration or setup,
and so you might need to look at quite a few different sources to pin
it down.&lt;/p&gt;
&lt;p&gt;Sometimes, too, you want to enlist the help of a colleague, &lt;a href="/contact"&gt;or maybe
&lt;strong&gt;our&lt;/strong&gt; help even&lt;/a&gt;, to get to the bottom of the issue. And
sometimes it's not practical to let someone access to system to just
trigger the problem and watch what breaks.&lt;/p&gt;
&lt;p&gt;Thankfully, Pacemaker ships with a utility that helps you collect
everything you or someone else might need to look at, in a simple,
compact format. Unfortunately few people, including even long-time
Pacemaker users, know that it exists: it's called &lt;code&gt;crm_report&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Running &lt;code&gt;crm_report&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;crm_report&lt;/code&gt;'s command syntax is rather quite simple. You just tell it
how far in the past you want the report to start, and which directory
you want to collect data in:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;crm_report -f &lt;span class="s2"&gt;"2016-01-25 00:00:00"&lt;/span&gt; /tmp/crm_report
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The directory you specify must not exist. If it does, &lt;code&gt;crm_report&lt;/code&gt;
will refuse to run, rather than clobber or mess up your existing
report data.&lt;/p&gt;
&lt;p&gt;By analyzing your logs all the way back to a start date you specify,
&lt;strong&gt;&lt;code&gt;crm_report&lt;/code&gt; makes it unnecessary for you to actually try to
reproduce the problem.&lt;/strong&gt; All you need is a rough idea when the issue
occurred, and then you give &lt;code&gt;crm_report&lt;/code&gt; a timestamp a little earlier
than that as its start date.&lt;/p&gt;
&lt;p&gt;You can also specify the &lt;em&gt;end&lt;/em&gt; of the period you're interested
in. Suppose you're exactly aware of a 10-minute time window in which
the problem occurred. In that case, you could run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;crm_report -f &lt;span class="s2"&gt;"2016-01-25 01:15:00"&lt;/span&gt; -t &lt;span class="s2"&gt;"2016-01-25 01:25:00"&lt;/span&gt; /tmp/crm_report
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Either way, &lt;code&gt;crm_report&lt;/code&gt; will collect relevant log data for the
specified time window on the host it is run on, and then connect to
the other cluster nodes (via &lt;code&gt;ssh&lt;/code&gt;) and do the same there. The latter
behavior can be disabled by adding the &lt;code&gt;-S&lt;/code&gt; or &lt;code&gt;--single-node&lt;/code&gt; option,
but there usually isn't a good reason to do that. In the end,
everything will be rolled into one tarball at
&lt;code&gt;/tmp/crm_report.tar.bz2&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You can then pull the report tarball off the node (with &lt;code&gt;scp&lt;/code&gt;,
&lt;code&gt;rsync&lt;/code&gt;, whatever you prefer), and then share it with whom you need
to. &lt;strong&gt;Note that the tarball can contain sensitive information such as
passwords, so be careful whom you share it with.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;What's in a &lt;code&gt;crm_report&lt;/code&gt; tarball?&lt;/h2&gt;
&lt;p&gt;There's a bunch of truly helpful information in a &lt;code&gt;crm_report&lt;/code&gt;
generated tarball. Depending on how your cluster is configured and
what problems were detected, it will contain, among other things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Your current Pacemaker Cluster Information Base (CIB),&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Your Corosync configuration,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Corosync Blackbox output (if &lt;code&gt;qb-blackbox&lt;/code&gt; is installed on your
  cluster nodes; you can read more about blackbox support
  &lt;a href="http://blog.clusterlabs.org/blog/2013/pacemaker-logging/"&gt;here&lt;/a&gt;),&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;drbd.conf&lt;/code&gt; and all your DRBD resource configuration files (if your
  cluster runs DRBD),&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;sysinfo.txt&lt;/code&gt;, a text file including your kernel, distro, Pacemaker
  version, and version information for all your installed packages,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;your Syslog, filtered for the time period you specified in your
  &lt;code&gt;crm_report&lt;/code&gt; command invocation,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;diffs for critical system information, if &lt;code&gt;crm_report&lt;/code&gt; detected
  discrepancies between nodes.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In other words, it contains pretty much everything that needs to be
shared in a critical troubleshooting situation.&lt;/p&gt;
&lt;h2&gt;Why isn't this more widely known?&lt;/h2&gt;
&lt;p&gt;To be perfectly honest, we have no idea. &lt;code&gt;crm_report&lt;/code&gt; has been in
Pacemaker for years, and even prior to its existence, there was a
predecessor named &lt;code&gt;hb_report&lt;/code&gt;. It's an extraordinarily useful utility,
yet when we ask customers to send a &lt;code&gt;crm_report&lt;/code&gt; tarball during a
Pacemaker troubleshooting engagement, the usual response is, “a
what?”&lt;/p&gt;
&lt;p&gt;We hope this post makes &lt;code&gt;crm_report&lt;/code&gt; known to a wider audience, so it
gets the love it deserves. &lt;i class="fa fa-smile-o"&gt;&lt;/i&gt;&lt;/p&gt;</content><category term="Pacemaker"></category></entry><entry><title>Hosting a web site in radosgw</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/hosting-website-radosgw/index.html" rel="alternate"></link><published>2016-01-26T00:00:00+00:00</published><updated>2016-01-26T00:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2016-01-26:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/hosting-website-radosgw/index.html</id><summary type="html">&lt;p&gt;If you're familiar with &lt;a href="//docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html"&gt;web site hosting on Amazon
S3&lt;/a&gt;,
which is a simple and cheap way to host a static web site, you might
be wondering whether or not you can do the same in Ceph radosgw.&lt;/p&gt;
&lt;p&gt;The short answer is you can't. Bucket Website is listed as &lt;em&gt;Not …&lt;/em&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you're familiar with &lt;a href="//docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html"&gt;web site hosting on Amazon
S3&lt;/a&gt;,
which is a simple and cheap way to host a static web site, you might
be wondering whether or not you can do the same in Ceph radosgw.&lt;/p&gt;
&lt;p&gt;The short answer is you can't. Bucket Website is listed as &lt;em&gt;Not
Supported&lt;/em&gt; in the radosgw S3 API
&lt;a href="http://docs.ceph.com/docs/master/radosgw/s3/"&gt;support matrix&lt;/a&gt;, and
radosgw doesn't have
&lt;a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/IndexDocumentSupport.html"&gt;index document support&lt;/a&gt;
either.&lt;/p&gt;
&lt;p&gt;But the longer answer is that you can, provided you use radosgw in
combination with a front-end load-balancer — which, as it happens,
can add a few more bells and whistles, as well. You could probably do
the same thing with nginx, Varnish, or Apache in a
&lt;code&gt;mod_proxy_balancer&lt;/code&gt; balancer setup, but in this example
configuration, we'll use HAProxy.&lt;/p&gt;
&lt;h2&gt;Getting started: the radosgw basics&lt;/h2&gt;
&lt;p&gt;Let's take look at a simple radosgw configuration with virtual host
support, such that you can access your buckets as either
&lt;code&gt;http://ceph.example.com/bucketname&lt;/code&gt; or
&lt;code&gt;http://bucketname.ceph.example.com&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;[client.rgw.radosgw01]&lt;/span&gt;
&lt;span class="na"&gt;rgw_frontends&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;civetweb port=7480&lt;/span&gt;
&lt;span class="na"&gt;rgw_dns_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;ceph.example.com&lt;/span&gt;
&lt;span class="na"&gt;rgw_resolve_cname&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;True&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Suppose we use &lt;code&gt;s3cmd&lt;/code&gt; to upload an HTML file to this bucket, setting
a public ACL:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;s3cmd mb s3://testwebsite
s3cmd put --acl-public index.html s3://testwebsite/
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then if you exposed your radosgw to the web, any client (without
authentication) would be able to retrieve
&lt;code&gt;http://testwebsite.ceph.example.com:7480/index.html&lt;/code&gt; with a web
browser, or any other HTTP client application (such as &lt;code&gt;curl&lt;/code&gt; or
&lt;code&gt;wget&lt;/code&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;curl -I http://testwebsite.ceph.example.com:7480/index.html
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Which would then return something like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kr"&gt;HTTP&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="m"&gt;1.1&lt;/span&gt; &lt;span class="m"&gt;200&lt;/span&gt; &lt;span class="ne"&gt;OK&lt;/span&gt;
&lt;span class="na"&gt;Content-Length&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="l"&gt;18050&lt;/span&gt;
&lt;span class="na"&gt;Accept-Ranges&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="l"&gt;bytes&lt;/span&gt;
&lt;span class="na"&gt;Last-Modified&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="l"&gt;Mon, 25 Jan 2016 21:28:47 GMT&lt;/span&gt;
&lt;span class="na"&gt;ETag&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="l"&gt;"b03130a4a1fc24df0f9f336f2b6d1d90"&lt;/span&gt;
&lt;span class="na"&gt;x-amz-request-id&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="l"&gt;tx000000000000000005a88-0056a7b7eb-312df-default&lt;/span&gt;
&lt;span class="na"&gt;Content-type&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="l"&gt;text/html&lt;/span&gt;
&lt;span class="na"&gt;Date&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="l"&gt;Tue, 26 Jan 2016 18:16:11 GMT&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Introducing HAProxy&lt;/h2&gt;
&lt;p&gt;Now let's start out with putting HAproxy in between. Nothing special
there: radosgw listens on the conventional 7480 port, and we simply
hand HAproxy traffic through there, and bind HAProxy itself to
port 80.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;global&lt;/span&gt;
    &lt;span class="s"&gt;log&lt;/span&gt;         &lt;span class="s"&gt;/dev/log&lt;/span&gt; &lt;span class="s"&gt;local0&lt;/span&gt;
    &lt;span class="s"&gt;pidfile&lt;/span&gt;     &lt;span class="s"&gt;/var/run/haproxy.pid&lt;/span&gt;
    &lt;span class="s"&gt;maxconn&lt;/span&gt;     &lt;span class="mi"&gt;4000&lt;/span&gt;
    &lt;span class="s"&gt;user&lt;/span&gt;        &lt;span class="s"&gt;haproxy&lt;/span&gt;
    &lt;span class="s"&gt;group&lt;/span&gt;       &lt;span class="s"&gt;haproxy&lt;/span&gt;
    &lt;span class="s"&gt;daemon&lt;/span&gt;

    &lt;span class="c1"&gt;# turn on stats unix socket&lt;/span&gt;
    &lt;span class="s"&gt;stats&lt;/span&gt; &lt;span class="s"&gt;socket&lt;/span&gt; &lt;span class="s"&gt;/var/lib/haproxy/stats&lt;/span&gt; &lt;span class="s"&gt;level&lt;/span&gt; &lt;span class="s"&gt;admin&lt;/span&gt;

    &lt;span class="c1"&gt;# Default SSL material locations&lt;/span&gt;
    &lt;span class="s"&gt;ca-base&lt;/span&gt; &lt;span class="s"&gt;/etc/ssl/certs&lt;/span&gt;
    &lt;span class="s"&gt;crt-base&lt;/span&gt; &lt;span class="s"&gt;/etc/haproxy/ssl&lt;/span&gt;

    &lt;span class="c1"&gt;# Default ciphers to use on SSL-enabled listening sockets.&lt;/span&gt;
    &lt;span class="c1"&gt;# For more information, see ciphers(1SSL).&lt;/span&gt;
    &lt;span class="s"&gt;ssl-default-bind-ciphers&lt;/span&gt; &lt;span class="s"&gt;HIGH&lt;/span&gt;
    &lt;span class="s"&gt;tune.ssl.default-dh-param&lt;/span&gt; &lt;span class="mi"&gt;2048&lt;/span&gt;

&lt;span class="s"&gt;defaults&lt;/span&gt;
    &lt;span class="s"&gt;log&lt;/span&gt; &lt;span class="s"&gt;global&lt;/span&gt;
    &lt;span class="s"&gt;mode&lt;/span&gt; &lt;span class="s"&gt;http&lt;/span&gt;
    &lt;span class="s"&gt;option&lt;/span&gt; &lt;span class="s"&gt;httplog&lt;/span&gt;
    &lt;span class="s"&gt;option&lt;/span&gt; &lt;span class="s"&gt;dontlognull&lt;/span&gt;
    &lt;span class="s"&gt;retries&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
    &lt;span class="s"&gt;timeout&lt;/span&gt; &lt;span class="s"&gt;queue&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;
    &lt;span class="s"&gt;timeout&lt;/span&gt; &lt;span class="s"&gt;connect&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;
    &lt;span class="s"&gt;timeout&lt;/span&gt; &lt;span class="s"&gt;client&lt;/span&gt; &lt;span class="mi"&gt;30000&lt;/span&gt;
    &lt;span class="s"&gt;timeout&lt;/span&gt; &lt;span class="s"&gt;server&lt;/span&gt; &lt;span class="mi"&gt;30000&lt;/span&gt;
    &lt;span class="s"&gt;option&lt;/span&gt; &lt;span class="s"&gt;forwardfor&lt;/span&gt;


&lt;span class="s"&gt;frontend&lt;/span&gt; &lt;span class="s"&gt;ceph_front&lt;/span&gt;
    &lt;span class="s"&gt;bind&lt;/span&gt; &lt;span class="n"&gt;0.0.0.0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;80&lt;/span&gt;
    &lt;span class="s"&gt;default_backend&lt;/span&gt; &lt;span class="s"&gt;ceph_back&lt;/span&gt;

&lt;span class="s"&gt;backend&lt;/span&gt; &lt;span class="s"&gt;ceph_back&lt;/span&gt;
    &lt;span class="s"&gt;balance&lt;/span&gt; &lt;span class="s"&gt;source&lt;/span&gt;
    &lt;span class="s"&gt;server&lt;/span&gt; &lt;span class="s"&gt;radosgw01&lt;/span&gt; &lt;span class="n"&gt;127.0.0.1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;7480&lt;/span&gt; &lt;span class="s"&gt;check&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Index documents&lt;/h2&gt;
&lt;p&gt;So, the first thing we'll need to add is support for index
documents. We'd like to make sure that when we retrieve
&lt;code&gt;https://testwebsite.ceph.example.com/&lt;/code&gt;, what's actually fetched from
the backend is &lt;code&gt;/index.html&lt;/code&gt;. We can do that by adding an HAproxy ACL
that matches for the trailing slash in the path, and an &lt;code&gt;http-request
set-path&lt;/code&gt; directive that appends the index document name:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;frontend&lt;/span&gt; &lt;span class="s"&gt;ceph_front&lt;/span&gt;
    &lt;span class="s"&gt;bind&lt;/span&gt; &lt;span class="n"&gt;0.0.0.0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;80&lt;/span&gt;
    &lt;span class="s"&gt;acl&lt;/span&gt; &lt;span class="s"&gt;path_ends_in_slash&lt;/span&gt; &lt;span class="s"&gt;path_end&lt;/span&gt; &lt;span class="s"&gt;-i&lt;/span&gt; &lt;span class="s"&gt;/&lt;/span&gt;
    &lt;span class="c1"&gt;# Append index document (index.html) to any path&lt;/span&gt;
    &lt;span class="c1"&gt;# ending in "/".&lt;/span&gt;
    &lt;span class="s"&gt;http-request&lt;/span&gt; &lt;span class="s"&gt;set-path&lt;/span&gt; &lt;span class="s"&gt;%[path]index.html&lt;/span&gt; &lt;span class="s"&gt;if&lt;/span&gt; &lt;span class="s"&gt;path_ends_in_slash&lt;/span&gt;
    &lt;span class="s"&gt;default_backend&lt;/span&gt; &lt;span class="s"&gt;ceph_back&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, that's fine in terms of &lt;strong&gt;getting&lt;/strong&gt; the index document correctly:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;curl -I http://testwebsite.ceph.example.com/index.html
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kr"&gt;HTTP&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="m"&gt;1.1&lt;/span&gt; &lt;span class="m"&gt;200&lt;/span&gt; &lt;span class="ne"&gt;OK&lt;/span&gt;
&lt;span class="na"&gt;Content-Length&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="l"&gt;18050&lt;/span&gt;
&lt;span class="na"&gt;Accept-Ranges&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="l"&gt;bytes&lt;/span&gt;
&lt;span class="na"&gt;Last-Modified&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="l"&gt;Mon, 25 Jan 2016 21:28:47 GMT&lt;/span&gt;
&lt;span class="na"&gt;ETag&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="l"&gt;"b03130a4a1fc24df0f9f336f2b6d1d90"&lt;/span&gt;
&lt;span class="na"&gt;x-amz-request-id&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="l"&gt;tx000000000000000005a94-0056a7b9e3-312df-default&lt;/span&gt;
&lt;span class="na"&gt;Content-type&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="l"&gt;text/html&lt;/span&gt;
&lt;span class="na"&gt;Date&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="l"&gt;Tue, 26 Jan 2016 18:24:35 GMT&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;However, it of course breaks uploads and even bucket listings, or in
other words, anything that uses the S3 API. Now you could test for
some S3-specific headers in the request, but really, you should just
check whether the request is authorized, and only apply the index
document logic if it isn't, like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;frontend&lt;/span&gt; &lt;span class="s"&gt;ceph_front&lt;/span&gt;
    &lt;span class="s"&gt;bind&lt;/span&gt; &lt;span class="n"&gt;0.0.0.0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;80&lt;/span&gt;
    &lt;span class="s"&gt;acl&lt;/span&gt; &lt;span class="s"&gt;path_ends_in_slash&lt;/span&gt; &lt;span class="s"&gt;path_end&lt;/span&gt; &lt;span class="s"&gt;-i&lt;/span&gt; &lt;span class="s"&gt;/&lt;/span&gt;
    &lt;span class="s"&gt;acl&lt;/span&gt; &lt;span class="s"&gt;auth_header&lt;/span&gt; &lt;span class="s"&gt;hdr(Authorization)&lt;/span&gt; &lt;span class="s"&gt;-m&lt;/span&gt; &lt;span class="s"&gt;found&lt;/span&gt;
    &lt;span class="c1"&gt;# Append index document (index.html) to any path&lt;/span&gt;
    &lt;span class="c1"&gt;# ending in "/", unless the request has an auth header&lt;/span&gt;
    &lt;span class="s"&gt;http-request&lt;/span&gt; &lt;span class="s"&gt;set-path&lt;/span&gt; &lt;span class="s"&gt;%[path]index.html&lt;/span&gt; &lt;span class="s"&gt;if&lt;/span&gt; &lt;span class="s"&gt;path_ends_in_slash&lt;/span&gt; &lt;span class="s"&gt;!auth_header&lt;/span&gt;
    &lt;span class="s"&gt;default_backend&lt;/span&gt; &lt;span class="s"&gt;ceph_back&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Great. Now we can upload using full paths without mangling, and on any
un-authenticated requests, we substitute &lt;code&gt;/index.html&lt;/code&gt; for any trailing
&lt;code&gt;/&lt;/code&gt;. In case you're wondering: yes, this works for any path, not just
the root path.&lt;/p&gt;
&lt;h2&gt;Directory paths&lt;/h2&gt;
&lt;p&gt;However, you may also want something else, which is the ability to
correctly handle a request like
&lt;code&gt;http://testwebsite.ceph.example.com/my/sub/directory&lt;/code&gt;, where of
course you want the path &lt;code&gt;/my/sub/directory&lt;/code&gt; translated into
&lt;code&gt;/my/sub/directory/index.html&lt;/code&gt;, which means we want to append a slash
&lt;em&gt;and&lt;/em&gt; an index document name to the request path.&lt;/p&gt;
&lt;p&gt;So let's do that:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;frontend&lt;/span&gt; &lt;span class="s"&gt;ceph_front&lt;/span&gt;
    &lt;span class="s"&gt;bind&lt;/span&gt; &lt;span class="n"&gt;0.0.0.0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;80&lt;/span&gt;
    &lt;span class="s"&gt;acl&lt;/span&gt; &lt;span class="s"&gt;path_has_dot&lt;/span&gt; &lt;span class="s"&gt;path_sub&lt;/span&gt; &lt;span class="s"&gt;-i&lt;/span&gt; &lt;span class="s"&gt;.&lt;/span&gt;
    &lt;span class="s"&gt;acl&lt;/span&gt; &lt;span class="s"&gt;path_ends_in_slash&lt;/span&gt; &lt;span class="s"&gt;path_end&lt;/span&gt; &lt;span class="s"&gt;-i&lt;/span&gt; &lt;span class="s"&gt;/&lt;/span&gt;
    &lt;span class="s"&gt;acl&lt;/span&gt; &lt;span class="s"&gt;auth_header&lt;/span&gt; &lt;span class="s"&gt;hdr(Authorization)&lt;/span&gt; &lt;span class="s"&gt;-m&lt;/span&gt; &lt;span class="s"&gt;found&lt;/span&gt;
    &lt;span class="s"&gt;http-request&lt;/span&gt; &lt;span class="s"&gt;set-path&lt;/span&gt; &lt;span class="s"&gt;%[path]index.html&lt;/span&gt; &lt;span class="s"&gt;if&lt;/span&gt; &lt;span class="s"&gt;path_ends_in_slash&lt;/span&gt; &lt;span class="s"&gt;!auth_header&lt;/span&gt;
    &lt;span class="c1"&gt;# Append trailing slash if necessary.&lt;/span&gt;
    &lt;span class="s"&gt;http-request&lt;/span&gt; &lt;span class="s"&gt;set-path&lt;/span&gt; &lt;span class="s"&gt;%[path]/index.html&lt;/span&gt; &lt;span class="s"&gt;if&lt;/span&gt; &lt;span class="s"&gt;!path_has_dot&lt;/span&gt; &lt;span class="s"&gt;!path_ends_in_slash&lt;/span&gt; &lt;span class="s"&gt;!auth_header&lt;/span&gt;
    &lt;span class="s"&gt;default_backend&lt;/span&gt; &lt;span class="s"&gt;ceph_back&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that what we're doing here is somewhat crude. We're assuming that
any actual file that we want to retrieve looks like &lt;code&gt;name.ext&lt;/code&gt;,
meaning it has a dot (period, full stop) character in it. The
&lt;code&gt;path_sub -i .&lt;/code&gt; expression in the &lt;code&gt;path_has_dot&lt;/code&gt; ACL simply matches
any path with &lt;code&gt;.&lt;/code&gt; in it, and we're assuming that if a path has a dot
then it points to a file, if it doesn't then it points to a directory.&lt;/p&gt;
&lt;p&gt;You could be a little more clever here and use &lt;code&gt;path_regex&lt;/code&gt; instead of
&lt;code&gt;path_sub&lt;/code&gt; for a full regular expression match. But regex lookups are
slower than simple substring matches, so if the substring match works
for you, go for it.&lt;/p&gt;
&lt;p&gt;So now, we can do this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;s3cmd put --acl-public index.html s3://testwebsite/my/sub/directory/
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And then:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Note omitted trailing slash&lt;/span&gt;
curl -I http://testwebsite.ceph.example.com/my/sub/directory
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kr"&gt;HTTP&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="m"&gt;1.1&lt;/span&gt; &lt;span class="m"&gt;200&lt;/span&gt; &lt;span class="ne"&gt;OK&lt;/span&gt;
&lt;span class="na"&gt;Content-Length&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="l"&gt;24235&lt;/span&gt;
&lt;span class="na"&gt;Accept-Ranges&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="l"&gt;bytes&lt;/span&gt;
&lt;span class="na"&gt;Last-Modified&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="l"&gt;Mon, 25 Jan 2016 23:57:04 GMT&lt;/span&gt;
&lt;span class="na"&gt;ETag&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="l"&gt;"fecd005b33c0f6bfdee61b787cf54cb0"&lt;/span&gt;
&lt;span class="na"&gt;x-amz-request-id&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="l"&gt;tx00000000000000000bc83-0056a7bd25-312cd-default&lt;/span&gt;
&lt;span class="na"&gt;Content-type&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="l"&gt;text/html&lt;/span&gt;
&lt;span class="na"&gt;Date&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="l"&gt;Tue, 26 Jan 2016 18:38:29 GMT&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;HTTPS support&lt;/h2&gt;
&lt;p&gt;So, what else might you want to do? One obvious thing that you can use
HAproxy for is SSL termination. The radosgw embedded &lt;code&gt;civetweb&lt;/code&gt;
webserver can do that for you, but that feature is &lt;a href="http://tracker.ceph.com/issues/11239"&gt;currently mildly
broken in a rather curious
way&lt;/a&gt;. So in order to allow HTTPS
access to all your content via HAproxy instead, you would add:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;frontend&lt;/span&gt; &lt;span class="s"&gt;ceph_front_ssl&lt;/span&gt;
    &lt;span class="s"&gt;bind&lt;/span&gt; &lt;span class="n"&gt;0.0.0.0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;443&lt;/span&gt; &lt;span class="s"&gt;ssl&lt;/span&gt; &lt;span class="s"&gt;crt&lt;/span&gt; &lt;span class="s"&gt;ceph.pem&lt;/span&gt; &lt;span class="s"&gt;no-sslv3&lt;/span&gt; &lt;span class="s"&gt;no-tls-tickets&lt;/span&gt;
    &lt;span class="s"&gt;reqadd&lt;/span&gt; &lt;span class="s"&gt;X-Forwarded-Proto:\&lt;/span&gt; &lt;span class="s"&gt;https&lt;/span&gt;
    &lt;span class="s"&gt;acl&lt;/span&gt; &lt;span class="s"&gt;path_has_dot&lt;/span&gt; &lt;span class="s"&gt;path_sub&lt;/span&gt; &lt;span class="s"&gt;-i&lt;/span&gt; &lt;span class="s"&gt;.&lt;/span&gt;
    &lt;span class="s"&gt;acl&lt;/span&gt; &lt;span class="s"&gt;path_ends_in_slash&lt;/span&gt; &lt;span class="s"&gt;path_end&lt;/span&gt; &lt;span class="s"&gt;-i&lt;/span&gt; &lt;span class="s"&gt;/&lt;/span&gt;
    &lt;span class="s"&gt;acl&lt;/span&gt; &lt;span class="s"&gt;auth_header&lt;/span&gt; &lt;span class="s"&gt;hdr(Authorization)&lt;/span&gt; &lt;span class="s"&gt;-m&lt;/span&gt; &lt;span class="s"&gt;found&lt;/span&gt;
    &lt;span class="s"&gt;http-request&lt;/span&gt; &lt;span class="s"&gt;set-path&lt;/span&gt; &lt;span class="s"&gt;%[path]index.html&lt;/span&gt; &lt;span class="s"&gt;if&lt;/span&gt; &lt;span class="s"&gt;path_ends_in_slash&lt;/span&gt; &lt;span class="s"&gt;!auth_header&lt;/span&gt;
    &lt;span class="s"&gt;http-request&lt;/span&gt; &lt;span class="s"&gt;set-path&lt;/span&gt; &lt;span class="s"&gt;%[path]/index.html&lt;/span&gt; &lt;span class="s"&gt;if&lt;/span&gt; &lt;span class="s"&gt;!path_has_dot&lt;/span&gt; &lt;span class="s"&gt;!path_ends_in_slash&lt;/span&gt; &lt;span class="s"&gt;!auth_header&lt;/span&gt;
    &lt;span class="s"&gt;default_backend&lt;/span&gt; &lt;span class="s"&gt;ceph_back&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;But maybe you'd like to &lt;strong&gt;force,&lt;/strong&gt; not merely allow, HTTPS
access. &lt;code&gt;redirect&lt;/code&gt; to the rescue:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;frontend&lt;/span&gt; &lt;span class="s"&gt;ceph_front&lt;/span&gt;
    &lt;span class="s"&gt;bind&lt;/span&gt; &lt;span class="n"&gt;0.0.0.0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;80&lt;/span&gt;
    &lt;span class="s"&gt;reqadd&lt;/span&gt; &lt;span class="s"&gt;X-Forwarded-Proto:\&lt;/span&gt; &lt;span class="s"&gt;http&lt;/span&gt;
    &lt;span class="s"&gt;redirect&lt;/span&gt; &lt;span class="s"&gt;scheme&lt;/span&gt; &lt;span class="s"&gt;https&lt;/span&gt; &lt;span class="s"&gt;code&lt;/span&gt; &lt;span class="mi"&gt;301&lt;/span&gt; &lt;span class="s"&gt;if&lt;/span&gt; &lt;span class="s"&gt;!&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="kn"&gt;ssl_fc&lt;/span&gt; &lt;span class="err"&gt;}&lt;/span&gt;

&lt;span class="s"&gt;frontend&lt;/span&gt; &lt;span class="s"&gt;ceph_front_ssl&lt;/span&gt;
    &lt;span class="s"&gt;bind&lt;/span&gt; &lt;span class="n"&gt;0.0.0.0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;443&lt;/span&gt; &lt;span class="s"&gt;ssl&lt;/span&gt; &lt;span class="s"&gt;crt&lt;/span&gt; &lt;span class="s"&gt;ceph.pem&lt;/span&gt; &lt;span class="s"&gt;no-sslv3&lt;/span&gt; &lt;span class="s"&gt;no-tls-tickets&lt;/span&gt;
    &lt;span class="s"&gt;reqadd&lt;/span&gt; &lt;span class="s"&gt;X-Forwarded-Proto:\&lt;/span&gt; &lt;span class="s"&gt;https&lt;/span&gt;
    &lt;span class="s"&gt;acl&lt;/span&gt; &lt;span class="s"&gt;path_has_dot&lt;/span&gt; &lt;span class="s"&gt;path_sub&lt;/span&gt; &lt;span class="s"&gt;-i&lt;/span&gt; &lt;span class="s"&gt;.&lt;/span&gt;
    &lt;span class="s"&gt;acl&lt;/span&gt; &lt;span class="s"&gt;path_ends_in_slash&lt;/span&gt; &lt;span class="s"&gt;path_end&lt;/span&gt; &lt;span class="s"&gt;-i&lt;/span&gt; &lt;span class="s"&gt;/&lt;/span&gt;
    &lt;span class="s"&gt;acl&lt;/span&gt; &lt;span class="s"&gt;auth_header&lt;/span&gt; &lt;span class="s"&gt;hdr(Authorization)&lt;/span&gt; &lt;span class="s"&gt;-m&lt;/span&gt; &lt;span class="s"&gt;found&lt;/span&gt;
    &lt;span class="s"&gt;http-request&lt;/span&gt; &lt;span class="s"&gt;set-path&lt;/span&gt; &lt;span class="s"&gt;%[path]index.html&lt;/span&gt; &lt;span class="s"&gt;if&lt;/span&gt; &lt;span class="s"&gt;path_ends_in_slash&lt;/span&gt; &lt;span class="s"&gt;!auth_header&lt;/span&gt;
    &lt;span class="s"&gt;http-request&lt;/span&gt; &lt;span class="s"&gt;set-path&lt;/span&gt; &lt;span class="s"&gt;%[path]/index.html&lt;/span&gt; &lt;span class="s"&gt;if&lt;/span&gt; &lt;span class="s"&gt;!path_has_dot&lt;/span&gt; &lt;span class="s"&gt;!path_ends_in_slash&lt;/span&gt; &lt;span class="s"&gt;!auth_header&lt;/span&gt;
    &lt;span class="s"&gt;default_backend&lt;/span&gt; &lt;span class="s"&gt;ceph_back&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And here we go:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Note HTTP&lt;/span&gt;
curl -IL http://testwebsite.ceph.example.com/my/sub/directory
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kr"&gt;HTTP&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="m"&gt;1.1&lt;/span&gt; &lt;span class="m"&gt;301&lt;/span&gt; &lt;span class="ne"&gt;Moved Permanently&lt;/span&gt;
&lt;span class="na"&gt;Content-length&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="l"&gt;0&lt;/span&gt;
&lt;span class="na"&gt;Location&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="l"&gt;https://testwebsite.ceph.example.com/my/sub/directory&lt;/span&gt;
&lt;span class="na"&gt;Connection&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="l"&gt;close&lt;/span&gt;

HTTP/1.1 200 OK
Content-Length: 24235
Accept-Ranges: bytes
Last-Modified: Mon, 25 Jan 2016 23:57:04 GMT
ETag: "fecd005b33c0f6bfdee61b787cf54cb0"
x-amz-request-id: tx00000000000000000bdeb-0056a7bf9b-312cd-default
Content-type: text/html
Date: Tue, 26 Jan 2016 18:48:59 GMT
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Compression&lt;/h2&gt;
&lt;p&gt;And finally, maybe you'd like to speed up access to the stuff on your
site. Why not add gzip on-the-fly-compression? It's supported by every
browser worth its salt, and will make your users happier. You'll want
to restrict compression to specific MIME types though. In the
configuration below, we enable compression for plain text, HTML, XML,
CSS, JavaScript, and SVG images.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;frontend&lt;/span&gt; &lt;span class="s"&gt;ceph_front&lt;/span&gt;
    &lt;span class="s"&gt;bind&lt;/span&gt; &lt;span class="n"&gt;0.0.0.0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;80&lt;/span&gt;
    &lt;span class="s"&gt;reqadd&lt;/span&gt; &lt;span class="s"&gt;X-Forwarded-Proto:\&lt;/span&gt; &lt;span class="s"&gt;http&lt;/span&gt;
    &lt;span class="s"&gt;redirect&lt;/span&gt; &lt;span class="s"&gt;scheme&lt;/span&gt; &lt;span class="s"&gt;https&lt;/span&gt; &lt;span class="s"&gt;code&lt;/span&gt; &lt;span class="mi"&gt;301&lt;/span&gt; &lt;span class="s"&gt;if&lt;/span&gt; &lt;span class="s"&gt;!&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="kn"&gt;ssl_fc&lt;/span&gt; &lt;span class="err"&gt;}&lt;/span&gt;

&lt;span class="s"&gt;frontend&lt;/span&gt; &lt;span class="s"&gt;ceph_front_ssl&lt;/span&gt;
    &lt;span class="s"&gt;bind&lt;/span&gt; &lt;span class="n"&gt;0.0.0.0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;443&lt;/span&gt; &lt;span class="s"&gt;ssl&lt;/span&gt; &lt;span class="s"&gt;crt&lt;/span&gt; &lt;span class="s"&gt;ceph.pem&lt;/span&gt; &lt;span class="s"&gt;no-sslv3&lt;/span&gt; &lt;span class="s"&gt;no-tls-tickets&lt;/span&gt;
    &lt;span class="s"&gt;reqadd&lt;/span&gt; &lt;span class="s"&gt;X-Forwarded-Proto:\&lt;/span&gt; &lt;span class="s"&gt;https&lt;/span&gt;
    &lt;span class="s"&gt;acl&lt;/span&gt; &lt;span class="s"&gt;path_has_dot&lt;/span&gt; &lt;span class="s"&gt;path_sub&lt;/span&gt; &lt;span class="s"&gt;-i&lt;/span&gt; &lt;span class="s"&gt;.&lt;/span&gt;
    &lt;span class="s"&gt;acl&lt;/span&gt; &lt;span class="s"&gt;path_ends_in_slash&lt;/span&gt; &lt;span class="s"&gt;path_end&lt;/span&gt; &lt;span class="s"&gt;-i&lt;/span&gt; &lt;span class="s"&gt;/&lt;/span&gt;
    &lt;span class="s"&gt;acl&lt;/span&gt; &lt;span class="s"&gt;auth_header&lt;/span&gt; &lt;span class="s"&gt;hdr(Authorization)&lt;/span&gt; &lt;span class="s"&gt;-m&lt;/span&gt; &lt;span class="s"&gt;found&lt;/span&gt;
    &lt;span class="s"&gt;http-request&lt;/span&gt; &lt;span class="s"&gt;set-path&lt;/span&gt; &lt;span class="s"&gt;%[path]index.html&lt;/span&gt; &lt;span class="s"&gt;if&lt;/span&gt; &lt;span class="s"&gt;path_ends_in_slash&lt;/span&gt; &lt;span class="s"&gt;!auth_header&lt;/span&gt;
    &lt;span class="s"&gt;http-request&lt;/span&gt; &lt;span class="s"&gt;set-path&lt;/span&gt; &lt;span class="s"&gt;%[path]/index.html&lt;/span&gt; &lt;span class="s"&gt;if&lt;/span&gt; &lt;span class="s"&gt;!path_has_dot&lt;/span&gt; &lt;span class="s"&gt;!path_ends_in_slash&lt;/span&gt; &lt;span class="s"&gt;!auth_header&lt;/span&gt;
    &lt;span class="s"&gt;compression&lt;/span&gt; &lt;span class="s"&gt;algo&lt;/span&gt; &lt;span class="s"&gt;gzip&lt;/span&gt;
    &lt;span class="s"&gt;compression&lt;/span&gt; &lt;span class="s"&gt;type&lt;/span&gt; &lt;span class="s"&gt;text/html&lt;/span&gt; &lt;span class="s"&gt;text/xml&lt;/span&gt; &lt;span class="s"&gt;text/plain&lt;/span&gt; &lt;span class="s"&gt;text/css&lt;/span&gt; &lt;span class="s"&gt;application/javascript&lt;/span&gt; &lt;span class="s"&gt;image/svg+xml&lt;/span&gt;
    &lt;span class="s"&gt;default_backend&lt;/span&gt; &lt;span class="s"&gt;ceph_back&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Let's see how that helps us. Do a request without gzip encoding
support, and observe that its total download size matches the
document's &lt;code&gt;Content-Length&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;curl https://testwebsite.ceph.example.com/my/sub/directory &amp;gt; /dev/null
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  &lt;span class="c1"&gt;% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current&lt;/span&gt;
                                 &lt;span class="nv"&gt;Dload&lt;/span&gt;  &lt;span class="nv"&gt;Upload&lt;/span&gt;   &lt;span class="nv"&gt;Total&lt;/span&gt;   &lt;span class="nv"&gt;Spent&lt;/span&gt;    &lt;span class="nv"&gt;Left&lt;/span&gt;  &lt;span class="nv"&gt;Speed&lt;/span&gt;
&lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="mi"&gt;24235&lt;/span&gt;  &lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="mi"&gt;24235&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;     &lt;span class="mi"&gt;0&lt;/span&gt;  &lt;span class="mi"&gt;94565&lt;/span&gt;      &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="s s-Atom"&gt;--:--:--&lt;/span&gt; &lt;span class="s s-Atom"&gt;--:--:--&lt;/span&gt; &lt;span class="s s-Atom"&gt;--:--:--&lt;/span&gt; &lt;span class="mi"&gt;94299&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, add an &lt;code&gt;Accept-Encoding&lt;/code&gt; header:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;curl -H &lt;span class="s1"&gt;'Accept-Encoding: gzip'&lt;/span&gt; https://testwebsite.ceph.example.com/my/sub/directory &amp;gt; /dev/null
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  &lt;span class="c1"&gt;% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current&lt;/span&gt;
                                 &lt;span class="nv"&gt;Dload&lt;/span&gt;  &lt;span class="nv"&gt;Upload&lt;/span&gt;   &lt;span class="nv"&gt;Total&lt;/span&gt;   &lt;span class="nv"&gt;Spent&lt;/span&gt;    &lt;span class="nv"&gt;Left&lt;/span&gt;  &lt;span class="nv"&gt;Speed&lt;/span&gt;
&lt;span class="mi"&gt;100&lt;/span&gt;  &lt;span class="mi"&gt;5237&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;  &lt;span class="mi"&gt;5237&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;     &lt;span class="mi"&gt;0&lt;/span&gt;  &lt;span class="mi"&gt;19243&lt;/span&gt;      &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="s s-Atom"&gt;--:--:--&lt;/span&gt; &lt;span class="s s-Atom"&gt;--:--:--&lt;/span&gt; &lt;span class="s s-Atom"&gt;--:--:--&lt;/span&gt; &lt;span class="mi"&gt;19324&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;There. Actual download size goes from 24KB down to just 5KB. &lt;/p&gt;
&lt;h2&gt;Where to go from here&lt;/h2&gt;
&lt;p&gt;There's a few additional features to be added here. You
could enable CORS or HSTS, for example, and of course you could add
more backends. But if you read this far, you surely get the idea.&lt;/p&gt;
&lt;p&gt;And you're welcome to examine the headers you can pull from this page
you're reading, wink wink. :)&lt;/p&gt;</content><category term="Ceph"></category></entry><entry><title>My first Open edX contribution</title><link href="fghaas.github.io/xahteiwi.eu/blog/2016/01/05/my-first-open-edx-contribution/index.html" rel="alternate"></link><published>2016-01-05T00:00:00+00:00</published><updated>2016-01-05T00:00:00+00:00</updated><author><name>florian</name></author><id>tag:None,2016-01-05:fghaas.github.io/xahteiwi.eu/blog/2016/01/05/my-first-open-edx-contribution/index.html</id><summary type="html">&lt;p&gt;In which Florian talks about landing his first patch in Open edX.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've finally submitted my first code contribution to &lt;a href="https://open.edx.org/"&gt;Open
edX&lt;/a&gt;, a trivial patch for an annoying issue in
the LMS start page. The PR is
&lt;a href="https://github.com/edx/edx-platform/pull/11138"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The LMS component in Open edX is the stuff that actually provides a
learning platform to students, including the courseware itself, a
discussion forum, a wiki, and everything else you need for an
immersive learning experience. In our own &lt;a href="//academy.hastexo.com"&gt;hastexo
Academy&lt;/a&gt; environment, it of course also loads
the &lt;a href="//github.com/hastexo/hastexo-xblock"&gt;hastexo XBlock&lt;/a&gt; to interface
with arbitrarily complex, on-demand lab environments.&lt;/p&gt;
&lt;p&gt;If you want to know how the LMS fits into Open edX overall, there's
&lt;a href="//open.edx.org/sites/default/files/wysiwyg/open-edx-pages/edX_architecture_CMS_LMS_0.png" title="Open edX architecture diagram"&gt;an overview
graphic&lt;/a&gt; over at
&lt;a href="//open.edx.org"&gt;open.edx.org&lt;/a&gt; for your perusal.&lt;/p&gt;
&lt;p&gt;Being a new contributor to Open edX, this obviously involves jumping
through &lt;a href="https://github.com/edx/edx-platform/blob/master/CONTRIBUTING.rst"&gt;yet another Contributor Agreement
process&lt;/a&gt;. Here's
to hoping this gets resolved quickly.&lt;/p&gt;
&lt;h3&gt;Update, 2016-02-03&lt;/h3&gt;
&lt;p&gt;The contributor agreement was &lt;a href="//github.com/edx/edx-platform/pull/11138#issuecomment-168964638"&gt;squared away really
fast&lt;/a&gt;;
the patch review did, however, take some time. &lt;a href="//github.com/edx/edx-platform/commit/71a6779dfa44baa27d9c2b509587385edb4380af"&gt;But it's in
now&lt;/a&gt;.&lt;/p&gt;</content><category term="Open edX"></category></entry><entry><title>Removing buckets in radosgw (and their contents)</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/removing-buckets-in-radosgw-and-their-contents/index.html" rel="alternate"></link><published>2015-12-23T11:34:34+01:00</published><updated>2015-12-23T11:34:34+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2015-12-23:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/removing-buckets-in-radosgw-and-their-contents/index.html</id><summary type="html">&lt;p&gt;Every once in a while you'll want to remove a bucket in radosgw,
including all the objects contained in that bucket.&lt;/p&gt;
&lt;p&gt;Now you might use a utility like &lt;a href="http://s3tools.org/s3cmd"&gt;&lt;code&gt;s3cmd&lt;/code&gt;&lt;/a&gt;
for that purpose:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;s3cmd rb --recursive s3://mybucket
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The advantage to this approach is that your users can do it, using …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Every once in a while you'll want to remove a bucket in radosgw,
including all the objects contained in that bucket.&lt;/p&gt;
&lt;p&gt;Now you might use a utility like &lt;a href="http://s3tools.org/s3cmd"&gt;&lt;code&gt;s3cmd&lt;/code&gt;&lt;/a&gt;
for that purpose:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;s3cmd rb --recursive s3://mybucket
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The advantage to this approach is that your users can do it, using
just the regular S3 API. But this approach may be slow, particularly
if you have previously created your objects with &lt;code&gt;rest-bench&lt;/code&gt;,
&lt;code&gt;cosbench&lt;/code&gt;, or another benchmarking tool.&lt;/p&gt;
&lt;p&gt;So in the event that you want to remove buckets, and their objects,
directly from &lt;code&gt;radosgw&lt;/code&gt;, you can do so with&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;radosgw-admin bucket rm --bucket=mybucket --purge-objects
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is usually the faster approach.&lt;/p&gt;
&lt;p&gt;If, at any time, you want to nuke all buckets owned by a particular
user, there is a command for that, as well. Use this one with care:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;radosgw-admin user rm --uid=[username] --purge-data
&lt;/pre&gt;&lt;/div&gt;</content><category term="Ceph"></category></entry><entry><title>A minimal Ubuntu OpenStack Juju configuration in just four nodes</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/ubuntu-openstack-juju-4-nodes/index.html" rel="alternate"></link><published>2015-12-23T00:00:00+00:00</published><updated>2015-12-23T00:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2015-12-23:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/ubuntu-openstack-juju-4-nodes/index.html</id><summary type="html">&lt;p&gt;Juju is Ubuntu's supported and preferred means of deployment
automation for an OpenStack cloud. While in Juju, a deployment unit (a
&lt;em&gt;Juju charm&lt;/em&gt;) generally expects to fully own the filesystem it is
being deployed on, Juju allows you to co-deploy charms on the same
physical machines, by way of using …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Juju is Ubuntu's supported and preferred means of deployment
automation for an OpenStack cloud. While in Juju, a deployment unit (a
&lt;em&gt;Juju charm&lt;/em&gt;) generally expects to fully own the filesystem it is
being deployed on, Juju allows you to co-deploy charms on the same
physical machines, by way of using LXC containers.&lt;/p&gt;
&lt;p&gt;Now in general, Juju should allow you to deploy complex service
&lt;em&gt;bundles&lt;/em&gt; in one swoop, however this works best when deploying to the
bare metal (i.e. without containers). Still, it is perfectly possible
to automate Juju deployment of an entire OpenStack cloud in just 4
physical nodes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A controller node (running your OpenStack APIs and your dashboard);&lt;/li&gt;
&lt;li&gt;a compute node (running VMs under libvirt/KVM management);&lt;/li&gt;
&lt;li&gt;a network gateway node (providing L3 network connectivity);&lt;/li&gt;
&lt;li&gt;a storage node (providing Cinder volumes via iSCSI and LVM).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The assumption for the setup below is that you already have a Juju
infrastructure in place. You may have set this up with MAAS, or you
may have just bootstrapped a deployment node and then created a Juju
&lt;code&gt;manual&lt;/code&gt; environment and added your 4 nodes via SSH.&lt;/p&gt;
&lt;p&gt;Note that the environment described here should not be used for
production purposes. However, the same approach is also applicable to
a 3-node controller HA cluster, 2-node Neutron gateway cluster with
support for HA routers, and as many converged Ceph/&lt;code&gt;nova-compute&lt;/code&gt;
nodes as you want.&lt;/p&gt;
&lt;h2&gt;Juju configuration&lt;/h2&gt;
&lt;p&gt;Consider the following Juju configuration YAML example, which you
might put into your home directory as &lt;code&gt;juju-config.yaml&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;keystone&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;openstack-origin&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;'cloud:trusty-liberty'&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;admin-password&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;'my&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;very&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;secret&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;password'&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;nova-cloud-controller&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;openstack-origin&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;'cloud:trusty-liberty'&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;network-manager&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;Neutron&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;neutron-gateway&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;openstack-origin&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;'cloud:trusty-liberty'&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;ext-port&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;eth2&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;bridge-mappings&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;'external:br-ex'&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;os-data-network&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;192.168.133.0/24&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;instance-mtu&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;1400&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;neutron-api&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;openstack-origin&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;'cloud:trusty-liberty'&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;network-device-mtu&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;1400&lt;/span&gt;
  &lt;span class="c1"&gt;# Always make sure you enable security groups&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;neutron-security-groups&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;true&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;overlay-network-type&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;vxlan&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;rabbitmq-server&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
&lt;span class="c1"&gt;# Cinder is deployed in two parts: one for the API and scheduler&lt;/span&gt;
&lt;span class="c1"&gt;# (which can live in a container), one for the volume service (which&lt;/span&gt;
&lt;span class="c1"&gt;# cannot, at least not for the LVM/iSCSI backend)&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;cinder-api&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;openstack-origin&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;'cloud:trusty-liberty'&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;enabled-services&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;api,scheduler&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;cinder-volume&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;openstack-origin&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;'cloud:trusty-liberty'&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;enabled-services&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;volume&lt;/span&gt;
  &lt;span class="c1"&gt;# Adjust this to match the block device on your volume host&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;block-device&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;vdb&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;glance&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;openstack-origin&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;'cloud:trusty-liberty'&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;heat&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;openstack-origin&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;'cloud:trusty-liberty'&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;mysql&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;openstack-dashboard&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;openstack-origin&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;'cloud:trusty-liberty'&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;webroot&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;/&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;nova-compute&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;openstack-origin&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="s"&gt;'cloud:trusty-liberty'&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;manage-neutron-plugin-legacy-mode&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;false&lt;/span&gt;
  &lt;span class="c1"&gt;# Change to qemu if in a nested cloud environment&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;virt-type&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;kvm&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;neutron-openvswitch&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;
  &lt;span class="l l-Scalar l-Scalar-Plain"&gt;os-data-network&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;192.168.133.0/24&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Deployment&lt;/h2&gt;
&lt;p&gt;Then, you can run the following shell script to deploy your control
services to LXC containers on machine 1, &lt;code&gt;nova-compute&lt;/code&gt; (and its
subordinate charm, &lt;code&gt;neutron-openvswitch&lt;/code&gt;) to machine 2,
&lt;code&gt;neutron-gateway&lt;/code&gt; to machine 3, and &lt;code&gt;cinder-volume&lt;/code&gt; to machine 4.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/bin/bash -ex&lt;/span&gt;

&lt;span class="nv"&gt;CONFIG&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;~/juju-config.yaml

juju deploy --config&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$CONFIG&lt;/span&gt; mysql --to lxc:1
juju deploy --config&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$CONFIG&lt;/span&gt; rabbitmq-server --to lxc:1

sleep 120s

juju deploy --config&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$CONFIG&lt;/span&gt; keystone --to lxc:1
juju add-relation keystone:shared-db mysql:shared-db

juju deploy --config&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$CONFIG&lt;/span&gt; glance --to lxc:1
juju add-relation glance:identity-service keystone:identity-service
juju add-relation glance:shared-db mysql:shared-db

juju deploy --config&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$CONFIG&lt;/span&gt; neutron-api --to lxc:1
juju add-relation neutron-api:amqp rabbitmq-server:amqp
juju add-relation neutron-api:identity-service keystone:identity-service
juju add-relation neutron-api:shared-db mysql:shared-db

juju deploy --config&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$CONFIG&lt;/span&gt; neutron-gateway --to &lt;span class="m"&gt;3&lt;/span&gt;
juju add-relation neutron-gateway:amqp rabbitmq-server:amqp
juju add-relation neutron-gateway:neutron-plugin-api neutron-api:neutron-plugin-api
juju add-relation neutron-gateway:shared-db mysql:shared-db

juju deploy --config&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$CONFIG&lt;/span&gt; nova-cloud-controller --to lxc:1
juju add-relation nova-cloud-controller:amqp rabbitmq-server:amqp
juju add-relation nova-cloud-controller:identity-service keystone:identity-service
juju add-relation nova-cloud-controller:image-service glance:image-service
juju add-relation nova-cloud-controller:neutron-api neutron-api:neutron-api
juju add-relation nova-cloud-controller:shared-db mysql:shared-db

juju deploy --config&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$CONFIG&lt;/span&gt; nova-compute --to &lt;span class="m"&gt;2&lt;/span&gt;
juju add-relation nova-compute:amqp rabbitmq-server:amqp
juju add-relation nova-compute:cloud-compute nova-cloud-controller:cloud-compute
juju add-relation nova-compute:image-service glance:image-service
juju add-relation nova-compute:shared-db mysql:shared-db

juju deploy --config&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$CONFIG&lt;/span&gt; neutron-openvswitch
juju add-relation neutron-openvswitch:amqp rabbitmq-server:amqp
juju add-relation neutron-openvswitch:neutron-plugin-api neutron-api:neutron-plugin-api
juju add-relation neutron-openvswitch:neutron-plugin nova-compute:neutron-plugin 
juju deploy --config&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$CONFIG&lt;/span&gt; cinder cinder-api --to lxc:1
juju add-relation cinder-api:amqp rabbitmq-server:amqp
juju add-relation cinder-api:cinder-volume-service nova-cloud-controller:cinder-volume-service
juju add-relation cinder-api:identity-service keystone:identity-service
juju add-relation cinder-api:image-service glance:image-service
juju add-relation cinder-api:shared-db mysql:shared-db

juju deploy --config&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$CONFIG&lt;/span&gt; cinder cinder-volume --to &lt;span class="m"&gt;4&lt;/span&gt;
juju add-relation cinder-volume:amqp rabbitmq-server:amqp
juju add-relation cinder-volume:shared-db mysql:shared-db
juju add-relation cinder-volume:image-service glance:image-service

juju deploy --config&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$CONFIG&lt;/span&gt; openstack-dashboard --to &lt;span class="m"&gt;1&lt;/span&gt;
juju add-relation openstack-dashboard:identity-service keystone:identity-service

juju deploy --config&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$CONFIG&lt;/span&gt; heat --to lxc:1
juju add-relation heat:amqp rabbitmq-server:amqp
juju add-relation heat:identity-service keystone:identity-service
juju add-relation heat:shared-db mysql:shared-db
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And you're done! The whole process should give you an OpenStack cloud
in about 20-30 minutes.&lt;/p&gt;
&lt;p&gt;By the way, an exceedingly useful command to watch the installation progress of your Juju environment is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;watch "juju stat --format=tabular"
&lt;/pre&gt;&lt;/div&gt;</content><category term="OpenStack"></category><category term="Juju"></category></entry><entry><title>A Python one-liner for pretty-printing radosgw utilization</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/radosgw-utilization-one-liner/index.html" rel="alternate"></link><published>2015-12-17T00:00:00+00:00</published><updated>2015-12-17T00:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2015-12-17:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/radosgw-utilization-one-liner/index.html</id><summary type="html">&lt;p&gt;In case you need a quick overview of how many radosgw objects live in
your Ceph cluster, your first step is normally this command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;radosgw-admin bucket stats
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When used &lt;em&gt;without&lt;/em&gt; the &lt;code&gt;--bucket=&amp;lt;name&amp;gt;&lt;/code&gt; argument, this command lists
a bunch of statistics for &lt;em&gt;all&lt;/em&gt; your radosgw buckets, in a somewhat
convoluted …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In case you need a quick overview of how many radosgw objects live in
your Ceph cluster, your first step is normally this command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;radosgw-admin bucket stats
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When used &lt;em&gt;without&lt;/em&gt; the &lt;code&gt;--bucket=&amp;lt;name&amp;gt;&lt;/code&gt; argument, this command lists
a bunch of statistics for &lt;em&gt;all&lt;/em&gt; your radosgw buckets, in a somewhat
convoluted JSON format. If you only want a simple list of all your
buckets and the number of objects they contain, you can use the
following bit of Python list comprehension magic:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;radosgw-admin bucket stats &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
  python -c &lt;span class="s1"&gt;'import json; import sys; print "\n".join(["%s: %s" % (str(x["bucket"]), ", ".join(["%s: %s" % (k, v["num_objects"]) for k,v in x["usage"].iteritems()])) for x in json.load(sys.stdin) if isinstance(x,dict)])'&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And while the above is all on one line so you can easily copy and
paste, here are the Python bits in a slightly more legible format:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;

&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stdin&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"bucket"&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
                             &lt;span class="s2"&gt;", "&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                    &lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"num_objects"&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
                                        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"usage"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iteritems&lt;/span&gt;&lt;span class="p"&gt;()]))&lt;/span&gt;
                 &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;
                 &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Of course, you'll need to substitute &lt;code&gt;print()&lt;/code&gt; for &lt;code&gt;print&lt;/code&gt; if your
system runs only Python 3.&lt;/p&gt;</content><category term="Ceph"></category><category term="Python"></category></entry><entry><title>Understanding radosgw benchmarks</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/understanding-radosgw-benchmarks/index.html" rel="alternate"></link><published>2015-11-18T14:01:42+01:00</published><updated>2015-11-18T14:01:42+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2015-11-18:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/understanding-radosgw-benchmarks/index.html</id><summary type="html">&lt;p&gt;We've noticed that there are a few common misconceptions around
radosgw performance, and we're hoping that this post can clear up some
of those.&lt;/p&gt;
&lt;p&gt;radosgw is of course Ceph's RESTful object gateway. That means that
you can use any client that speaks the Amazon S3 or OpenStack Swift
protocol to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We've noticed that there are a few common misconceptions around
radosgw performance, and we're hoping that this post can clear up some
of those.&lt;/p&gt;
&lt;p&gt;radosgw is of course Ceph's RESTful object gateway. That means that
you can use any client that speaks the Amazon S3 or OpenStack Swift
protocol to interact with your Ceph cluster. Since RESTful object
access is HTTP based, this also means you can combine radosgw with
HTTP load balancers, reverse proxies and the like, which often comes
in handy.&lt;/p&gt;
&lt;p&gt;In general, as any RESTful object storage, you would generally store
data in radosgw that you read and write in one chunk, and where bulk
storage is more important than online availability (if you need data
at your fingertips, you'd use RBD or CephFS or even straight-up RADOS
instead, but those are for different use cases).&lt;/p&gt;
&lt;p&gt;The performance implications of using radosgw (or any RESTful object
storage, for that matter) usually apply to one of two different use
cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Either you want to store lots of data in bulk, and come back to it
  later. This, for example, is why in OpenStack backups of volumes and
  databases typically go to OpenStack Swift or radosgw speaking the
  Swift protocol.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Or you want to store lots of relatively small data chunks really
  fast. Suppose you have a monitoring system storing data points in
  S3.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So either you want to write big chunks of data, in which case you're
interested in throughput (typically measured in amount of data per
unit time, such as MB/s). Or you want to write small chunks, then
what's important is completed operations per unit time (typically
measured in number of writes per second, which in the RESTful case
would be HTTP PUTs per second).&lt;/p&gt;
&lt;p&gt;Now with radosgw, you can measure this with a handy tool called
rest-bench. Sadly rest-bench no longer builds with Ceph for Infernalis
and later, because the Ceph developers now favor Intel's COSbench
utility. But rest-bench from older Ceph releases will be around for a
while and it's handy because unlike COSbench, it doesn't require Java.&lt;/p&gt;
&lt;p&gt;So let's take a look. The general invocation for rest-bench is like
this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;rest-bench -t &lt;span class="nv"&gt;$CONCURRENCY&lt;/span&gt; -b &lt;span class="nv"&gt;$SIZE&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
  --seconds&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$SECS&lt;/span&gt; --api-host&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$RGW&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
  --bucket&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$BUCKET&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
  --access-key&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$KEY&lt;/span&gt; --secret&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$SECRET&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
  --no-cleanup write
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;What does that mean?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;$CONCURRENCY&lt;/code&gt; is the number of concurrently running PUT
  operations. Basically, this is how many clients you want to
  simulate. The default is 16.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;$SIZE&lt;/code&gt; is the size of an individual object being written. The default
  here is 4MB.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;$RGW&lt;/code&gt; is of course your radosgw host including a port number.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;$SECS&lt;/code&gt; is the number of seconds to run the benchmark. The default is
  60, but in order to get a quick idea of your radosgw performance, as
  little as 10 is often sufficient.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;$BUCKET&lt;/code&gt; is the scratch bucket where you're creating objects during
  the benchmark run.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;$ACCESS_KEY&lt;/code&gt; and &lt;code&gt;$SECRET&lt;/code&gt; are the access and secret keys you created
  with &lt;code&gt;radosgw-admin user create&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;write&lt;/code&gt; specifies a random write benchmark.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;--no-cleanup&lt;/code&gt; specifies that you don't want the bucket to be
    cleaned out after the benchmark run. It's normally fine to run
    several benchmarks in a row and only empty the benchmark bucket
    when done with all.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Object size&lt;/h2&gt;
&lt;p&gt;First, we'll examine how object size affects radosgw throughput and
latency.&lt;/p&gt;
&lt;p&gt;So let's start out with a benchmark run that uses the default settings
for concurrency and object sizes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;RGW&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;localhost:7480
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;SECS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;10&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;SIZE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$((&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&amp;lt;&amp;lt;&lt;span class="m"&gt;22&lt;/span&gt;&lt;span class="k"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# 4MB object size&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;BUCKET&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;bench
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;CONCURRENCY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;16&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;KEY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;your_radosgw_key
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;SECRET&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;your_radosgw_secret

rest-bench -t &lt;span class="nv"&gt;$CONCURRENCY&lt;/span&gt; -b &lt;span class="nv"&gt;$SIZE&lt;/span&gt; --seconds&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$SECS&lt;/span&gt; --api-host&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$RGW&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--bucket&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$BUCKET&lt;/span&gt; --access-key&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$KEY&lt;/span&gt; --secret&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$SECRET&lt;/span&gt; --no-cleanup write
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;host=localhost:7480
Maintaining 16 concurrent writes of 4194304 bytes for up to 10 seconds or 0 objects
[...]
Total time run:         10.312134
Total writes made:      399
Write size:             4194304
Bandwidth (MB/sec):     154.769 
[...]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So that means we achieved a bandwidth of just under 155 MB/s (which is
near the max RADOS bandwidth this particular cluster is capable of;
it's by no means a high-end system) and we managed 399 writes, or
approx. 40 PUTs/s.&lt;/p&gt;
&lt;p&gt;Let's see how going even bigger changes things:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;SIZE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$((&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&amp;lt;&amp;lt;&lt;span class="m"&gt;26&lt;/span&gt;&lt;span class="k"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# 64MB object size&lt;/span&gt;

$ rest-bench -t &lt;span class="nv"&gt;$CONCURRENCY&lt;/span&gt; -b &lt;span class="nv"&gt;$SIZE&lt;/span&gt; --seconds&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$SECS&lt;/span&gt; --api-host&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$RGW&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--bucket&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$BUCKET&lt;/span&gt; --access-key&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$KEY&lt;/span&gt; --secret&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$SECRET&lt;/span&gt; --no-cleanup write
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;host=localhost:7480
 Maintaining 16 concurrent writes of 67108864 bytes for up to 10 seconds or 0 objects
[...]
 Total time run:         13.959088
Total writes made:      35
Write size:             67108864
Bandwidth (MB/sec):     160.469
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Perfectly logical. Our bandwidth doesn't change much, but of course
the number of PUTs we get done per second decreases significantly, to
a puny 3 PUTs/s. (Note: radosgw does break down objects into smaller
chunks when it talks to RADOS. However, this doesn't change the fact
that a client needs to haul a 64MB object across the network and
through the radosgw HTTP server.)&lt;/p&gt;
&lt;p&gt;Let's do the opposite now, and go for smaller objects. Suppose your
application is using a typical object size of 32K.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;SIZE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$((&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&amp;lt;&amp;lt;&lt;span class="m"&gt;15&lt;/span&gt;&lt;span class="k"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# 32KB object size&lt;/span&gt;
rest-bench -t &lt;span class="nv"&gt;$CONCURRENCY&lt;/span&gt; -b &lt;span class="nv"&gt;$SIZE&lt;/span&gt; --seconds&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$SECS&lt;/span&gt; --api-host&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$RGW&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--bucket&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$BUCKET&lt;/span&gt; --access-key&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$KEY&lt;/span&gt; --secret&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$SECRET&lt;/span&gt; --no-cleanup write
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;host=localhost:7480
 Maintaining 16 concurrent writes of 32768 bytes for up to 10 seconds or 0 objects
[...]
 Total time run:         10.042325
Total writes made:      2965
Write size:             32768
Bandwidth (MB/sec):     9.227
[...]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Of course if we looked at our bandwidth alone, this would be an
abysmal result. But your application is trying to write 32K chunks,
and lots of them. And it's succeeding just fine; we're now near 300
PUTs/s.&lt;/p&gt;
&lt;p&gt;Going even smaller, we'd expect PUTs/s to trend further up and nominal
MB/s to go down. Let's try with 4K objects:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;SIZE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$((&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&amp;lt;&amp;lt;&lt;span class="m"&gt;12&lt;/span&gt;&lt;span class="k"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# 4KB object size&lt;/span&gt;
rest-bench -t &lt;span class="nv"&gt;$CONCURRENCY&lt;/span&gt; -b &lt;span class="nv"&gt;$SIZE&lt;/span&gt; --seconds&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$SECS&lt;/span&gt; --api-host&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$RGW&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--bucket&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$BUCKET&lt;/span&gt; --access-key&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$KEY&lt;/span&gt; --secret&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$SECRET&lt;/span&gt; --no-cleanup write
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;host=localhost:7480
 Maintaining 16 concurrent writes of 4096 bytes for up to 10 seconds or 0 objects
[...]
Total time run:         10.052134
Total writes made:      3249
Write size:             4096
Bandwidth (MB/sec):     1.263 
[...]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And sure enough, 325 PUTs/s.&lt;/p&gt;
&lt;p&gt;So in summary, larger object sizes increase your write bandwidth to
your radosgw cluster, while smaller objects enable a higher
writes-per-second load.&lt;/p&gt;
&lt;h2&gt;Concurrency&lt;/h2&gt;
&lt;p&gt;Another aspect that influences your radosgw performance is
concurrency. Generally, the principle is simple: if you have multiple
parallel applications that write to radosgw and that don't have to
wait for each other, your aggregate throughput will be higher, and
your writes-per-second will be higher as well. If you have a small
number (in the worst case, a single one that is single-threaded) and
you can only ever issue one PUT at a time, both throughput and
writes-per-second will be lower in aggregate.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;RGW&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;localhost:7480
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;SECS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;10&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;SIZE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$((&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&amp;lt;&amp;lt;&lt;span class="m"&gt;22&lt;/span&gt;&lt;span class="k"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# back to 4MB object size&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;BUCKET&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;bench
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;CONCURRENCY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;16&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;KEY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&amp;lt;key&amp;gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;SECRET&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&amp;lt;secret&amp;gt;

$ rest-bench -t &lt;span class="nv"&gt;$CONCURRENCY&lt;/span&gt; -b &lt;span class="nv"&gt;$SIZE&lt;/span&gt; --seconds&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$SECS&lt;/span&gt; --api-host&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$RGW&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--bucket&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$BUCKET&lt;/span&gt; --access-key&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$KEY&lt;/span&gt; --secret&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$SECRET&lt;/span&gt; --no-cleanup write
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;host=localhost:7480
 Maintaining 16 concurrent writes of 4194304 bytes for up to 10 seconds or 0 objects
[...]
Total time run:         10.294444
Total writes made:      394
Write size:             4194304
Bandwidth (MB/sec):     153.092 
[...]
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;CONCURRENCY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;
rest-bench -t &lt;span class="nv"&gt;$CONCURRENCY&lt;/span&gt; -b &lt;span class="nv"&gt;$SIZE&lt;/span&gt; --seconds&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$SECS&lt;/span&gt; --api-host&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$RGW&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--bucket&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$BUCKET&lt;/span&gt; --access-key&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$KEY&lt;/span&gt; --secret&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$SECRET&lt;/span&gt; --no-cleanup write
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;host=localhost:7480
 Maintaining 1 concurrent writes of 4194304 bytes for up to 10 seconds or 0 objects
[...]
 Total time run:         10.090768
Total writes made:      147
Write size:             4194304
Bandwidth (MB/sec):     58.271
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Logical, right? Rather than allowing 16 threads to interact with the
cluster in parallel, we now have to wait for every single PUT to
complete before we can issue the next. Pretty obvious to see both our
writes-per-second and our aggregate bandwidth to drop by more than
half.&lt;/p&gt;
&lt;p&gt;The effect is even slightly less pronounced with smaller
objects. Compare the two for 4KB objects:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;SIZE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$((&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="s"&gt;&amp;lt;&amp;lt;12)) # 4KB object size&lt;/span&gt;
&lt;span class="s"&gt;export CONCURRENCY=1&lt;/span&gt;&lt;span class="m"&gt;6&lt;/span&gt;
rest-bench &lt;span class="o"&gt;-&lt;/span&gt;t &lt;span class="nv"&gt;$CONCURRENCY&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;b &lt;span class="nv"&gt;$SIZE&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="nv"&gt;seconds&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$SECS&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;api-host&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$RGW&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="nv"&gt;bucket&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$BUCKET&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;access-key&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$KEY&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="nv"&gt;secret&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$SECRET&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;no-cleanup write
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;host=localhost:7480
 Maintaining 16 concurrent writes of 4096 bytes for up to 10 seconds or 0 objects
[...]
 Total time run:         10.053976
Total writes made:      3211
Write size:             4096
Bandwidth (MB/sec):     1.248 
[...]
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;CONCURRENCY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt; 
rest-bench -t &lt;span class="nv"&gt;$CONCURRENCY&lt;/span&gt; -b &lt;span class="nv"&gt;$SIZE&lt;/span&gt; --seconds&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$SECS&lt;/span&gt; --api-host&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$RGW&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
--bucket&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$BUCKET&lt;/span&gt; --access-key&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$KEY&lt;/span&gt; --secret&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$SECRET&lt;/span&gt; --no-cleanup write
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;host=localhost:7480
 Maintaining 1 concurrent writes of 4096 bytes for up to 10 seconds or 0 objects
[...]
 Total time run:         10.007962
Total writes made:      1632
Write size:             4096
Bandwidth (MB/sec):     0.637
[...]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Both writes-per-second and throughput drop by half.&lt;/p&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Note: If you've dealt with storage performance considerations before,
some of these will be blindingly obvious. Apologies for that; it just
shows that Ceph is generally a well-behaved system that does what you
would normally expect.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Larger objects have less overhead, and as such increase your
  throughput,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Smaller objects increase writes-per-second at the expense of
  aggregate throughput, because they have more overhead,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Serialization and contention (both of which mean reduced
  concurrency) reduce your data throughput and your writes-per-second.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What does this mean for your radosgw application?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Concurrency is good. If your application can fire a bunch of RESTful
  objects at radosgw, which don't have to wait for each other, great.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you need to optimize for lots of PUTs per second, make sure that
  your application sends data in reasonably sized chunks. And again,
  make sure it is capable of doing so in parallel.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you need to optimize for throughput instead, make sure that your application coalesces data into large objects. There is a big difference between sending one object of 10MB, and 10 objects of 1 MB.&lt;/p&gt;</content><category term="Ceph"></category></entry><entry><title>OpenStack for Open edX: Inside and Out (SWITCH ICT-Focus 2015)</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/ictfocus2015/index.html" rel="alternate"></link><published>2015-11-10T00:00:00+00:00</published><updated>2015-11-10T00:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2015-11-10:fghaas.github.io/xahteiwi.eu/resources/presentations/ictfocus2015/index.html</id><summary type="html">&lt;p&gt;Florian's presentation at SWITCH ICT-Focus 2015.&lt;/p&gt;
&lt;!--break--&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//hastexo.github.io/ictfocus2015/"&gt;&lt;/iframe&gt;
&lt;/div&gt;</summary><content type="html">&lt;p&gt;Florian's presentation at SWITCH ICT-Focus 2015.&lt;/p&gt;
&lt;!--break--&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//hastexo.github.io/ictfocus2015/"&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><category term="Conference"></category><category term="OpenStack"></category><category term="Open edX"></category></entry><entry><title>Clusters, Routers, Agents and Networks: High Availability in Neutron</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/openstacksummit2015-tokyo-neutron-ha/index.html" rel="alternate"></link><published>2015-10-28T00:00:00+00:00</published><updated>2015-10-28T00:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2015-10-28:fghaas.github.io/xahteiwi.eu/resources/presentations/openstacksummit2015-tokyo-neutron-ha/index.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;Of everything that we can build and deploy in a highly-available
fashion in OpenStack, deploying highly available networking has been
one of the trickiest, most complex aspects to get right.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Florian teams up with Adam Spiers (SUSE) and Assaf Muller (Red Hat) to
discuss high availability in OpenStack Neutron.&lt;/p&gt;
&lt;!--break--&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//www.youtube.com/embed/vBZgtHgSdOY"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//fghaas.github.io/openstacksummit2015-tokyo-neutron-ha/"&gt;&lt;/iframe&gt;
&lt;/div&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;Of everything that we can build and deploy in a highly-available
fashion in OpenStack, deploying highly available networking has been
one of the trickiest, most complex aspects to get right.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Florian teams up with Adam Spiers (SUSE) and Assaf Muller (Red Hat) to
discuss high availability in OpenStack Neutron.&lt;/p&gt;
&lt;!--break--&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//www.youtube.com/embed/vBZgtHgSdOY"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//fghaas.github.io/openstacksummit2015-tokyo-neutron-ha/"&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><category term="OpenStack"></category><category term="Conference"></category></entry><entry><title>Automated OpenStack deployment: A comparison</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/automated-openstack-deployment-a-comparison/index.html" rel="alternate"></link><published>2015-10-27T00:00:00+00:00</published><updated>2015-10-27T00:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2015-10-27:fghaas.github.io/xahteiwi.eu/resources/presentations/automated-openstack-deployment-a-comparison/index.html</id><summary type="html">&lt;p&gt;From the 2015 OpenStack Summit in Tokyo. A comparison of automated
deployment tools for OpenStack.
&lt;!--break--&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OSP Director on RHEL OSP 7&lt;/li&gt;
&lt;li&gt;Juju on Ubuntu Trusty&lt;/li&gt;
&lt;li&gt;Chef/Crowbar on SUSE OpenStack Cloud 5&lt;/li&gt;
&lt;li&gt;Ansible on Rackspace Private Cloud&lt;/li&gt;
&lt;li&gt;Fuel on Mirantis OpenStack&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;About 45 minutes.&lt;/p&gt;
&lt;!--break--&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//www.youtube.com/embed/LM1ANSge01g"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//hastexo.github.io/openstacksummit2015-tokyo-deployment/"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;Use the arrow keys to navigate …&lt;/p&gt;</summary><content type="html">&lt;p&gt;From the 2015 OpenStack Summit in Tokyo. A comparison of automated
deployment tools for OpenStack.
&lt;!--break--&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OSP Director on RHEL OSP 7&lt;/li&gt;
&lt;li&gt;Juju on Ubuntu Trusty&lt;/li&gt;
&lt;li&gt;Chef/Crowbar on SUSE OpenStack Cloud 5&lt;/li&gt;
&lt;li&gt;Ansible on Rackspace Private Cloud&lt;/li&gt;
&lt;li&gt;Fuel on Mirantis OpenStack&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;About 45 minutes.&lt;/p&gt;
&lt;!--break--&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//www.youtube.com/embed/LM1ANSge01g"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//hastexo.github.io/openstacksummit2015-tokyo-deployment/"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;Use the arrow keys to navigate through the presentation, hit &lt;code&gt;Esc&lt;/code&gt; to
zoom out for an overview, or just advance by hitting the spacebar.&lt;/p&gt;</content><category term="OpenStack"></category></entry><entry><title>Open edX</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/open-edx/index.html" rel="alternate"></link><published>2015-10-12T05:45:00+00:00</published><updated>2015-10-12T05:45:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2015-10-12:fghaas.github.io/xahteiwi.eu/resources/presentations/open-edx/index.html</id><summary type="html">&lt;p&gt;&lt;a href="http://open.edx.org"&gt;Open edX&lt;/a&gt; is an extensible, highly scalable,
open-source learning management platform. Originally conceived at
&lt;a href="http://www.mit.edu"&gt;MIT&lt;/a&gt; and &lt;a href="http://www.harvard.edu"&gt;Harvard
University&lt;/a&gt;, the edx.org platform it is
currently backed by more than 70 organizations world-wide and serves
more than 4 million students. Since 2013, the software running edx.org
is available under an …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="http://open.edx.org"&gt;Open edX&lt;/a&gt; is an extensible, highly scalable,
open-source learning management platform. Originally conceived at
&lt;a href="http://www.mit.edu"&gt;MIT&lt;/a&gt; and &lt;a href="http://www.harvard.edu"&gt;Harvard
University&lt;/a&gt;, the edx.org platform it is
currently backed by more than 70 organizations world-wide and serves
more than 4 million students. Since 2013, the software running edx.org
is available under an open source software license, enabling private and
public organizations to run and operate the same learning platform under
the Open edX initiative.&lt;/p&gt;
&lt;p&gt;hastexo's contribution to Open edX is twofold: first, we enabled Open
edX to run in an automated, distributed fashion on the OpenStack
platform. Then, we taught Open edX the ability to automate the
orchestration of on-demand OpenStack training environments. Both are now
an integral part of our hastexo Academy service offering.&lt;/p&gt;
&lt;p&gt;Want to find out how you can use Open edX and OpenStack to your
advantage? Check out the short animation below (with audio) to find out.&lt;/p&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="https://hastexo.github.io/academy-openedx/"&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><category term="Open edX"></category></entry><entry><title>OpenStack for Open edX: Inside and Out (Open edX Con 2015)</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/openedxcon2015/index.html" rel="alternate"></link><published>2015-10-12T00:00:00+00:00</published><updated>2015-10-12T00:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2015-10-12:fghaas.github.io/xahteiwi.eu/resources/presentations/openedxcon2015/index.html</id><summary type="html">&lt;p&gt;Adolfo's presentation from the 2015 Open edX Conference, in Wellesley,
MA.&lt;/p&gt;
&lt;p&gt;Adolfo talks about our work integrating OpenStack with Open edX,
our work on the hastexo XBlock, and also gives a live demonstration of
an OpenStack-based on-demand interactive lab environment.&lt;/p&gt;
&lt;!--break--&gt;
&lt;p&gt;Adolfo's full presentation (video courtesy of edX) and slides are …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Adolfo's presentation from the 2015 Open edX Conference, in Wellesley,
MA.&lt;/p&gt;
&lt;p&gt;Adolfo talks about our work integrating OpenStack with Open edX,
our work on the hastexo XBlock, and also gives a live demonstration of
an OpenStack-based on-demand interactive lab environment.&lt;/p&gt;
&lt;!--break--&gt;
&lt;p&gt;Adolfo's full presentation (video courtesy of edX) and slides are
below.&lt;/p&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//www.youtube.com/embed/QSgLBxTvxTY?start=3095"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//hastexo.github.io/openedx2015/"&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><category term="Conference"></category><category term="OpenStack"></category><category term="Open edX"></category></entry><entry><title>Manageable Application Containers: Lightning Quick Updates, Scaleable Security, Easy High Availability</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/manageable-application-containers/index.html" rel="alternate"></link><published>2015-10-07T00:00:00+00:00</published><updated>2015-10-07T00:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2015-10-07:fghaas.github.io/xahteiwi.eu/resources/presentations/manageable-application-containers/index.html</id><summary type="html">&lt;p&gt;From LinuxCon Europe 2015 in Dublin. An alternative approach to
managing application containers.
&lt;!--break--&gt; &lt;/p&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//hastexo.github.io/lceu2015/"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;Use the arrow keys to navigate through the presentation, hit &lt;code&gt;Esc&lt;/code&gt; to
zoom out for an overview, or just advance by hitting the spacebar.&lt;/p&gt;</summary><content type="html">&lt;p&gt;From LinuxCon Europe 2015 in Dublin. An alternative approach to
managing application containers.
&lt;!--break--&gt; &lt;/p&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//hastexo.github.io/lceu2015/"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;Use the arrow keys to navigate through the presentation, hit &lt;code&gt;Esc&lt;/code&gt; to
zoom out for an overview, or just advance by hitting the spacebar.&lt;/p&gt;</content><category term="LXC"></category><category term="Containers"></category><category term="Conference"></category></entry><entry><title>OpenStack Orchestration and Automation</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/osil2015-orchestration/index.html" rel="alternate"></link><published>2015-06-15T00:00:00+00:00</published><updated>2015-06-15T00:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2015-06-15:fghaas.github.io/xahteiwi.eu/resources/presentations/osil2015-orchestration/index.html</id><summary type="html">&lt;p&gt;Florian's presentation from OpenStack Israel 2015. A fast-paced
introduction to cloud-init and OpenStack Heat.&lt;/p&gt;
&lt;!--break--&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//www.youtube.com/embed/oXYXqwnr5io"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//hastexo.github.io/osil2015-orchestration/"&gt;&lt;/iframe&gt;
&lt;/div&gt;</summary><content type="html">&lt;p&gt;Florian's presentation from OpenStack Israel 2015. A fast-paced
introduction to cloud-init and OpenStack Heat.&lt;/p&gt;
&lt;!--break--&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//www.youtube.com/embed/oXYXqwnr5io"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//hastexo.github.io/osil2015-orchestration/"&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><category term="OpenStack"></category><category term="Conference"></category></entry><entry><title>Ceph Tech Talk: Placement Groups</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/ceph-tech-talk-pg/index.html" rel="alternate"></link><published>2015-05-27T00:00:00+00:00</published><updated>2015-05-27T00:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2015-05-27:fghaas.github.io/xahteiwi.eu/resources/presentations/ceph-tech-talk-pg/index.html</id><summary type="html">&lt;p&gt;A Ceph Tech Talk on the ins and outs of Ceph Placement Groups (PGs).&lt;/p&gt;
&lt;!--break--&gt;
&lt;p&gt;Florian's full presentation (video courtesy of the Ceph team at Red
Hat) and slides are below. Special thanks to Patrick McGarry for
inviting us to speak on a Ceph Tech Talk.&lt;/p&gt;
&lt;p&gt;For the slide deck, use …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A Ceph Tech Talk on the ins and outs of Ceph Placement Groups (PGs).&lt;/p&gt;
&lt;!--break--&gt;
&lt;p&gt;Florian's full presentation (video courtesy of the Ceph team at Red
Hat) and slides are below. Special thanks to Patrick McGarry for
inviting us to speak on a Ceph Tech Talk.&lt;/p&gt;
&lt;p&gt;For the slide deck, use the PgUp/PgDown keys to navigate, or just
advance by hitting the spacebar.&lt;/p&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//www.youtube.com/embed/BPuaKErc0uA"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//hastexo.github.io/ceph-tech-talk-pg/"&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><category term="Ceph"></category></entry><entry><title>Have Data, Want Scale, Indefinitely: Exploring Ceph</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/ceph-intro/index.html" rel="alternate"></link><published>2014-11-15T00:00:00+00:00</published><updated>2014-11-15T00:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2014-11-15:fghaas.github.io/xahteiwi.eu/resources/presentations/ceph-intro/index.html</id><summary type="html">&lt;p&gt;An introduction to Ceph (with audio).&lt;/p&gt;
&lt;!--break--&gt;
&lt;p&gt;For the slide deck, use the PgUp/PgDown keys to navigate, or just
advance by hitting the spacebar. For audio narration, just click the
icon in the bottom-left corner and the presentation will auto-advance
in step with the narration.&lt;/p&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//hastexo.github.io/ceph-intro/"&gt;&lt;/iframe&gt;
&lt;/div&gt;</summary><content type="html">&lt;p&gt;An introduction to Ceph (with audio).&lt;/p&gt;
&lt;!--break--&gt;
&lt;p&gt;For the slide deck, use the PgUp/PgDown keys to navigate, or just
advance by hitting the spacebar. For audio narration, just click the
icon in the bottom-left corner and the presentation will auto-advance
in step with the narration.&lt;/p&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//hastexo.github.io/ceph-intro/"&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><category term="Ceph"></category></entry><entry><title>Ceph Performance Demystified: Benchmarks, Tools, and the Metrics that Matter</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/ceph-day-london/index.html" rel="alternate"></link><published>2014-10-22T00:00:00+00:00</published><updated>2014-10-22T00:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2014-10-22:fghaas.github.io/xahteiwi.eu/resources/presentations/ceph-day-london/index.html</id><summary type="html">&lt;p&gt;Mystified about Ceph performance tuning and benchmarking? Don't
despair! Florian is here to help.&lt;/p&gt;
&lt;p&gt;This presentation was given at Ceph Day London in 2014. &lt;!--break--&gt;
The full video (courtesy of the Ceph team at Red Hat) and Florian's
slides are below.&lt;/p&gt;
&lt;div class="alert alert-dismissible alert-info"&gt;
&lt;p&gt;&lt;button class="close" data-dismiss="alert" type="button"&gt;×&lt;/button&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you're interested in Ceph performance optimization, you
  may …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;Mystified about Ceph performance tuning and benchmarking? Don't
despair! Florian is here to help.&lt;/p&gt;
&lt;p&gt;This presentation was given at Ceph Day London in 2014. &lt;!--break--&gt;
The full video (courtesy of the Ceph team at Red Hat) and Florian's
slides are below.&lt;/p&gt;
&lt;div class="alert alert-dismissible alert-info"&gt;
&lt;p&gt;&lt;button class="close" data-dismiss="alert" type="button"&gt;×&lt;/button&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you're interested in Ceph performance optimization, you
  may want to take a look at &lt;a href="{filename}/pages/services/academy/hx113.md"&gt;our HX113
  class&lt;/a&gt; which we've put
  together to focus exclusively on Ceph performance issues.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//www.youtube.com/embed/0B_A9VkRb1E"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//hastexo.github.io/ceph-day-london/"&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><category term="Ceph"></category></entry><entry><title>OpenStack High Availability: Are We There Yet?</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/lceu2014-openstack-ha/index.html" rel="alternate"></link><published>2014-10-14T00:00:00+00:00</published><updated>2014-10-14T00:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2014-10-14:fghaas.github.io/xahteiwi.eu/resources/presentations/lceu2014-openstack-ha/index.html</id><summary type="html">&lt;p&gt;Florian's presentation from LinuxCon Europe 2014, outlining the state
of high availability in OpenStack.&lt;/p&gt;
&lt;!--break--&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//hastexo.github.io/lceu2014-openstack-ha/"&gt;&lt;/iframe&gt;
&lt;/div&gt;</summary><content type="html">&lt;p&gt;Florian's presentation from LinuxCon Europe 2014, outlining the state
of high availability in OpenStack.&lt;/p&gt;
&lt;!--break--&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//hastexo.github.io/lceu2014-openstack-ha/"&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><category term="OpenStack"></category><category term="Conference"></category></entry><entry><title>Hands On Trove: Database as a Service in OpenStack</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/hands-trove-database-service-openstack/index.html" rel="alternate"></link><published>2014-03-27T11:17:00+00:00</published><updated>2014-03-27T11:17:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2014-03-27:fghaas.github.io/xahteiwi.eu/resources/presentations/hands-trove-database-service-openstack/index.html</id><summary type="html">&lt;p&gt;&lt;a href="http://www.percona.com/live/mysql-conference-2014/sessions/hands-trove-database-service-openstack-mysql"&gt;This
tutorial&lt;/a&gt;
covered OpenStack Trove at Percona Live 2014. If you want to recreate
the experience, read on!&lt;/p&gt;
&lt;!--break--&gt;
&lt;p&gt;In order to make the most of this tutorial, you can recreate the
interactive steps presented. Please note: the process, while simple, is
extremely bandwidth intensive and you don't want to be …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="http://www.percona.com/live/mysql-conference-2014/sessions/hands-trove-database-service-openstack-mysql"&gt;This
tutorial&lt;/a&gt;
covered OpenStack Trove at Percona Live 2014. If you want to recreate
the experience, read on!&lt;/p&gt;
&lt;!--break--&gt;
&lt;p&gt;In order to make the most of this tutorial, you can recreate the
interactive steps presented. Please note: the process, while simple, is
extremely bandwidth intensive and you don't want to be the bandwidth hog
that everyone hates in your hotel, or on a conference wifi. Do so in
your office (or home) instead.&lt;/p&gt;
&lt;p&gt;The set-up process is decribed in &lt;a href="https://github.com/fghaas/perconalive2014/blob/master/README.md"&gt;a brief
README&lt;/a&gt;.
Effectively, it boils down to cloning a Git repo and then running
vagrant up, and you'll be good to go. But do pay attention to the system
requirements.&lt;/p&gt;
&lt;p&gt;The slide deck is below. And please send us your questions in the
comments!&lt;/p&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//www.slideshare.net/slideshow/embed_code/33588994"&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><category term="Conference"></category><category term="MySQL"></category><category term="OpenStack"></category></entry><entry><title>Fun with extended attributes in Ceph Dumpling</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/fun-extended-attributes-ceph-dumpling/index.html" rel="alternate"></link><published>2014-02-24T16:50:17+01:00</published><updated>2014-02-24T16:50:17+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2014-02-24:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/fun-extended-attributes-ceph-dumpling/index.html</id><summary type="html">&lt;p&gt;This is a rather nasty bug in Ceph OSD, affecting 0.67 "Dumpling" and
earlier releases. It is fixed in versions later than 0.70, and a
simple workaround is available, but when it hits, this issue can be
pretty painful.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Please read this post to the end.&lt;/strong&gt; This is …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is a rather nasty bug in Ceph OSD, affecting 0.67 "Dumpling" and
earlier releases. It is fixed in versions later than 0.70, and a
simple workaround is available, but when it hits, this issue can be
pretty painful.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Please read this post to the end.&lt;/strong&gt; This is by no means a punch
being thrown at Ceph, in fact it rather clearly illustrates a very
sane choice that the Ceph developers have made. If you run Ceph
Emperor or later, you are not affected by this issue, but it will be
an interesting read in data integrity in distributed systems anyway.&lt;/p&gt;
&lt;h2&gt;Too much of a good thing: large extended attributes&lt;/h2&gt;
&lt;p&gt;Here is how to reproduce the problem in a very simple bit of Python
code, against Ceph Dumpling.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Do not run this on a production system. Don't. Ever.&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/python&lt;/span&gt;

&lt;span class="c1"&gt;# import rados&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;rados&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Rados&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conffile&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'/etc/ceph/ceph.conf'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;cluster&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;cluster&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;open_ioctx&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'test'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;ioctx&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;o&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rados&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Object&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ioctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'onebyte'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;# Write one byte as the object content&lt;/span&gt;
        &lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'a'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Wrote object'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;# Write an attribute of 8M&lt;/span&gt;
        &lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_xattr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'val'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'a'&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Set large attribute'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;# Retrieving an attribute by name should succeed&lt;/span&gt;
        &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_xattr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'val'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Retrieved large attribute'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;# Walking the attribute list should fail&lt;/span&gt;
        &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;alist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_xattrs&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Retrieved whole attribute list'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="n"&gt;rados&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Error&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Failed to retrieve attribute list. '&lt;/span&gt;
                  &lt;span class="s1"&gt;'Congratulations, you probably just '&lt;/span&gt;
                  &lt;span class="s1"&gt;'corrupted one of your PGs.'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Removing the disabling comment character is left as an exercise for
the daring reader, just in case your cut &amp;amp; paste trigger finger is
itchy. &lt;strong&gt;Do not run this against a production system.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;So what are we doing here? We're creating a single RADOS object named
&lt;code&gt;onebyte&lt;/code&gt; in a pool called &lt;code&gt;test&lt;/code&gt;. It is, as the name implies, only
one byte long (it contains just the letter a), but it has a very long
attribute named &lt;code&gt;val&lt;/code&gt;, which is 8 Megabytes' worth of &lt;code&gt;a&lt;/code&gt;'s.&lt;/p&gt;
&lt;p&gt;(In case you're wondering: yes, there are applications that set very
large attributes on RADOS objects. radosgw is one of them.)&lt;/p&gt;
&lt;p&gt;Since you've been able to set the attribute, you can also retrieve it,
which is why the call to &lt;code&gt;get_xattr('val')&lt;/code&gt; succeeds just fine. But if
you fetch the entire attribute &lt;em&gt;list&lt;/em&gt; (with &lt;code&gt;get_xattrs&lt;/code&gt;), then you
run into an &lt;code&gt;E2BIG&lt;/code&gt; error.&lt;/p&gt;
&lt;p&gt;You can confirm that on the Linux command line, using the &lt;code&gt;rados&lt;/code&gt;
utility, just the same. First, getting the object and getting an xattr
by name:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo rados -p &lt;span class="nb"&gt;test&lt;/span&gt; get onebyte -
a

$ sudo rados -p &lt;span class="nb"&gt;test&lt;/span&gt; getxattr onebyte val - &lt;span class="m"&gt;2&lt;/span&gt;&amp;gt;&lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt; head -c &lt;span class="m"&gt;50&lt;/span&gt;
aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Obviously, you're welcome to omit the head redirection if you prefer
to flood your screen. But for proving we can still retrieve the
attribute value, 50 characters is quite sufficient.&lt;/p&gt;
&lt;p&gt;Let's try &lt;em&gt;listing&lt;/em&gt; the attributes, though:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo rados -p &lt;span class="nb"&gt;test&lt;/span&gt; listxattr onebyte 
error getting xattr &lt;span class="nb"&gt;set&lt;/span&gt; test/onebyte: Argument list too long
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Oops. &lt;code&gt;Argument list too long&lt;/code&gt; is bash's way of translating the
&lt;code&gt;E2BIG&lt;/code&gt; error for you, because that's what it usually means. In this
case, though, it's actually what we get from the rados utility, and
that gets it from the OSD it's talking to, and that gets it from the
filesystem.&lt;/p&gt;
&lt;h2&gt;Digging deeper&lt;/h2&gt;
&lt;p&gt;Now let's take a look where this object is stored.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo ceph osd map &lt;span class="nb"&gt;test&lt;/span&gt; onebyte
$ osdmap e191 pool &lt;span class="s1"&gt;'test'&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; object &lt;span class="s1"&gt;'onebyte'&lt;/span&gt; -&amp;gt; pg &lt;span class="m"&gt;3&lt;/span&gt;.ed47d009 &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;.1&lt;span class="o"&gt;)&lt;/span&gt; -&amp;gt; up &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;,2&lt;span class="o"&gt;]&lt;/span&gt; acting &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;,2&lt;span class="o"&gt;]&lt;/span&gt;
So it&lt;span class="s1"&gt;'s PG 3.1, currently mapped to OSDs 0 (primary) and 2 (replica). We happen to be on the very host where OSD 0 is running, so let'&lt;/span&gt;s take a closer look:

$ sudo getfattr -d /var/lib/ceph/osd/ceph-0/current/3.1_head/onebyte__head_ED47D009__3 
/var/lib/ceph/osd/ceph-0/current/3.1_head/onebyte__head_ED47D009__3: Argument list too long
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Same thing, E2BIG. Sure, if we can't enumerate the attributes
ourselves, the OSD can't either. But it's still fairly benign, because
we can still retrieve the object, right?&lt;/p&gt;
&lt;h2&gt;Adding daemon failure&lt;/h2&gt;
&lt;p&gt;Well, not so much. Let's see what happens if one of our OSDs gets
restarted. This is a perfectly benign operation that Ceph is expected
to (and does) handle very gracefully.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo restart ceph-osd &lt;span class="nv"&gt;id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;
ceph-osd &lt;span class="o"&gt;(&lt;/span&gt;ceph/0&lt;span class="o"&gt;)&lt;/span&gt; start/running, process &lt;span class="m"&gt;7922&lt;/span&gt;
$ sudo rados -p &lt;span class="nb"&gt;test&lt;/span&gt; get onebyte -
a
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The object is still there. What if, incidentally, the other OSD also
happens to go down some time later, and stays down?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo stop ceph-osd &lt;span class="nv"&gt;id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;
ceph-osd &lt;span class="o"&gt;(&lt;/span&gt;ceph/2&lt;span class="o"&gt;)&lt;/span&gt; stop/waiting
$ sudo ceph osd out &lt;span class="m"&gt;2&lt;/span&gt;
marked out osd.2.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Remember, &lt;em&gt;"at scale, something always fails"&lt;/em&gt;. Ceph is built for
exactly that, and its algorithms deal with this type of failure in
stride. So at this point, we would expect Ceph to remap the PGs that
were previously on OSD 2 to OSD 1, and synchronize with OSD 0. And a
few minutes later, all hell breaks loose:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;sudo&lt;/span&gt; &lt;span class="nt"&gt;ceph&lt;/span&gt; &lt;span class="nt"&gt;-s&lt;/span&gt;
  &lt;span class="nt"&gt;cluster&lt;/span&gt; &lt;span class="nt"&gt;bd70ea39-58fc-4117-ade1-03a4d429cb49&lt;/span&gt;
   &lt;span class="nt"&gt;health&lt;/span&gt; &lt;span class="nt"&gt;HEALTH_WARN&lt;/span&gt; &lt;span class="nt"&gt;200&lt;/span&gt; &lt;span class="nt"&gt;pgs&lt;/span&gt; &lt;span class="nt"&gt;degraded&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="nt"&gt;1&lt;/span&gt; &lt;span class="nt"&gt;pgs&lt;/span&gt; &lt;span class="nt"&gt;recovering&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="nt"&gt;200&lt;/span&gt; &lt;span class="nt"&gt;pgs&lt;/span&gt; &lt;span class="nt"&gt;stuck&lt;/span&gt; &lt;span class="nt"&gt;unclean&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="nt"&gt;recovery&lt;/span&gt; &lt;span class="nt"&gt;2&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;2&lt;/span&gt; &lt;span class="nt"&gt;degraded&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;100&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;000&lt;/span&gt;&lt;span class="o"&gt;%);&lt;/span&gt; &lt;span class="nt"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;1&lt;/span&gt; &lt;span class="nt"&gt;unfound&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;100&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;000&lt;/span&gt;&lt;span class="o"&gt;%)&lt;/span&gt;
   &lt;span class="nt"&gt;monmap&lt;/span&gt; &lt;span class="nt"&gt;e4&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;3&lt;/span&gt; &lt;span class="nt"&gt;mons&lt;/span&gt; &lt;span class="nt"&gt;at&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="err"&gt;ubuntu-ceph1=192.168.122.201:6789/0,ubuntu-ceph2=192.168.122.202:6789/0,ubuntu-ceph3=192.168.122.203:6789/0&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;election&lt;/span&gt; &lt;span class="nt"&gt;epoch&lt;/span&gt; &lt;span class="nt"&gt;180&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;quorum&lt;/span&gt; &lt;span class="nt"&gt;0&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="nt"&gt;1&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="nt"&gt;2&lt;/span&gt; &lt;span class="nt"&gt;ubuntu-ceph1&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="nt"&gt;ubuntu-ceph2&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="nt"&gt;ubuntu-ceph3&lt;/span&gt;
   &lt;span class="nt"&gt;osdmap&lt;/span&gt; &lt;span class="nt"&gt;e237&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;3&lt;/span&gt; &lt;span class="nt"&gt;osds&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;1&lt;/span&gt; &lt;span class="nt"&gt;up&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;1&lt;/span&gt; &lt;span class="nt"&gt;in&lt;/span&gt;
    &lt;span class="nt"&gt;pgmap&lt;/span&gt; &lt;span class="nt"&gt;v1335&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;200&lt;/span&gt; &lt;span class="nt"&gt;pgs&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;1&lt;/span&gt; &lt;span class="nt"&gt;active&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nt"&gt;recovering&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nt"&gt;degraded&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;199&lt;/span&gt; &lt;span class="nt"&gt;active&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nt"&gt;degraded&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="nt"&gt;1&lt;/span&gt; &lt;span class="nt"&gt;bytes&lt;/span&gt; &lt;span class="nt"&gt;data&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;38684&lt;/span&gt; &lt;span class="nt"&gt;KB&lt;/span&gt; &lt;span class="nt"&gt;used&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;5071&lt;/span&gt; &lt;span class="nt"&gt;MB&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nt"&gt;5108&lt;/span&gt; &lt;span class="nt"&gt;MB&lt;/span&gt; &lt;span class="nt"&gt;avail&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="nt"&gt;2&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;2&lt;/span&gt; &lt;span class="nt"&gt;degraded&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;100&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;000&lt;/span&gt;&lt;span class="o"&gt;%);&lt;/span&gt; &lt;span class="nt"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;1&lt;/span&gt; &lt;span class="nt"&gt;unfound&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;100&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;000&lt;/span&gt;&lt;span class="o"&gt;%)&lt;/span&gt;
   &lt;span class="nt"&gt;mdsmap&lt;/span&gt; &lt;span class="nt"&gt;e1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;0&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;0&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;1&lt;/span&gt; &lt;span class="nt"&gt;up&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Fighting a fire&lt;/h2&gt;
&lt;p&gt;Wow. We shut down only one OSD (OSD 2), the other one (OSD 0) was
merely restarted, but it has crashed in the interim. Its mon osd down
out interval has also expired, so it has been marked out as well. All
of our PGs are stuck degraded, one has an unfound object (that's the
one whose xattrs can no longer be enumerated). Yikes.&lt;/p&gt;
&lt;p&gt;We scramble to bring our just-shutdown OSD back in.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo start ceph-osd &lt;span class="nv"&gt;id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;
ceph-osd &lt;span class="o"&gt;(&lt;/span&gt;ceph/2&lt;span class="o"&gt;)&lt;/span&gt; start/running, process &lt;span class="m"&gt;7426&lt;/span&gt;
$ sudo ceph osd in &lt;span class="m"&gt;2&lt;/span&gt;
marked in osd.2.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Does this make things better?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo ceph -w
 cluster bd70ea39-58fc-4117-ade1-03a4d429cb49
 health HEALTH_WARN &lt;span class="m"&gt;200&lt;/span&gt; pgs degraded&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; pgs recovering&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="m"&gt;200&lt;/span&gt; pgs stuck unclean&lt;span class="p"&gt;;&lt;/span&gt; recovery &lt;span class="m"&gt;2&lt;/span&gt;/2 degraded &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;100&lt;/span&gt;.000%&lt;span class="o"&gt;)&lt;/span&gt;
 monmap e4: &lt;span class="m"&gt;3&lt;/span&gt; mons at &lt;span class="o"&gt;{&lt;/span&gt;ubuntu-ceph1&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;192&lt;/span&gt;.168.122.201:6789/0,ubuntu-ceph2&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;192&lt;/span&gt;.168.122.202:6789/0,ubuntu-ceph3&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;192&lt;/span&gt;.168.122.203:6789/0&lt;span class="o"&gt;}&lt;/span&gt;, election epoch &lt;span class="m"&gt;180&lt;/span&gt;, quorum &lt;span class="m"&gt;0&lt;/span&gt;,1,2 ubuntu-ceph1,ubuntu-ceph2,ubuntu-ceph3
 osdmap e243: &lt;span class="m"&gt;3&lt;/span&gt; osds: &lt;span class="m"&gt;2&lt;/span&gt; up, &lt;span class="m"&gt;2&lt;/span&gt; in
  pgmap v1343: &lt;span class="m"&gt;200&lt;/span&gt; pgs: &lt;span class="m"&gt;1&lt;/span&gt; active+recovering+degraded, &lt;span class="m"&gt;199&lt;/span&gt; active+degraded&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; bytes data, &lt;span class="m"&gt;38812&lt;/span&gt; KB used, &lt;span class="m"&gt;5071&lt;/span&gt; MB / &lt;span class="m"&gt;5108&lt;/span&gt; MB avail&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;/2 degraded &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;100&lt;/span&gt;.000%&lt;span class="o"&gt;)&lt;/span&gt;
 mdsmap e1: &lt;span class="m"&gt;0&lt;/span&gt;/0/1 up

&lt;span class="m"&gt;2014&lt;/span&gt;-02-11 &lt;span class="m"&gt;19&lt;/span&gt;:09:56.868771 mon.0 &lt;span class="o"&gt;[&lt;/span&gt;INF&lt;span class="o"&gt;]&lt;/span&gt; osdmap e242: &lt;span class="m"&gt;3&lt;/span&gt; osds: &lt;span class="m"&gt;2&lt;/span&gt; up, &lt;span class="m"&gt;2&lt;/span&gt; in
&lt;span class="m"&gt;2014&lt;/span&gt;-02-11 &lt;span class="m"&gt;19&lt;/span&gt;:09:56.895559 mon.0 &lt;span class="o"&gt;[&lt;/span&gt;INF&lt;span class="o"&gt;]&lt;/span&gt; pgmap v1342: &lt;span class="m"&gt;200&lt;/span&gt; pgs: &lt;span class="m"&gt;1&lt;/span&gt; active+recovering+degraded, &lt;span class="m"&gt;199&lt;/span&gt; active+degraded&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; bytes data, &lt;span class="m"&gt;38812&lt;/span&gt; KB used, &lt;span class="m"&gt;5071&lt;/span&gt; MB / &lt;span class="m"&gt;5108&lt;/span&gt; MB avail&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;/2 degraded &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;100&lt;/span&gt;.000%&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="m"&gt;2014&lt;/span&gt;-02-11 &lt;span class="m"&gt;19&lt;/span&gt;:09:57.901188 mon.0 &lt;span class="o"&gt;[&lt;/span&gt;INF&lt;span class="o"&gt;]&lt;/span&gt; osdmap e243: &lt;span class="m"&gt;3&lt;/span&gt; osds: &lt;span class="m"&gt;2&lt;/span&gt; up, &lt;span class="m"&gt;2&lt;/span&gt; in
&lt;span class="m"&gt;2014&lt;/span&gt;-02-11 &lt;span class="m"&gt;19&lt;/span&gt;:09:57.918612 mon.0 &lt;span class="o"&gt;[&lt;/span&gt;INF&lt;span class="o"&gt;]&lt;/span&gt; pgmap v1343: &lt;span class="m"&gt;200&lt;/span&gt; pgs: &lt;span class="m"&gt;1&lt;/span&gt; active+recovering+degraded, &lt;span class="m"&gt;199&lt;/span&gt; active+degraded&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; bytes data, &lt;span class="m"&gt;38812&lt;/span&gt; KB used, &lt;span class="m"&gt;5071&lt;/span&gt; MB / &lt;span class="m"&gt;5108&lt;/span&gt; MB avail&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;/2 degraded &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;100&lt;/span&gt;.000%&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="m"&gt;2014&lt;/span&gt;-02-11 &lt;span class="m"&gt;19&lt;/span&gt;:09:59.920149 mon.0 &lt;span class="o"&gt;[&lt;/span&gt;INF&lt;span class="o"&gt;]&lt;/span&gt; osdmap e244: &lt;span class="m"&gt;3&lt;/span&gt; osds: &lt;span class="m"&gt;1&lt;/span&gt; up, &lt;span class="m"&gt;2&lt;/span&gt; in
&lt;span class="m"&gt;2014&lt;/span&gt;-02-11 &lt;span class="m"&gt;19&lt;/span&gt;:09:59.931825 mon.0 &lt;span class="o"&gt;[&lt;/span&gt;INF&lt;span class="o"&gt;]&lt;/span&gt; pgmap v1344: &lt;span class="m"&gt;200&lt;/span&gt; pgs: &lt;span class="m"&gt;1&lt;/span&gt; active+recovering+degraded, &lt;span class="m"&gt;199&lt;/span&gt; active+degraded&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; bytes data, &lt;span class="m"&gt;38812&lt;/span&gt; KB used, &lt;span class="m"&gt;5071&lt;/span&gt; MB / &lt;span class="m"&gt;5108&lt;/span&gt; MB avail&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;/2 degraded &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;100&lt;/span&gt;.000%&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="m"&gt;2014&lt;/span&gt;-02-11 &lt;span class="m"&gt;19&lt;/span&gt;:10:00.940319 mon.0 &lt;span class="o"&gt;[&lt;/span&gt;INF&lt;span class="o"&gt;]&lt;/span&gt; osd.2 &lt;span class="m"&gt;192&lt;/span&gt;.168.122.203:6800/8362 boot
&lt;span class="m"&gt;2014&lt;/span&gt;-02-11 &lt;span class="m"&gt;19&lt;/span&gt;:10:00.940987 mon.0 &lt;span class="o"&gt;[&lt;/span&gt;INF&lt;span class="o"&gt;]&lt;/span&gt; osdmap e245: &lt;span class="m"&gt;3&lt;/span&gt; osds: &lt;span class="m"&gt;2&lt;/span&gt; up, &lt;span class="m"&gt;2&lt;/span&gt; in
&lt;span class="m"&gt;2014&lt;/span&gt;-02-11 &lt;span class="m"&gt;19&lt;/span&gt;:10:00.954275 mon.0 &lt;span class="o"&gt;[&lt;/span&gt;INF&lt;span class="o"&gt;]&lt;/span&gt; pgmap v1345: &lt;span class="m"&gt;200&lt;/span&gt; pgs: &lt;span class="m"&gt;1&lt;/span&gt; active+recovering+degraded, &lt;span class="m"&gt;199&lt;/span&gt; active+degraded&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; bytes data, &lt;span class="m"&gt;38812&lt;/span&gt; KB used, &lt;span class="m"&gt;5071&lt;/span&gt; MB / &lt;span class="m"&gt;5108&lt;/span&gt; MB avail&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;/2 degraded &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;100&lt;/span&gt;.000%&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="m"&gt;2014&lt;/span&gt;-02-11 &lt;span class="m"&gt;19&lt;/span&gt;:10:01.960942 mon.0 &lt;span class="o"&gt;[&lt;/span&gt;INF&lt;span class="o"&gt;]&lt;/span&gt; osdmap e246: &lt;span class="m"&gt;3&lt;/span&gt; osds: &lt;span class="m"&gt;2&lt;/span&gt; up, &lt;span class="m"&gt;2&lt;/span&gt; in
&lt;span class="m"&gt;2014&lt;/span&gt;-02-11 &lt;span class="m"&gt;19&lt;/span&gt;:10:01.975509 mon.0 &lt;span class="o"&gt;[&lt;/span&gt;INF&lt;span class="o"&gt;]&lt;/span&gt; pgmap v1346: &lt;span class="m"&gt;200&lt;/span&gt; pgs: &lt;span class="m"&gt;1&lt;/span&gt; active+recovering+degraded, &lt;span class="m"&gt;199&lt;/span&gt; active+degraded&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; bytes data, &lt;span class="m"&gt;38812&lt;/span&gt; KB used, &lt;span class="m"&gt;5071&lt;/span&gt; MB / &lt;span class="m"&gt;5108&lt;/span&gt; MB avail&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;/2 degraded &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;100&lt;/span&gt;.000%&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="m"&gt;2014&lt;/span&gt;-02-11 &lt;span class="m"&gt;19&lt;/span&gt;:10:03.982202 mon.0 &lt;span class="o"&gt;[&lt;/span&gt;INF&lt;span class="o"&gt;]&lt;/span&gt; osdmap e247: &lt;span class="m"&gt;3&lt;/span&gt; osds: &lt;span class="m"&gt;1&lt;/span&gt; up, &lt;span class="m"&gt;2&lt;/span&gt; in
&lt;span class="m"&gt;2014&lt;/span&gt;-02-11 &lt;span class="m"&gt;19&lt;/span&gt;:10:03.994963 mon.0 &lt;span class="o"&gt;[&lt;/span&gt;INF&lt;span class="o"&gt;]&lt;/span&gt; pgmap v1347: &lt;span class="m"&gt;200&lt;/span&gt; pgs: &lt;span class="m"&gt;1&lt;/span&gt; active+recovering+degraded, &lt;span class="m"&gt;199&lt;/span&gt; active+degraded&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; bytes data, &lt;span class="m"&gt;38812&lt;/span&gt; KB used, &lt;span class="m"&gt;5071&lt;/span&gt; MB / &lt;span class="m"&gt;5108&lt;/span&gt; MB avail&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;/2 degraded &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;100&lt;/span&gt;.000%&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="m"&gt;2014&lt;/span&gt;-02-11 &lt;span class="m"&gt;19&lt;/span&gt;:10:05.005162 mon.0 &lt;span class="o"&gt;[&lt;/span&gt;INF&lt;span class="o"&gt;]&lt;/span&gt; osd.2 &lt;span class="m"&gt;192&lt;/span&gt;.168.122.203:6800/8483 boot
&lt;span class="m"&gt;2014&lt;/span&gt;-02-11 &lt;span class="m"&gt;19&lt;/span&gt;:10:05.005386 mon.0 &lt;span class="o"&gt;[&lt;/span&gt;INF&lt;span class="o"&gt;]&lt;/span&gt; osdmap e248: &lt;span class="m"&gt;3&lt;/span&gt; osds: &lt;span class="m"&gt;2&lt;/span&gt; up, &lt;span class="m"&gt;2&lt;/span&gt; in
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Hardly. OSDs flapping right and left. Ouch ouch ouch.&lt;/p&gt;
&lt;h2&gt;Desperation: not your friend&lt;/h2&gt;
&lt;p&gt;OK, let's try to do something really terrible and get rid of that file manually.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo ceph osd map &lt;span class="nb"&gt;test&lt;/span&gt; onebyte
osdmap e254 pool &lt;span class="s1"&gt;'test'&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; object &lt;span class="s1"&gt;'onebyte'&lt;/span&gt; -&amp;gt; pg &lt;span class="m"&gt;3&lt;/span&gt;.ed47d009 &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;.1&lt;span class="o"&gt;)&lt;/span&gt; -&amp;gt; up &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt; acting &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So it's mapped to OSD 1 now, which is expected. Let's take a look and see if we can find and remove it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ceph@ubuntu-ceph2:~$ ls /var/lib/ceph/osd/ceph-1/current/3.1_head/
ceph@ubuntu-ceph2:~$
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;An empty directory. Well of course, they could never actually peer, so
the data never got synchronized. So there's pretty much one thing
left.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ceph@ubuntu-ceph3:~$ sudo stop ceph-osd id=2
stop: Unknown instance: ceph/2
ceph@ubuntu-ceph3:~$ sudo rm /var/lib/ceph/osd/ceph-2/current/3.1_head/onebyte__head_ED47D009__3
ceph@ubuntu-ceph3:~$ sudo start ceph-osd id=2
ceph-osd (ceph/2) start/running, process 9069

ceph@ubuntu-ceph1:~$ sudo stop ceph-osd id=0
stop: Unknown instance: ceph/0
ceph@ubuntu-ceph1:~$ sudo rm /var/lib/ceph/osd/ceph-0/current/3.1_head/onebyte__head_ED47D009__3 
ceph@ubuntu-ceph1:~$ sudo start ceph-osd id=0
ceph-osd (ceph/0) start/running, process 9485
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;There. Shut down the OSDs, nuked the files, brought the OSDs back up.&lt;/p&gt;
&lt;h2&gt;A Fire Contained&lt;/h2&gt;
&lt;p&gt;And after a few more seconds, finally:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo ceph -s
  cluster bd70ea39-58fc-4117-ade1-03a4d429cb49
   health HEALTH_OK
   monmap e4: &lt;span class="m"&gt;3&lt;/span&gt; mons at &lt;span class="o"&gt;{&lt;/span&gt;ubuntu-ceph1&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;192&lt;/span&gt;.168.122.201:6789/0,ubuntu-ceph2&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;192&lt;/span&gt;.168.122.202:6789/0,ubuntu-ceph3&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;192&lt;/span&gt;.168.122.203:6789/0&lt;span class="o"&gt;}&lt;/span&gt;, election epoch &lt;span class="m"&gt;180&lt;/span&gt;, quorum &lt;span class="m"&gt;0&lt;/span&gt;,1,2 ubuntu-ceph1,ubuntu-ceph2,ubuntu-ceph3
   osdmap e259: &lt;span class="m"&gt;3&lt;/span&gt; osds: &lt;span class="m"&gt;3&lt;/span&gt; up, &lt;span class="m"&gt;3&lt;/span&gt; in
    pgmap v1367: &lt;span class="m"&gt;200&lt;/span&gt; pgs: &lt;span class="m"&gt;200&lt;/span&gt; active+clean&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; bytes data, &lt;span class="m"&gt;122&lt;/span&gt; MB used, &lt;span class="m"&gt;15204&lt;/span&gt; MB / &lt;span class="m"&gt;15326&lt;/span&gt; MB avail
   mdsmap e1: &lt;span class="m"&gt;0&lt;/span&gt;/0/1 up
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Whew.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo rados -p &lt;span class="nb"&gt;test&lt;/span&gt; get onebyte -
error getting test/onebyte: No such file or directory
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now obviously the offending object is gone, which is ugly and we could
have manually recreated that file and set some magic user.ceph
attributes enabling us to keep the object, but in this case we just
didn't care and wanted our cluster back up and running as soon as
possible.&lt;/p&gt;
&lt;h2&gt;Prevention&lt;/h2&gt;
&lt;p&gt;So we have a brutal cure for this problem that is roughly akin to
performing brain surgery with a fork and spoon. What could we have
done better?&lt;/p&gt;
&lt;p&gt;LevelDB to the rescue. Ceph optionally (and in later versions, by
default) stores attributes that would overflow the filesystem xattr
store in a separate database called an omap, using Google's embedded
LevelDB database. And in Dumpling, this feature is disabled by default
-- with an exception for ext3/4, which have interesting attribute
limitations themselves.&lt;/p&gt;
&lt;p&gt;This is the all-important option that needs to go in your ceph.conf:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="na"&gt;filestore xattr use omap&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;true&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can enable this on a running cluster and this will retain and
preserve any xattrs previously set on RADOS objects. Attributes mapped
to file xattrs will simply be moved to the omap database (note however
that the opposite is not true, but you'll never want to disable this
option anymore, anyway).&lt;/p&gt;
&lt;p&gt;As of
&lt;a href="https://github.com/ceph/ceph/commit/dc0dfb9e01d593afdd430ca776cf4da2c2240a20"&gt;this Ceph commit&lt;/a&gt;
(which went into Ceph 0.70), the option is no longer available and is
always treated as if set to true, so those versions are not affected
by the issue described in this post.&lt;/p&gt;</content><category term="Ceph"></category></entry><entry><title>Unrecoverable unfound objects in Ceph 0.67 and earlier</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/unrecoverable-unfound-objects-ceph-067-and-earlier/index.html" rel="alternate"></link><published>2014-01-28T18:52:02+01:00</published><updated>2014-01-28T18:52:02+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2014-01-28:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/unrecoverable-unfound-objects-ceph-067-and-earlier/index.html</id><summary type="html">&lt;p&gt;As &lt;a href="http://ceph.com/"&gt;Ceph&lt;/a&gt; author &lt;a href="https://twitter.com/Liewegas"&gt;Sage
Weil&lt;/a&gt; points out frequently, distributed
storage solutions for all their goodness &lt;a href="http://youtu.be/JfRqpdgoiRQ?t=36m20s"&gt;have a "dirty little
secret"&lt;/a&gt;: No matter just how
redundant and reliable they are by design, a bug in the storage
software itself can be a real issue.&lt;/p&gt;
&lt;p&gt;And occasionally, the bug doesn't have to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;As &lt;a href="http://ceph.com/"&gt;Ceph&lt;/a&gt; author &lt;a href="https://twitter.com/Liewegas"&gt;Sage
Weil&lt;/a&gt; points out frequently, distributed
storage solutions for all their goodness &lt;a href="http://youtu.be/JfRqpdgoiRQ?t=36m20s"&gt;have a "dirty little
secret"&lt;/a&gt;: No matter just how
redundant and reliable they are by design, a bug in the storage
software itself can be a real issue.&lt;/p&gt;
&lt;p&gt;And occasionally, the bug doesn't have to be in the storage software
itself.&lt;/p&gt;
&lt;p&gt;Every self-respecting Linux file system supports &lt;a href="http://en.wikipedia.org/wiki/Extended_file_attributes"&gt;extended file
attributes
("xattrs")&lt;/a&gt;,
and XFS (commonly used with Ceph OSDs) is no exception. When OSDs
store RADOS objects in the OSD filestore, they make heavy use of
key-value pairs. To do so, they can employ two approaches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;storing key-value pairs in filesystem xattrs directly (inline
  xattrs);&lt;/li&gt;
&lt;li&gt;storing them in a separate key-value store known as an object map or
  omap (based on &lt;a href="https://github.com/google/leveldb"&gt;Google LevelDB&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;RADOS generally expects that the maximum xattr size on a file is
practically unlimited, so if your filestore is on a filesystem where
that is &lt;em&gt;not&lt;/em&gt; the case (such as ext4), you would generally use omaps.&lt;/p&gt;
&lt;p&gt;Enabling the use of omaps is easy enough. This goes in your ceph.conf:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;[osd]&lt;/span&gt;
&lt;span class="na"&gt;filestore xattr use omap&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;true&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Ceph releases since 0.66 will
&lt;a href="https://github.com/ceph/ceph/commit/6d90dad45e089447562e9a01fd9ca0f7a2aaf2b1"&gt;enable this automatically&lt;/a&gt;
if the filestore is determined to be running on ext4. But for the XFS
and BTRFS filesystem, the general recommendation (and default
behavior) remained to just use inline xattrs. This is also true for
the stable Ceph "Dumpling" release (0.67).&lt;/p&gt;
&lt;p&gt;Since Ceph 0.70, the configuration option
&lt;a href="https://github.com/ceph/ceph/commit/dc0dfb9e01d593afdd430ca776cf4da2c2240a20"&gt;has been dropped&lt;/a&gt;
and Ceph since always behaves as if &lt;code&gt;filestore xattr use omap&lt;/code&gt; was set
to &lt;code&gt;true&lt;/code&gt;. Now there is a reason for that, and it is a bit trickier
than you might expect.&lt;/p&gt;
&lt;p&gt;When manipulating extended attributes, applications (including
ceph-osd) make use of the
&lt;a href="http://man7.org/linux/man-pages/man2/fgetxattr.2.html"&gt;&lt;code&gt;getxattr()&lt;/code&gt;, &lt;code&gt;setxattr()&lt;/code&gt;, and &lt;code&gt;listxattr()&lt;/code&gt; syscalls&lt;/a&gt;. Expectedly,
these syscalls retrieve, set, and enumerate extended attributes set on
a file.&lt;/p&gt;
&lt;p&gt;Now it is actually possible to set so many keys, or so large values,
that while &lt;code&gt;getxattr()&lt;/code&gt; and &lt;code&gt;setxattr()&lt;/code&gt; executed on a specific file
continue to work just fine, &lt;code&gt;listxattr()&lt;/code&gt; returns with &lt;code&gt;-E2BIG&lt;/code&gt;. Now
it turns out that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;radosgw can actually set attribute lists that large, and&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ceph-osd will fail if it cannot determine the file attributes for a
  file under its control.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When this happens, the object shows as &lt;code&gt;unfound&lt;/code&gt; in &lt;code&gt;ceph health
detail&lt;/code&gt;, and sadly, the documented operation to recover unfound
objects fails. The affected Placement Group (PG) also remains stuck,
again being reported as such in ceph health detail.&lt;/p&gt;
&lt;p&gt;If you actually have run into this problem, you should really call
Inktank for support. (You can also give us a call, of course, and
we'll be happy to help you confirm the problem. But we will refer you
to Inktank for the actual fix -- we don't fiddle and mess around with
RADOS object internals, and neither should you.)&lt;/p&gt;
&lt;h2&gt;How to avoid this in the first place?&lt;/h2&gt;
&lt;p&gt;If you're on Ceph 0.70 or later, congratulations. You should be safe,
as omaps are enabled and anything that would overflow your xattrs
instead gets stored in an omap.&lt;/p&gt;
&lt;p&gt;If you're on any earlier version, including the currently
stable 0.67.x "Dumpling" series, enable filestore xattr use omap. Do
it now, regardless of what filesystem your OSDs run on. Then restart
your OSDs one by one; your existing xattrs won't get lost.&lt;/p&gt;</content><category term="Ceph"></category></entry><entry><title>linux.conf.au 2014, or My Annual Journey To Awesome</title><link href="fghaas.github.io/xahteiwi.eu/blog/2014/01/20/linuxconfau-2014-or-my-annual-journey-to-awesome/index.html" rel="alternate"></link><published>2014-01-20T11:36:00+00:00</published><updated>2014-01-20T11:36:00+00:00</updated><author><name>florian</name></author><id>tag:None,2014-01-20:fghaas.github.io/xahteiwi.eu/blog/2014/01/20/linuxconfau-2014-or-my-annual-journey-to-awesome/index.html</id><summary type="html">&lt;p&gt;Earlier this month, I got on a flight to Perth, WA to make my annual
trek to &lt;strong&gt;linux.conf.au&lt;/strong&gt;, the largest open-source conference in the
Southern Hemisphere and one of my favorite conferences on the circuit.
LCA is an excellent, volunteer-run, hugely insightful conference and
well worth the 26-hour …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Earlier this month, I got on a flight to Perth, WA to make my annual
trek to &lt;strong&gt;linux.conf.au&lt;/strong&gt;, the largest open-source conference in the
Southern Hemisphere and one of my favorite conferences on the circuit.
LCA is an excellent, volunteer-run, hugely insightful conference and
well worth the 26-hour trip.&lt;/p&gt;
&lt;p&gt;I arrived in Perth in time to catch part of the Systems Administration
mini-conference on day 1. In particular, I caught a &lt;a href="https://www.youtube.com/watch?v=jwBdrvEXMk0"&gt;&lt;strong&gt;systemd&lt;/strong&gt;
talk&lt;/a&gt; by &lt;strong&gt;Rodger
Donaldson&lt;/strong&gt; which was both informative and entertaining, so I encourage
you to watch that when you get the chance.&lt;/p&gt;
&lt;p&gt;Day 2 had the OpenStack miniconf, which was good but a bit too focused
on OpenStack governance and organizational issues in its first half. In
the afternoon, talks got more technical, which was a clear improvement.&lt;/p&gt;
&lt;p&gt;Wednesday talks included a highly entertaining &lt;a href="https://www.youtube.com/watch?v=_mQKyIAx6Mc"&gt;look at MySQL's
history&lt;/a&gt; from &lt;strong&gt;Stewart
Smith&lt;/strong&gt;, &lt;strong&gt;Dave Chinner&lt;/strong&gt;'s musings on where &lt;a href="https://www.youtube.com/watch?v=DxZzSifuV4Q"&gt;Linux
filesystems&lt;/a&gt; came from, and
an insightful &lt;a href="https://www.youtube.com/watch?v=XyDcYV9doL8"&gt;RADOS deep
dive&lt;/a&gt; from &lt;strong&gt;Sage Weil.&lt;/strong&gt;
There was also a DRBD talk that I was not so fond of, but I've already
shared my thoughts about that one on Google+, and you're certainly
welcome to &lt;a href="https://plus.google.com/u/0/+FlorianHaas/posts/KTtvUzmATJM"&gt;take a
look&lt;/a&gt; over
there.&lt;/p&gt;
&lt;p&gt;My own &lt;a href="http://youtu.be/YWVz1CSxayU"&gt;Rapid OpenStack Deployment for Novices and Experts
Alike&lt;/a&gt; tutorial was on Thursday, had a nice
turnout, and my hands-on stuff all worked! What more could I possibly
ask for?&lt;/p&gt;
&lt;p&gt;Besides my own talk, you should also totally watch &lt;strong&gt;Matthew Garrett&lt;/strong&gt;'s
&lt;a href="https://www.youtube.com/watch?v=ixMStnFzgRM"&gt;Thursday keynote&lt;/a&gt; and
&lt;strong&gt;Lana Brindley&lt;/strong&gt;'s &lt;a href="https://www.youtube.com/watch?v=HKmaCpOv0Ww"&gt;agile documentation tutorial with Lego
goodness&lt;/a&gt;. And of course,
my clear favorite among all LCA talks this year, Bdale Garbee's
&lt;a href="https://www.youtube.com/watch?v=j7Et_eWJExU"&gt;reflections on losing his
house&lt;/a&gt; in a wildfire last
year.&lt;/p&gt;
&lt;p&gt;As for Friday, again a stellar keynote from &lt;strong&gt;Jonathan Oxer&lt;/strong&gt; (&lt;a href="https://www.youtube.com/watch?v=0GHMTXiDqoA"&gt;Arduino
satellites in space&lt;/a&gt;, geek
overload), Lennart's &lt;a href="https://www.youtube.com/watch?v=sJyVaKZ8tbc"&gt;kdbus
talk&lt;/a&gt; which you've probably
already &lt;a href="https://lwn.net/Articles/580194/"&gt;read on LWN&lt;/a&gt; about, and a
brilliant &lt;a href="http://mirror.linux.org.au/pub/linux.conf.au/2014/Friday/122-LCA_2014_-_Lightning_Talks_pt2_and_Thanks.mp4"&gt;lightning
talk&lt;/a&gt;
from Tim Serong about building a DIY bookscanner.&lt;/p&gt;
&lt;p&gt;All in all, linux.conf.au in Perth was terrific, as usual, and I am
absolutely planning to be there again next year. The trip will be even
more atrociuous as next year's venue is
&lt;a href="http://lca2015.linux.org.au"&gt;Auckland&lt;/a&gt;, but it will be totally worth
it. No doubt in my mind at all.&lt;/p&gt;
&lt;p&gt;I'd like to extend a huge thank-you to all LCA attendees, fellow
speakers, organizers and volunteers who make this conference a fantastic
event year after year. See you in 2015!&lt;/p&gt;</content><category term="lca2014"></category><category term="OpenStack"></category></entry><entry><title>Greetings from Havana: A fresh perspective on globally distributed OpenStack</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/greetings-from-havana/index.html" rel="alternate"></link><published>2013-12-10T00:00:00+00:00</published><updated>2013-12-10T00:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2013-12-10:fghaas.github.io/xahteiwi.eu/resources/presentations/greetings-from-havana/index.html</id><summary type="html">&lt;p&gt;&lt;a href="/who/florian"&gt;Florian&lt;/a&gt;' second appearance at OpenStack Israel, this
time with news on distributed OpenStack environments in the Havana
release.&lt;/p&gt;
&lt;!--break--&gt;
&lt;p&gt;The video (courtesy of OpenStack Israel) and slides are below. For the
introduction given in Hebrew, the slides contain a transcript in
English.&lt;/p&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//www.youtube.com/embed/28p6Ls6hQJM"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//fghaas.github.io/openstackisraeldec2013/#/personal-intro"&gt;&lt;/iframe&gt;
&lt;/div&gt;</summary><content type="html">&lt;p&gt;&lt;a href="/who/florian"&gt;Florian&lt;/a&gt;' second appearance at OpenStack Israel, this
time with news on distributed OpenStack environments in the Havana
release.&lt;/p&gt;
&lt;!--break--&gt;
&lt;p&gt;The video (courtesy of OpenStack Israel) and slides are below. For the
introduction given in Hebrew, the slides contain a transcript in
English.&lt;/p&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//www.youtube.com/embed/28p6Ls6hQJM"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//fghaas.github.io/openstackisraeldec2013/#/personal-intro"&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><category term="Conference"></category><category term="OpenStack"></category></entry><entry><title>Ceph: object storage, block storage, file system, replication, massive scalability and then some!</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/ceph-object-storage-block-storage-file-system-replication-massive-scalabilit/index.html" rel="alternate"></link><published>2013-05-31T08:02:00+00:00</published><updated>2013-05-31T08:02:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2013-05-31:fghaas.github.io/xahteiwi.eu/resources/presentations/ceph-object-storage-block-storage-file-system-replication-massive-scalabilit/index.html</id><summary type="html">&lt;p&gt;Florian teams up with &lt;a href="http://ourobengr.com/"&gt;Tim Serong&lt;/a&gt; from
&lt;a href="https://www.suse.com/"&gt;SUSE&lt;/a&gt; in this tutorial presented at
linux.conf.au 2013.&lt;/p&gt;
&lt;p&gt;This is one of Florian's most popular talks, co-presented with intrepid
cartoonist-turned-engineer Tim Serong from SUSE. In this 90-minute
tutorial, Florian and Tim give both a theoretical and a practical
introduction to Ceph …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Florian teams up with &lt;a href="http://ourobengr.com/"&gt;Tim Serong&lt;/a&gt; from
&lt;a href="https://www.suse.com/"&gt;SUSE&lt;/a&gt; in this tutorial presented at
linux.conf.au 2013.&lt;/p&gt;
&lt;p&gt;This is one of Florian's most popular talks, co-presented with intrepid
cartoonist-turned-engineer Tim Serong from SUSE. In this 90-minute
tutorial, Florian and Tim give both a theoretical and a practical
introduction to Ceph. Rated one of the best talks at LCA2013 by many
attendees, you can see both presenters and the audience clearly enjoying
the experience.&lt;/p&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//www.youtube.com/embed/dDA1sBg4H98"&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><category term="Ceph"></category><category term="Conference"></category></entry><entry><title>Ceph: The Storage Stack for OpenStack</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/ceph-storage-stack-openstack/index.html" rel="alternate"></link><published>2013-05-28T11:49:00+00:00</published><updated>2013-05-28T11:49:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2013-05-28:fghaas.github.io/xahteiwi.eu/resources/presentations/ceph-storage-stack-openstack/index.html</id><summary type="html">&lt;p&gt;Florian's presentation from &lt;a href="http://www.openstack-israel.org"&gt;OpenStack
Israel&lt;/a&gt; 2013. After a brief
introduction into Ceph, Florian dives into OpenStack specific Ceph
features and outlines RBD integration with Glance and Cinder, and
explains RadosGW Swift compatibility.&lt;/p&gt;
&lt;!--break--&gt;
&lt;p&gt;On May 27, 2013, Florian presented this talk at the third annual
OpenStack Israel event, hosted by
&lt;a href="http://www.gigaspaces.com/"&gt;Gigaspaces …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Florian's presentation from &lt;a href="http://www.openstack-israel.org"&gt;OpenStack
Israel&lt;/a&gt; 2013. After a brief
introduction into Ceph, Florian dives into OpenStack specific Ceph
features and outlines RBD integration with Glance and Cinder, and
explains RadosGW Swift compatibility.&lt;/p&gt;
&lt;!--break--&gt;
&lt;p&gt;On May 27, 2013, Florian presented this talk at the third annual
OpenStack Israel event, hosted by
&lt;a href="http://www.gigaspaces.com/"&gt;Gigaspaces&lt;/a&gt; in Herzliya just outside Tel
Aviv. This presentation was very well received by the event attendees,
with some rating it a among the top two talks in the event.&lt;/p&gt;
&lt;p&gt;Use the PgUp/PgDown keys to navigate through the presentation, or just
advance by hitting the spacebar.&lt;/p&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//fghaas.github.io/openstackisrael2013/"&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><category term="Ceph"></category><category term="Conference"></category></entry><entry><title>Enter the cuttlefish!</title><link href="fghaas.github.io/xahteiwi.eu/blog/2013/05/07/enter-the-cuttlefish/index.html" rel="alternate"></link><published>2013-05-07T07:43:00+00:00</published><updated>2013-05-07T07:43:00+00:00</updated><author><name>florian</name></author><id>tag:None,2013-05-07:fghaas.github.io/xahteiwi.eu/blog/2013/05/07/enter-the-cuttlefish/index.html</id><summary type="html">&lt;p&gt;Today, the developers released Ceph 0.61, codenamed cuttlefish. There
are some interesting features in this new release, take a look.&lt;/p&gt;
&lt;p&gt;One thing that will undoubtedly make Ceph a lot more palatable to
RHEL/CentOS users is the &lt;strong&gt;availability of Ceph in EPEL&lt;/strong&gt;. This was
&lt;a href="http://www.inktank.com/ceph/ceph-is-in-epel-and-why-red-hat-users-should-care/"&gt;originally announced in late
March …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today, the developers released Ceph 0.61, codenamed cuttlefish. There
are some interesting features in this new release, take a look.&lt;/p&gt;
&lt;p&gt;One thing that will undoubtedly make Ceph a lot more palatable to
RHEL/CentOS users is the &lt;strong&gt;availability of Ceph in EPEL&lt;/strong&gt;. This was
&lt;a href="http://www.inktank.com/ceph/ceph-is-in-epel-and-why-red-hat-users-should-care/"&gt;originally announced in late
March&lt;/a&gt;,
but 0.61 is the first supported release that comes with Red Hat
compatible RPMs. Note that at the time of writing, EPEL is obviously
&lt;a href="http://dl.fedoraproject.org/pub/epel/testing/6/x86_64/"&gt;still stuck on the 0.56 bobtail
release&lt;/a&gt;, but it
is expected that cuttlefish support will follow shortly. In the interim,
cuttlefish packages are available outside EPEL, &lt;a href="http://ceph.com/docs/master/install/rpm/"&gt;on the ceph.com yum
repo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This allows you to run a Ceph cluster on RHEL/CentOS. It does, however
come with a few limitations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can't use RBD from a kvm/libvirt box that is running RHEL. RHEL
    does not ship with librados support enabled in the qemu-kvm builds,
    and removing this limitation would mean for third parties to provide
    their own libvirt/kvm build. As of today, tough, no RBD-support
    libvirt/kvm lives in &lt;a href="http://wiki.centos.org/AdditionalResources/Repositories/CentOSPlus"&gt;CentOS
    Plus&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You can't use the kernel rbd or ceph modules from a client that is
    running RHEL. RBD and Ceph filesystem support is absent from RHEL
    kernels.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I'm curious to see if and when that will change, given Red Hat's &lt;a href="http://www.gluster.org/2013/05/glusterfs-is-ready-for-openstack/"&gt;focus
on
GlusterFS&lt;/a&gt;
as their preferred distributed storage solution. It will be interesting
to see what happens there.&lt;/p&gt;
&lt;p&gt;Another neat little new feature is the ability to &lt;strong&gt;set quotas on
pools,&lt;/strong&gt; which is something that we've frequently had customers ask for
in our consulting practice.&lt;/p&gt;
&lt;p&gt;Then there are &lt;strong&gt;incremental snapshots for RBD,&lt;/strong&gt; another really handy
feature for RBD management in cloud solutions like
&lt;a href="https://www.hastexo.com/knowledge/openstack"&gt;OpenStack&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There's more, and you may head over to the press release and the Inktank
blog for more details. And then you might want to mark your calendars
for one of the following events:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;At the &lt;a href="https://www.hastexo.com/resources/news-releases/der-openstack-dach-tag-2013-das-erste-ganzt%C3%A4gige-event-der-openstack-communi"&gt;OpenStack DACH
    Day&lt;/a&gt;
    at &lt;a href="http://www.linuxtag.org/2013/de/program/program/freitag-24-mai-2013/open-stack.html"&gt;LinuxTag in Berlin on May
    24&lt;/a&gt;,
    Wolfgang Schulze from Inktank gives an overview about Ceph (in
    German, &lt;a href="http://www.eventbrite.com/e/openstack-dach-day-2013-tickets-3206509757"&gt;register
    here&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;At &lt;a href="http://www.openstack-israel.org/"&gt;OpenStack Israel&lt;/a&gt; on May 27,
    I'll be speaking about Ceph integration with OpenStack (in English,
    &lt;a href="http://www.meetup.com/IGTCloud/events/99146542/"&gt;register here&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;And at &lt;a href="http://openstackceeday.com/"&gt;OpenStack CEE&lt;/a&gt; on May 29 in
    Budapest, Martin speaks about &lt;em&gt;Scale-out Made Easy: Petabyte Storage
    with Ceph&lt;/em&gt; (in English, &lt;a href="http://www.eventbrite.com/e/openstack-cee-day-2013-budapest-registration-5634033546"&gt;register
    here&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All these events are expected to sell out beforehand, and they are only
a couple of weeks away. So make sure you grab your seat, and we'll see
you there!&lt;/p&gt;</content><category term="Ceph"></category></entry><entry><title>More Reliable, More Resilient, More Redundant</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/more-reliable-more-resilient-more-redundant/index.html" rel="alternate"></link><published>2013-04-17T16:33:00+00:00</published><updated>2013-04-17T16:33:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2013-04-17:fghaas.github.io/xahteiwi.eu/resources/presentations/more-reliable-more-resilient-more-redundant/index.html</id><summary type="html">&lt;p&gt;Another update on
&lt;a href="https://www.hastexo.com/knowledge/openstack"&gt;OpenStack's&lt;/a&gt; progress in
high availability, for the Grizzly and Havana releases. Presented at the
OpenStack Summit in Portland, on April 17, 2013..&lt;/p&gt;
&lt;!--break--&gt;
&lt;p&gt;Florian gives an overview of infrastructure, compute and networking high
availability development in the April 2013 OpenStack Grizzly release,
and an outlook for OpenStack Havana …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Another update on
&lt;a href="https://www.hastexo.com/knowledge/openstack"&gt;OpenStack's&lt;/a&gt; progress in
high availability, for the Grizzly and Havana releases. Presented at the
OpenStack Summit in Portland, on April 17, 2013..&lt;/p&gt;
&lt;!--break--&gt;
&lt;p&gt;Florian gives an overview of infrastructure, compute and networking high
availability development in the April 2013 OpenStack Grizzly release,
and an outlook for OpenStack Havana.&lt;/p&gt;
&lt;p&gt;Florian's presentation is available below. Use the PgUp/PgDown keys to
navigate through the presentation, or just advance by hitting the
spacebar.&lt;/p&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//fghaas.github.io/openstacksummitapril2013/"&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><category term="OpenStack"></category></entry><entry><title>High Availability Update: You can now vote our talk into the OpenStack summit!</title><link href="fghaas.github.io/xahteiwi.eu/blog/2013/02/21/high-availability-update-you-can-now-vote-our-talk-into-the-openstack-summit/index.html" rel="alternate"></link><published>2013-02-21T06:59:00+00:00</published><updated>2013-02-21T06:59:00+00:00</updated><author><name>florian</name></author><id>tag:None,2013-02-21:fghaas.github.io/xahteiwi.eu/blog/2013/02/21/high-availability-update-you-can-now-vote-our-talk-into-the-openstack-summit/index.html</id><summary type="html">&lt;p&gt;For the upcoming OpenStack Summit in Portland, Syed Armani and I have
&lt;a href="https://www.openstack.org/summit/portland-2013/vote-for-speakers/presentation/531"&gt;submitted a
talk&lt;/a&gt;
on OpenStack high availability. Here's how you can make sure it makes it
into the program.&lt;/p&gt;
&lt;p&gt;Our talk, &lt;strong&gt;&lt;em&gt;More reliable, more resilient, more redundant: High
Availability Update for Grizzly and beyond,&lt;/em&gt;&lt;/strong&gt; is an extended overview …&lt;/p&gt;</summary><content type="html">&lt;p&gt;For the upcoming OpenStack Summit in Portland, Syed Armani and I have
&lt;a href="https://www.openstack.org/summit/portland-2013/vote-for-speakers/presentation/531"&gt;submitted a
talk&lt;/a&gt;
on OpenStack high availability. Here's how you can make sure it makes it
into the program.&lt;/p&gt;
&lt;p&gt;Our talk, &lt;strong&gt;&lt;em&gt;More reliable, more resilient, more redundant: High
Availability Update for Grizzly and beyond,&lt;/em&gt;&lt;/strong&gt; is an extended overview
about current and future high availability features in OpenStack. It
covers infrastructure high availability, HA features in Nova, Quantum,
and several other topics.&lt;/p&gt;
&lt;p&gt;I've given a shorter version of this talk &lt;a href="https://www.hastexo.com/resources/presentations/high-availability-update-grizzly-and-havana"&gt;just this week, at the 2nd
Swiss OpenStack User Group
meetup,&lt;/a&gt;
where apparently &lt;a href="http://www.meetup.com/zhgeeks/events/97648722/"&gt;around 55 people liked it a
lot.&lt;/a&gt; You can take a
look at the slides
&lt;a href="https://www.hastexo.com/resources/presentations/high-availability-update-grizzly-and-havana"&gt;here,&lt;/a&gt;
and there will also be a video that should be available later this week.&lt;/p&gt;
&lt;p&gt;So, to make sure that this talk makes it into the Summit, we need your
help! Voting for Summit sessions is up, and you can vote for our talk
&lt;a href="https://www.openstack.org/summit/portland-2013/vote-for-speakers/presentation/531"&gt;here.&lt;/a&gt;
Please note, you must be an OpenStack Foundation member to vote. If
you're not, and you're into OpenStack, you can &lt;a href="https://www.openstack.org/join/register/"&gt;join (for free!) as an
Individual Member.&lt;/a&gt; Then, you
can immediately &lt;a href="https://www.openstack.org/summit/portland-2013/vote-for-speakers/presentation/531"&gt;proceed to the voting
page&lt;/a&gt;
to cast your vote.&lt;/p&gt;
&lt;p&gt;Thanks for your support, and we hope to see you in Portland!&lt;/p&gt;</content><category term="Conference"></category><category term="OpenStack"></category></entry><entry><title>High Availability Update (Grizzly and Havana)</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/high-availability-update-grizzly-and-havana/index.html" rel="alternate"></link><published>2013-02-20T12:58:00+00:00</published><updated>2013-02-20T12:58:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2013-02-20:fghaas.github.io/xahteiwi.eu/resources/presentations/high-availability-update-grizzly-and-havana/index.html</id><summary type="html">&lt;p&gt;Another update on
&lt;a href="https://www.hastexo.com/knowledge/openstack"&gt;OpenStack's&lt;/a&gt; progress in
high availability, for the Grizzly and Havana releases. Presented at the
Swiss OpenStack User Group meetup in Zurich, on February 19, 2013.&lt;/p&gt;
&lt;!--break--&gt;
&lt;p&gt;Florian gives an overview of infrastructure, compute and networking high
availability development in the run-up to the Grizzly feature freeze.&lt;/p&gt;
&lt;p&gt;An updated …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Another update on
&lt;a href="https://www.hastexo.com/knowledge/openstack"&gt;OpenStack's&lt;/a&gt; progress in
high availability, for the Grizzly and Havana releases. Presented at the
Swiss OpenStack User Group meetup in Zurich, on February 19, 2013.&lt;/p&gt;
&lt;!--break--&gt;
&lt;p&gt;Florian gives an overview of infrastructure, compute and networking high
availability development in the run-up to the Grizzly feature freeze.&lt;/p&gt;
&lt;p&gt;An updated an extended version of this talk is currently proposed for
the upcoming OpenStack Summit in Portland, in April of 2013. You can
&lt;a href="https://www.hastexo.com/blogs/florian/2013/02/21/high-availability-update-you-can-now-vote-our-talk-openstack-summit"&gt;help making sure it gets into the
program!&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Florian's presentation is available below. Use the PgUp/PgDown keys to
navigate through the presentation, or just advance by hitting the
spacebar.&lt;/p&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//fghaas.github.io/chosugmeetup201302/"&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><category term="OpenStack"></category></entry><entry><title>My week at linux.conf.au 2013</title><link href="fghaas.github.io/xahteiwi.eu/blog/2013/01/28/my-week-at-linuxconfau-2013/index.html" rel="alternate"></link><published>2013-01-28T01:27:00+00:00</published><updated>2013-01-28T01:27:00+00:00</updated><author><name>florian</name></author><id>tag:None,2013-01-28:fghaas.github.io/xahteiwi.eu/blog/2013/01/28/my-week-at-linuxconfau-2013/index.html</id><summary type="html">&lt;p&gt;linux.conf.au 2013 has kicked off this morning, and here is a brief
preview of my talks and related activities this week.&lt;/p&gt;
&lt;p&gt;This afternoon, as part of the &lt;a href="https://lca2013.linux.org.au/wiki/Miniconfs/CloudDistributedStorageandHighAvailability"&gt;Cloud, Distributed Storage and High
Availability
miniconf,&lt;/a&gt;
I am moderating the &lt;a href="https://lca2013.linux.org.au/wiki/Miniconfs/CloudDistributedStorageandHighAvailability"&gt;Grand Distributed Storage
Debate&lt;/a&gt;.
In this debate, we'll have Sage …&lt;/p&gt;</summary><content type="html">&lt;p&gt;linux.conf.au 2013 has kicked off this morning, and here is a brief
preview of my talks and related activities this week.&lt;/p&gt;
&lt;p&gt;This afternoon, as part of the &lt;a href="https://lca2013.linux.org.au/wiki/Miniconfs/CloudDistributedStorageandHighAvailability"&gt;Cloud, Distributed Storage and High
Availability
miniconf,&lt;/a&gt;
I am moderating the &lt;a href="https://lca2013.linux.org.au/wiki/Miniconfs/CloudDistributedStorageandHighAvailability"&gt;Grand Distributed Storage
Debate&lt;/a&gt;.
In this debate, we'll have Sage Weil (for Ceph) and John Mark Walker
(for GlusterFS) go head-to-head about the merits of their respective
projects.&lt;/p&gt;
&lt;p&gt;On Tuesday, during the &lt;a href="http://linux.conf.au/schedule/30100/view_talk?day=tuesday"&gt;OpenStack
miniconf,&lt;/a&gt;
I'm doing a talk on integrating Ceph with OpenStack.&lt;/p&gt;
&lt;p&gt;Finally, on Friday afternoon, Tim Serong and I are scheduled to do &lt;a href="http://linux.conf.au/schedule/30091/view_talk?day=friday"&gt;a
full hands-on Ceph
tutorial.&lt;/a&gt;
Watch this space, we'll announce where to download your VM images no
later than Wednesday (as soon as we've figured out just where in the
conference network we can upload it).&lt;/p&gt;</content></entry><entry><title>Solid-state drives and Ceph OSD journals</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/solid-state-drives-and-ceph-osd-journals/index.html" rel="alternate"></link><published>2013-01-13T20:33:58+01:00</published><updated>2013-01-13T20:33:58+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2013-01-13:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/solid-state-drives-and-ceph-osd-journals/index.html</id><summary type="html">&lt;p&gt;Considerations for running Ceph OSD journals on SSDs.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Object Storage Daemons
(&lt;a href="http://ceph.com/docs/master/man/8/ceph-osd/"&gt;OSDs&lt;/a&gt;) are the Ceph
stack's workhorses for data storage. They're significantly smarter
than many of their counterparts in distributed block-storage solutions
(open source or not), and their design is instrumental in securing the
stack's reliability and scalability.&lt;/p&gt;
&lt;p&gt;Among other things, OSDs are responsible for the decentralized
replication — which is highly configurable — of objects in the
store. They do so in a primary-copy fashion: every Ceph object (more
precisely, the Placement Group it is a part of) is written to the
primary OSD first, and from there replicates to one or several replica
OSDs to ensure redundancy. This replication is synchronous, such that
a new or updated object guarantees its availability (in the way
configured by the cluster administrator) before an application is
notified that the write has completed.&lt;/p&gt;
&lt;p&gt;More specifically, in order for an OSD to acknowledge a write as
completed, the new object must have been written to the OSD's
journal. OSDs use a write-ahead mode for local operations: a write
hits the journal first, and from there is then being copied into the
backing filestore. (Note: if your filestore is using btrfs, the
journal is applied in parallel with the filestore write instead. Btrfs
still being experimental, however, this is not a configuration often
used in production.) Thus, for best cluster performance it is crucial
that the journal is fast, whereas the filestore can be comparatively
slow.&lt;/p&gt;
&lt;p&gt;This, in turn, leads to a common design principle for Ceph clusters
that are both fast and cost-effective:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Put your filestores on slow, cheap drives (such as SATA spinners),&lt;/li&gt;
&lt;li&gt;put your journals on fast drives (SSDs, Fusion-IO cards, whatever
  you can afford).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Another common design principle is that you create one OSD per
spinning disk that you have in the system. Many contemporary systems
come with only two SSD slots, and then as many spinners as you
want. That is not a problem for journal capacity — a single OSD's
journal is usually no larger than about 6 GB, so even for a 16-spinner
system (approx. 96GB journal space) appropriate SSDs are available at
reasonable expense.&lt;/p&gt;
&lt;p&gt;Many operators are scared of an SSD suddenly dying a horrible death,
so they put their SSDs in a RAID-1. Many are also tempted to put their
OSD journal partitions onto the same RAID. Another option is to use,
say, one partition on each of your SSD in a RAID for the operating
system installation, and then chop up the rest of your SSDs as
non-RAIDed Ceph OSD journals.&lt;/p&gt;
&lt;p&gt;This creates an interesting situation when you get to more than about
10-or-so OSDs (the exact number is hard to give). Now you have your OS
and several OSD journals on the same physical SSD. SSDs are much
faster than spinners, but they have neither infinite throughput nor
zero latency. Eventually, you might hit your SSD's physical limits for
random I/O all over the place. For example, if one of your hosts dies
and the rest now reshuffles data to restore the desired level of
redundancy, you may see relatively intensive I/O all over the other
OSDs — this is exacerbated in a system where you have few OSD hosts
which host many OSD disks.&lt;/p&gt;
&lt;p&gt;Putting your journal SSDs in a RAID set looks like a good idea at
first. Specifically, Ceph OSDs currently cannot recover from a broken
SSD journal without reinitializing and recovering the entire
filestore. This means that as soon as SSD acting as journal backing
storage burns up, you've effectively lost those OSDs completely and
need to recover them from scratch.&lt;sup id="fnref:mkjournal"&gt;&lt;a class="footnote-ref" href="#fn:mkjournal" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Put them in a RAID-1, problem solved?  Well, not quite, because you've
now duplicated all of your journal writes and you're hitting two SSDs
all over the place. Thus it's generally a much better idea to put half
of your journals on one SSD, and half on the other. If one of your
SSDs burns up you'll still lose the OSDs whose journals it hosts — but
it'll only be half of the OSDs hosted on that node altogether.&lt;/p&gt;
&lt;p&gt;Any such performance issues get worse if some of your OSDs are also
MONs: your OSD journals now compete with your operating system and
your MONs for I/O on the same SSDs. Once your SSDs get hit so hard
that your MONs can't do I/O, those MONs eventually die. This might not
harm your operations if you have sufficient backup MONs available, and
everything will be fine again once your recovery is complete, but it's
still a nuisance. This is remarkably common specifically in POCs, by
the way, where people often try to repurpose three of their old,
two-SSDs-plus-dozens-of-disks storage servers for a 3-node Ceph
cluster.&lt;/p&gt;
&lt;p&gt;So, as you are considering your OSD journal and filestore layout, take
note of the following general guidelines:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;By and large, try to go for a relatively small number of OSDs per
  node, ideally not more than 8. This combined with SSD journals is
  likely to give you the best overall performance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you do go with OSD nodes with a very high number of disks,
  consider dropping the idea of an SSD-based journal. Yes, in this
  kind of setup you might actually do better with journals on the
  spinners.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Alternatively in the same scenario, consider putting your operating
  system install on one or a couple of the spinners (presumably
  smaller ones than the others), and use the (un-RAIDed) SSDs for OSD
  journals exclusively.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Consider having a few dedicated MONs (MONs that are not also OSDs).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Note on &lt;code&gt;ceph-osd --mkjournal&lt;/code&gt;&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:mkjournal"&gt;
&lt;p&gt;Since this article was originally published, a &lt;code&gt;--mkjournal&lt;/code&gt;
option was added to the &lt;code&gt;ceph-osd&lt;/code&gt; command, allowing you to
recreate a journal for an existing OSD. This mitigates the issue
in that you don't need to recreate OSDs from scratch when a
journal device breaks — but the OSDs will still be &lt;strong&gt;temporarily&lt;/strong&gt;
unavailable. &lt;a class="footnote-backref" href="#fnref:mkjournal" rev="footnote" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Ceph"></category><category term="Performance"></category></entry><entry><title>Thoughts on "ecosystems"</title><link href="fghaas.github.io/xahteiwi.eu/blog/2012/12/16/thoughts-on-ecosystems/index.html" rel="alternate"></link><published>2012-12-16T14:47:00+00:00</published><updated>2012-12-16T14:47:00+00:00</updated><author><name>florian</name></author><id>tag:None,2012-12-16:fghaas.github.io/xahteiwi.eu/blog/2012/12/16/thoughts-on-ecosystems/index.html</id><summary type="html">&lt;p&gt;Over the past couple of years, it seems that the term &lt;em&gt;ecosystem&lt;/em&gt; is
being broadly applied to what we previously called a &lt;em&gt;community.&lt;/em&gt; I
don't like that, and here's why.&lt;/p&gt;
&lt;p&gt;The origin of the term &lt;em&gt;ecosystem,&lt;/em&gt; when applied to the environment in
which a software project is being developed, used …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Over the past couple of years, it seems that the term &lt;em&gt;ecosystem&lt;/em&gt; is
being broadly applied to what we previously called a &lt;em&gt;community.&lt;/em&gt; I
don't like that, and here's why.&lt;/p&gt;
&lt;p&gt;The origin of the term &lt;em&gt;ecosystem,&lt;/em&gt; when applied to the environment in
which a software project is being developed, used and promoted, is
unknown, at least to the best of my knowledge. Some say that it was
&lt;a href="http://krow.net/" title="Brian Aker"&gt;Brian Aker&lt;/a&gt; who first spoke of “the
MySQL ecosystem”, and it seemed rather fitting at the time. Presently
though, it seems there's ecosystems everywhere: the Linux ecosystem, the
OpenStack ecosystem, the Python ecosystem, you name it.&lt;/p&gt;
&lt;p&gt;And it annoys me.&lt;/p&gt;
&lt;p&gt;It annoys me not in the way marketing drone babbling annoys me, like
when someone waxes lyrical about synergies or paradigm shifts — that's
the kind of fluff you automatically filter out and disregard, a bit like
page numbers in the slide decks of presenters stuck in the 20th century.
But the ecosystem thing is frequently used also by developers and users,
the actual movers and shakers, in the way we would previously use
&lt;em&gt;community.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Now let's look for a moment at how a community works. A community is
governed by rules and morals. Those can be explicit, as written-down
laws, covenants or contracts. Much more commonly though, they are
implicit: everybody understands them, everybody is expected to abide by
them, and if you break them, you're being shunned — but there's no
requirement to write these rules down.&lt;/p&gt;
&lt;p&gt;When we think about communities, most will naturally associate this with
a large group of people, like a clan or tribe, maybe a few hundred or
even a few thousand individuals. Puny, right? We need something grander,
something that alludes to hundreds or thousands of species with maybe
millions of individuals playing a part. Let's pick a term: &lt;em&gt;ecosystem.&lt;/em&gt;
Yay! Problem solved. Waaaay bigger than a community. So much more
awe-inspiring.&lt;/p&gt;
&lt;p&gt;But guess what: &lt;strong&gt;an ecosystem is fundamentally amoral.&lt;/strong&gt; In an
ecosystem, there is no right or wrong — other than survival being right,
and if it happens to be at everyone else's expense, that doesn't make it
wrong. From the inside perspective of an ecosystem, if an invading
species intrudes and steamrolls the entire habitat, so be it: it just
changed the ecosystem. Nature shrugs. Nature also shrugs at parasites,
disease, deception, camouflage, poison and gangs of predators
collaborating with swift and deadly force to mercilessly kill a
defenseless herbivore.&lt;/p&gt;
&lt;p&gt;Now you're welcome to call me out on my naïveté, and point out that it
is precisely those things that happen in business every day. I am
acutely aware of that. I believe, however, we ought to consider them
evils, and some may consider them &lt;em&gt;necessary&lt;/em&gt; evils at times. They
shouldn't the foundations on which we build our &lt;em&gt;communities.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://en.wikipedia.org/wiki/Speech_acts"&gt;Words matter.&lt;/a&gt; I think we
should use them wisely.&lt;/p&gt;</content><category term="Philosophy"></category></entry><entry><title>On the merits of working from home, in a distributed virtual team</title><link href="fghaas.github.io/xahteiwi.eu/blog/2012/12/06/on-the-merits-of-working-from-home-in-a-distributed-virtual-team/index.html" rel="alternate"></link><published>2012-12-06T17:38:00+00:00</published><updated>2012-12-06T17:38:00+00:00</updated><author><name>florian</name></author><id>tag:None,2012-12-06:fghaas.github.io/xahteiwi.eu/blog/2012/12/06/on-the-merits-of-working-from-home-in-a-distributed-virtual-team/index.html</id><summary type="html">&lt;p&gt;During lunch at the EMEA OpenStack day in London this week, I had a
brief but excellent conversation with fellow OpenStacker &lt;a href="https://twitter.com/adamspiers"&gt;Adam
Spiers&lt;/a&gt; from SUSE. Our chat turned to
the merits of working from home, and he encouraged me to write up a blog
post about some of the ideas …&lt;/p&gt;</summary><content type="html">&lt;p&gt;During lunch at the EMEA OpenStack day in London this week, I had a
brief but excellent conversation with fellow OpenStacker &lt;a href="https://twitter.com/adamspiers"&gt;Adam
Spiers&lt;/a&gt; from SUSE. Our chat turned to
the merits of working from home, and he encouraged me to write up a blog
post about some of the ideas of mine and of my co-founders' which we
have since made hastexo policy, however informal or unwritten.&lt;/p&gt;
&lt;p&gt;Note that much of what follows aren't necessarily original ideas of
ours. Many of my thoughts I owe to some very insightful chats I've had
over the past few months with the delightful &lt;a href="http://sarahnovotny.com/"&gt;Sarah
Novotny&lt;/a&gt;, original co-founder of &lt;a href="http://bluegecko.com/"&gt;Blue
Gecko&lt;/a&gt;, seasoned
&lt;a href="http://www.oscon.com/oscon2012"&gt;OSCON&lt;/a&gt; conference chair and now CIO at
&lt;a href="http://meteor-ent.com/"&gt;Meteor Entertainment&lt;/a&gt;. If you get a chance to
talk to Sarah at a conference and poll her views on this, I can highly
recommend you seize that chance.&lt;/p&gt;
&lt;p&gt;It all starts with the observation that the separation of the place you
live in, and the place you work at, is a fairly recent concept in human
history. Prior to the &lt;a href="http://en.wikipedia.org/wiki/Industrial_Revolution"&gt;Industrial
Revolution&lt;/a&gt;, which
originated in late 18th century England and steamrolled first Europe and
then the rest of the world, no such separation was common: the
blacksmith would live upstairs in his shop, so would the bakerman or the
butcher. The teacher would dwell, with his family, in the local school.
The farmer, and the farmhands, would live on that farm. In such a
setting it follows naturally that the work day spans essentially your
entire waking time: you would start your day's work as soon as you got
up, and finished it when you retired for the night. It would be equally
natural to close the shop and interrupt your work for perhaps an hour at
a time, in order to consume a meal with your family or run an errand, or
to hold the &lt;em&gt;&lt;a href="http://en.wikipedia.org/wiki/Siesta"&gt;siesta&lt;/a&gt;&lt;/em&gt; common in
the Mediterranean to pass the hottest hours of the day.&lt;/p&gt;
&lt;p&gt;Then with the Industrial Revolution, everything changed. In the name of
efficiency and progress, we decided that we had to pool workers in one
place — called a factory, or perhaps a shipyard — because now we needed
collaboration: one person could no longer fulfill the task alone, so we
had to get many people to one place to fulfill it together. And as a
natural continuation of our pre-industrial routine, people would work
ten to twelve hours a day, six days a week — until we realized that it
started messing up our lives, inflicting misery on our families and
social ties. And we invented a new concept called spare time: time we
could spend by ourselves, or with our families and friends, something we
didn't have to ask for in the pre-industrial age when our work and life
would naturally have been integrated. And we gradually got to “advances”
like first the
&lt;a href="http://en.wikipedia.org/wiki/Buffalo_switchmen%27s_strike"&gt;10-hour&lt;/a&gt;
workday, then the 8-hour workday, then the 40-hour &lt;a href="http://en.wikipedia.org/wiki/Workweek"&gt;work
week&lt;/a&gt; when we decided it would be
better to have a rest of two days a week rather than one.&lt;/p&gt;
&lt;p&gt;Then we invented white-collar, office jobs, and we gradually moved from
an industry-dominated to a service-dominated economy. And because by
this time we were all well trained in the rules of industrial life, and
because it had brought us progress and prosperity, we applied the same
concepts to offices that we previously had applied to factories and
shipyards: we would gather everyone in the same place, removed from home
and families, and we would get everyone to accept fixed “office hours”
when all hands would have to be present. Of course, we still needed to
collaborate, it's just that the tasks differed from the ones we faced in
factories and shipyards.&lt;/p&gt;
&lt;p&gt;Fast forward to the early 21st century, where we are suddenly endowed
with an abundance of readily available, cheap technology that allows us
to communicate and collaborate instantly, from almost anywhere. And it
is at this point that the unnatural split between work and non-work
life, which we inflicted upon ourselves during the Industrial Revolution
and which we have managed to rationalize with the brainwash that a
“clean separation” of “work and private life” is “essential” to
well-being — that has become a complete anachronism. It is no longer
vital for the people making up a company to physically be in the same
place to collaborate, to serve customers, and to be productive and make
a difference to communities. In fact, I consider it counterproductive.
We've finally arrived in a position where we can restore the very
natural way for humans to live and work, namely integrated with our
families, from home, connected through technology that enables us to
communicate just as effectively as sitting at the same desk. It also
enables us to live healthier, better lives.&lt;/p&gt;
&lt;p&gt;I'm fully aware that this style of work is probably not for everyone.
But if you're thinking it's not for you (and I was one of those, until a
little over a year ago) it's worth asking yourself &lt;em&gt;why&lt;/em&gt; you're thinking
that. Is it really because you &lt;em&gt;want&lt;/em&gt; to work in an office, or because
everyone has &lt;em&gt;told you&lt;/em&gt; for most of your career that you want to work in
an office?&lt;/p&gt;
&lt;p&gt;Here at hastexo, it took us some time — several months — to figure out
all-electronic collaboration, but the machinery is working extremely
well now. The &lt;a href="https://www.google.com/work/apps/business/"&gt;Google Apps&lt;/a&gt;
stack has been enormously useful for us in that regard. We practically
live in &lt;a href="http://www.google.com/+/learnmore/hangouts/"&gt;Google Hangouts&lt;/a&gt;
and documents shared on &lt;a href="http://drive.google.com"&gt;Google Drive&lt;/a&gt;. We jot
down ideas in Google Docs and sketch architectures in Google Drawings.
We do our weekly standups that way, and increasingly customer meetings,
too. We collaboratively draft and edit slide decks for training. And we
rehearse conference talks via video call. It just works, and it's huge
fun that way. And it enables us to close our laptops and go read bedtime
stories to our kids when we're done.&lt;/p&gt;</content><category term="Philosophy"></category></entry><entry><title>Adding MySQL/Galera resources to Pacemaker</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-adding-mysqlgalera-resources-pacemaker/index.html" rel="alternate"></link><published>2012-12-04T10:53:27+01:00</published><updated>2012-12-04T10:53:27+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-12-04:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-adding-mysqlgalera-resources-pacemaker/index.html</id><summary type="html">&lt;p&gt;Once you have one instance of Galera running, and it is running on the
same node that holds the temporarily-configured cluster IP
(192.168.122.99 in our example), you can add your resources to the
Pacemaker cluster configuration.&lt;/p&gt;
&lt;p&gt;Create a temporary file, such as &lt;code&gt;/tmp/galera.crm&lt;/code&gt;, with the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Once you have one instance of Galera running, and it is running on the
same node that holds the temporarily-configured cluster IP
(192.168.122.99 in our example), you can add your resources to the
Pacemaker cluster configuration.&lt;/p&gt;
&lt;p&gt;Create a temporary file, such as &lt;code&gt;/tmp/galera.crm&lt;/code&gt;, with the following
contents:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;primitive p_ip_mysql_galera ocf:heartbeat:IPaddr2 \
  params nic="eth1" iflabel="galera" \
    ip="192.168.122.99" cidr_netmask="24"
primitive p_mysql ocf:heartbeat:mysql \
  params config="/etc/mysql/my.cnf" \
    pid="/var/run/mysqld/mysqld.pid" \
    socket="/var/run/mysqld/mysqld.sock" \
    binary="/usr/sbin/mysqld" \
  op monitor interval="30s" \
  op start interval="0" timeout="60s" \
  op stop interval="0" timeout="60s"
clone cl_mysql p_mysql \
  meta interleave="true"
colocation c_ip_galera_on_mysql \
  inf: p_ip_mysql_galera cl_mysql
property stonith-enabled="false"
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then, import this into your Pacemaker configuration:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;crm configure load update /tmp/galera.crm
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;What this creates are a couple of Pacemaker resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The cluster IP address, 192.168.122.99
  (&lt;code&gt;p_ip_mysql_galera&lt;/code&gt;). Throughout the lifetime of the cluster, this
  will always be available on one of the nodes where any MySQL/Galera
  instance is running. This is the IP address new Galera nodes use
  when joining the cluster.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The MySQL server itself (&lt;code&gt;cl_mysql&lt;/code&gt;), which will be automatically
  recovered in-place if it ever fails.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="Galera"></category><category term="MySQL"></category><category term="Pacemaker"></category></entry><entry><title>Bootstrapping the Galera cluster</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-bootstrapping-galera-cluster/index.html" rel="alternate"></link><published>2012-12-04T10:53:27+01:00</published><updated>2012-12-04T10:53:27+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-12-04:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-bootstrapping-galera-cluster/index.html</id><summary type="html">&lt;p&gt;In order to bootstrap your Galera cluster, manually bring up the
cluster IP address on the desired interface. In this example, we'll
use 192.168.122.99 and eth1:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ip address add &lt;span class="m"&gt;192&lt;/span&gt;.168.122.99/24 dev eth1 label eth1:galera
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And initialize the Galera cluster:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mysqld --wsrep_cluster_address&lt;span class="o"&gt;=&lt;/span&gt;gcomm …&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;In order to bootstrap your Galera cluster, manually bring up the
cluster IP address on the desired interface. In this example, we'll
use 192.168.122.99 and eth1:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ip address add &lt;span class="m"&gt;192&lt;/span&gt;.168.122.99/24 dev eth1 label eth1:galera
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And initialize the Galera cluster:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mysqld --wsrep_cluster_address&lt;span class="o"&gt;=&lt;/span&gt;gcomm:// &lt;span class="p"&gt;&amp;amp;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note the empty &lt;code&gt;gcomm://&lt;/code&gt; address.&lt;/p&gt;
&lt;p&gt;An avalanche of output is likely to follow. Near the end, you should
see entries similar to these:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[Note] WSREP: Synchronized with group, ready for connections
[Note] mysqld: ready for connections.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;At this point, your MySQL/Galera cluster is properly initialized. It
only has one node, and it is not under cluster management yet, but
it's already a working Galera installation.&lt;/p&gt;</content><category term="Galera"></category><category term="MySQL"></category><category term="Pacemaker"></category></entry><entry><title>Configuring Corosync</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-configuring-corosync/index.html" rel="alternate"></link><published>2012-12-04T10:53:27+01:00</published><updated>2012-12-04T10:53:27+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-12-04:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-configuring-corosync/index.html</id><summary type="html">&lt;p&gt;You now need configure Corosync. The following example configuration
file assumes that your cluster nodes have two network interfaces,
using the 192.168.122.0/24 and 192.168.133.0/24 networks. You will
need to adjust this to your own network configuration.&lt;/p&gt;
&lt;p&gt;Set the contents of &lt;code&gt;/etc/corosync …&lt;/code&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;You now need configure Corosync. The following example configuration
file assumes that your cluster nodes have two network interfaces,
using the 192.168.122.0/24 and 192.168.133.0/24 networks. You will
need to adjust this to your own network configuration.&lt;/p&gt;
&lt;p&gt;Set the contents of &lt;code&gt;/etc/corosync/corosync.conf&lt;/code&gt; as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;compatibility&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;whitetank&lt;/span&gt;

&lt;span class="n"&gt;totem&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;version&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
        &lt;span class="n"&gt;secauth&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt;
        &lt;span class="n"&gt;threads&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="n"&gt;rrp_mode&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;active&lt;/span&gt;
        &lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;
        &lt;span class="kd"&gt;interface&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
                &lt;span class="n"&gt;ringnumber&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
                &lt;span class="n"&gt;bindnetaddr&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;192.168&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;122.0&lt;/span&gt;
                &lt;span class="n"&gt;mcastaddr&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;239.255&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;42.0&lt;/span&gt;
                &lt;span class="n"&gt;mcastport&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;5405&lt;/span&gt;
                &lt;span class="n"&gt;ttl&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="o"&gt;}&lt;/span&gt;
        &lt;span class="kd"&gt;interface&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
                &lt;span class="n"&gt;ringnumber&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
                &lt;span class="n"&gt;bindnetaddr&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;192.168&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;133.0&lt;/span&gt;
                &lt;span class="n"&gt;mcastaddr&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;239.255&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;42.1&lt;/span&gt;
                &lt;span class="n"&gt;mcastport&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;5405&lt;/span&gt;
                &lt;span class="n"&gt;ttl&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;logging&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;fileline&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;off&lt;/span&gt;
        &lt;span class="n"&gt;to_stderr&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;no&lt;/span&gt;
        &lt;span class="n"&gt;to_logfile&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;no&lt;/span&gt;
        &lt;span class="n"&gt;to_syslog&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;yes&lt;/span&gt;
        &lt;span class="n"&gt;debug&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;off&lt;/span&gt;
        &lt;span class="n"&gt;timestamp&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt;
        &lt;span class="n"&gt;logger_subsys&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
                &lt;span class="n"&gt;subsys&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;AMF&lt;/span&gt;
                &lt;span class="n"&gt;debug&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;off&lt;/span&gt;
        &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Also, create an authkey file for node authentication:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;dd &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/dev/urandom &lt;span class="nv"&gt;of&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/etc/corosync/authkey &lt;span class="nv"&gt;bs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;128&lt;/span&gt; &lt;span class="nv"&gt;count&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;
chmod &lt;span class="m"&gt;0400&lt;/span&gt; /etc/corosync/authkey
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And create &lt;code&gt;/etc/corosync/service.d/pacemaker&lt;/code&gt; with the following content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;service {
    name: pacemaker
    ver; 1
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, distribute the configuration across your cluster:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; n in bob charlie&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
  rsync -av /etc/corosync/* &lt;span class="nv"&gt;$n&lt;/span&gt;:/etc/corosync
&lt;span class="k"&gt;done&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And start Corosync on all cluster nodes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;service corosync start
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once Corosync has started on all nodes, you should be able to check its status with the &lt;code&gt;corosync-cfgtool&lt;/code&gt; and &lt;code&gt;corosync-objctl&lt;/code&gt; commands:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# corosync-cfgtool -s&lt;/span&gt;
Printing ring status.
Local node ID &lt;span class="m"&gt;1870309568&lt;/span&gt;
RING ID &lt;span class="m"&gt;0&lt;/span&gt;
    &lt;span class="nv"&gt;id&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;192&lt;/span&gt;.168.122.111
    &lt;span class="nv"&gt;status&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; ring &lt;span class="m"&gt;0&lt;/span&gt; active with no faults
RING ID &lt;span class="m"&gt;1&lt;/span&gt;
    &lt;span class="nv"&gt;id&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;192&lt;/span&gt;.168.133.111
    &lt;span class="nv"&gt;status&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; ring &lt;span class="m"&gt;1&lt;/span&gt; active with no faults
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Both rings should be in the &lt;code&gt;active with no faults&lt;/code&gt; state.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# corosync-objctl runtime.totem.pg.mrp.srp.members&lt;/span&gt;
runtime.totem.pg.mrp.srp.1870309568.ip&lt;span class="o"&gt;=&lt;/span&gt;r&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; ip&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;192&lt;/span&gt;.168.122.111&lt;span class="o"&gt;)&lt;/span&gt; r&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; ip&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;192&lt;/span&gt;.168.133.111&lt;span class="o"&gt;)&lt;/span&gt; 
runtime.totem.pg.mrp.srp.1870309568.join_count&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;
runtime.totem.pg.mrp.srp.1870309568.status&lt;span class="o"&gt;=&lt;/span&gt;joined
runtime.totem.pg.mrp.srp.1887086784.ip&lt;span class="o"&gt;=&lt;/span&gt;r&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; ip&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;192&lt;/span&gt;.168.122.112&lt;span class="o"&gt;)&lt;/span&gt; r&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; ip&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;192&lt;/span&gt;.168.133.112&lt;span class="o"&gt;)&lt;/span&gt; 
runtime.totem.pg.mrp.srp.1887086784.join_count&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;
runtime.totem.pg.mrp.srp.1887086784.status&lt;span class="o"&gt;=&lt;/span&gt;joined
runtime.totem.pg.mrp.srp.1903864000.ip&lt;span class="o"&gt;=&lt;/span&gt;r&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; ip&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;192&lt;/span&gt;.168.122.113&lt;span class="o"&gt;)&lt;/span&gt; r&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; ip&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;192&lt;/span&gt;.168.133.113&lt;span class="o"&gt;)&lt;/span&gt; 
runtime.totem.pg.mrp.srp.1903864000.join_count&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;
runtime.totem.pg.mrp.srp.1903864000.status&lt;span class="o"&gt;=&lt;/span&gt;joined
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;All three nodes members should be in the membership with both of their
interfaces, and their status should be &lt;code&gt;joined&lt;/code&gt;.&lt;/p&gt;</content><category term="Galera"></category><category term="MySQL"></category><category term="Pacemaker"></category></entry><entry><title>Dealing with node failure</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-dealing-node-failure/index.html" rel="alternate"></link><published>2012-12-04T10:53:27+01:00</published><updated>2012-12-04T10:53:27+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-12-04:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-dealing-node-failure/index.html</id><summary type="html">&lt;p&gt;If an entire node happens to get killed, and that node currently does
not hold the Galera IP (192.168.122.99 in our example), then the other
nodes simply continue to function normally, and you can connect to and
use them without interruption. In the example below, &lt;code&gt;alice&lt;/code&gt; has …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If an entire node happens to get killed, and that node currently does
not hold the Galera IP (192.168.122.99 in our example), then the other
nodes simply continue to function normally, and you can connect to and
use them without interruption. In the example below, &lt;code&gt;alice&lt;/code&gt; has left
the cluster:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;============&lt;/span&gt;
&lt;span class="n"&gt;Last&lt;/span&gt; &lt;span class="nl"&gt;updated&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Mon&lt;/span&gt; &lt;span class="n"&gt;Dec&lt;/span&gt;  &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;55&lt;/span&gt; &lt;span class="mi"&gt;2012&lt;/span&gt;
&lt;span class="n"&gt;Last&lt;/span&gt; &lt;span class="nl"&gt;change&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Mon&lt;/span&gt; &lt;span class="n"&gt;Dec&lt;/span&gt;  &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;19&lt;/span&gt; &lt;span class="mi"&gt;2012&lt;/span&gt; &lt;span class="n"&gt;via&lt;/span&gt; &lt;span class="n"&gt;crmd&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="n"&gt;charlie&lt;/span&gt;
&lt;span class="nl"&gt;Stack&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;openais&lt;/span&gt;
&lt;span class="n"&gt;Current&lt;/span&gt; &lt;span class="nl"&gt;DC&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;charlie&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;partition&lt;/span&gt; &lt;span class="n"&gt;with&lt;/span&gt; &lt;span class="n"&gt;quorum&lt;/span&gt;
&lt;span class="nl"&gt;Version&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.1.7&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ee0730e13d124c3d58f00016c3376a1de5323cff&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="n"&gt;Nodes&lt;/span&gt; &lt;span class="n"&gt;configured&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="n"&gt;expected&lt;/span&gt; &lt;span class="n"&gt;votes&lt;/span&gt;
&lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="n"&gt;Resources&lt;/span&gt; &lt;span class="n"&gt;configured&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="o"&gt;============&lt;/span&gt;

&lt;span class="nl"&gt;Online&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;bob&lt;/span&gt; &lt;span class="n"&gt;charlie&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="nl"&gt;OFFLINE&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;alice&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;Full&lt;/span&gt; &lt;span class="n"&gt;list&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="nl"&gt;resources&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

&lt;span class="n"&gt;p_ip_mysql_galera&lt;/span&gt;       &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ocf&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nl"&gt;heartbeat&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;IPaddr2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;       &lt;span class="n"&gt;Started&lt;/span&gt; &lt;span class="n"&gt;bob&lt;/span&gt;
 &lt;span class="n"&gt;Clone&lt;/span&gt; &lt;span class="nl"&gt;Set&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;cl_mysql&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;p_mysql&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
     &lt;span class="nl"&gt;Started&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;bob&lt;/span&gt; &lt;span class="n"&gt;charlie&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
     &lt;span class="nl"&gt;Stopped&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="nl"&gt;p_mysql&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If the node dies that does currently hold the Galera IP
(192.168.122.99 in our example), then the cluster IP shifts to a
different node, and when the failed node returns, it can re-fetch the
cluster state from the node that took over the IP address. In the
example below, in a healthy cluster the IP happens to be running on
&lt;code&gt;bob&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;============
Last updated: Mon Dec  3 22:32:35 2012
Last change: Mon Dec  3 22:23:19 2012 via crmd on charlie
Stack: openais
Current DC: charlie - partition with quorum
Version: 1.1.7-ee0730e13d124c3d58f00016c3376a1de5323cff
3 Nodes configured, 3 expected votes
4 Resources configured.
============

Online: [ bob alice charlie ]

Full list of resources:

p_ip_mysql_galera       (ocf::heartbeat:IPaddr2):       Started bob
 Clone Set: cl_mysql [p_mysql]
     Started: [ alice bob charlie ]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Subsequently, &lt;code&gt;bob&lt;/code&gt; is affected by a failure, and the IP address
shifts to &lt;code&gt;alice&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;============&lt;/span&gt;
&lt;span class="n"&gt;Last&lt;/span&gt; &lt;span class="nl"&gt;updated&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Mon&lt;/span&gt; &lt;span class="n"&gt;Dec&lt;/span&gt;  &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;33&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;33&lt;/span&gt; &lt;span class="mi"&gt;2012&lt;/span&gt;
&lt;span class="n"&gt;Last&lt;/span&gt; &lt;span class="nl"&gt;change&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Mon&lt;/span&gt; &lt;span class="n"&gt;Dec&lt;/span&gt;  &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;19&lt;/span&gt; &lt;span class="mi"&gt;2012&lt;/span&gt; &lt;span class="n"&gt;via&lt;/span&gt; &lt;span class="n"&gt;crmd&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="n"&gt;charlie&lt;/span&gt;
&lt;span class="nl"&gt;Stack&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;openais&lt;/span&gt;
&lt;span class="n"&gt;Current&lt;/span&gt; &lt;span class="nl"&gt;DC&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;charlie&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;partition&lt;/span&gt; &lt;span class="n"&gt;with&lt;/span&gt; &lt;span class="n"&gt;quorum&lt;/span&gt;
&lt;span class="nl"&gt;Version&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.1.7&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ee0730e13d124c3d58f00016c3376a1de5323cff&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="n"&gt;Nodes&lt;/span&gt; &lt;span class="n"&gt;configured&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="n"&gt;expected&lt;/span&gt; &lt;span class="n"&gt;votes&lt;/span&gt;
&lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="n"&gt;Resources&lt;/span&gt; &lt;span class="n"&gt;configured&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="o"&gt;============&lt;/span&gt;

&lt;span class="nl"&gt;Online&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;alice&lt;/span&gt; &lt;span class="n"&gt;charlie&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="nl"&gt;OFFLINE&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;bob&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;Full&lt;/span&gt; &lt;span class="n"&gt;list&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="nl"&gt;resources&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

&lt;span class="n"&gt;p_ip_mysql_galera&lt;/span&gt;       &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ocf&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nl"&gt;heartbeat&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;IPaddr2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;       &lt;span class="n"&gt;Started&lt;/span&gt; &lt;span class="n"&gt;alice&lt;/span&gt;
 &lt;span class="n"&gt;Clone&lt;/span&gt; &lt;span class="nl"&gt;Set&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;cl_mysql&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;p_mysql&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
     &lt;span class="nl"&gt;Started&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;alice&lt;/span&gt; &lt;span class="n"&gt;charlie&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
     &lt;span class="nl"&gt;Stopped&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="nl"&gt;p_mysql&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When &lt;code&gt;bob&lt;/code&gt; returns, it simply connects to &lt;code&gt;alice&lt;/code&gt; (which now hosts the
cluster IP), fetches the database state from there, and continues to
run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;============
Last updated: Mon Dec  3 22:35:46 2012
Last change: Mon Dec  3 22:23:19 2012 via crmd on charlie
Stack: openais
Current DC: charlie - partition with quorum
Version: 1.1.7-ee0730e13d124c3d58f00016c3376a1de5323cff
3 Nodes configured, 3 expected votes
4 Resources configured.
============

Online: [ bob alice charlie ]

Full list of resources:

p_ip_mysql_galera       (ocf::heartbeat:IPaddr2):       Started alice
 Clone Set: cl_mysql [p_mysql]
     Started: [ alice bob charlie ]
&lt;/pre&gt;&lt;/div&gt;</content><category term="Galera"></category><category term="MySQL"></category><category term="Pacemaker"></category></entry><entry><title>MySQL/Galera in Pacemaker High Availability Clusters</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-high-availability-clusters/index.html" rel="alternate"></link><published>2012-12-04T10:53:27+01:00</published><updated>2012-12-04T10:53:27+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-12-04:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-high-availability-clusters/index.html</id><summary type="html">&lt;p&gt;In this walkthrough, you will create a Pacemaker managed MySQL/Galera
cluster. It assumes that you are running on a Debian 6.0 (squeeze)
box, but the concepts should be equally applicable to other platforms
with minimal modifications.&lt;/p&gt;
&lt;p&gt;It also assumes that your Galera cluster will consist of three nodes …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this walkthrough, you will create a Pacemaker managed MySQL/Galera
cluster. It assumes that you are running on a Debian 6.0 (squeeze)
box, but the concepts should be equally applicable to other platforms
with minimal modifications.&lt;/p&gt;
&lt;p&gt;It also assumes that your Galera cluster will consist of three nodes,
named alice, bob and charlie. Furthermore, all cluster nodes can
resolve each other's hostnames.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Please note: All commands in this walkthrough require that you are
logged into your system as root.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First, make sure you have the required packages installed. One of the
easiest ways to get your hands on MySQL/Galera binaries is to install
Percona XtraDB Cluster, which our friends at Percona make available in
their public software repository.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;/etc/apt/sources.list.d/percona.list&lt;/code&gt; with the following
content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;deb&lt;/span&gt; &lt;span class="s"&gt;http://repo.percona.com/apt&lt;/span&gt; &lt;span class="kp"&gt;squeeze&lt;/span&gt; &lt;span class="kp"&gt;main&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Fetch the Percona repository signing key:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;apt-key adv --keyserver hkp://keys.gnupg.net --recv-keys 1C4CBDCDCD2EFD2A
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You also require Pacemaker packages from the Debian backports
repository. Do do so, create &lt;code&gt;/etc/apt/sources.list.d/backports.list&lt;/code&gt;
with the following content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;deb&lt;/span&gt; &lt;span class="s"&gt;http://backports.debian.org/debian-backports&lt;/span&gt; &lt;span class="kp"&gt;squeeze-backports&lt;/span&gt; &lt;span class="kp"&gt;main&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, update your package lists:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;apt-get update
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once that is completed, you are able to install the
&lt;code&gt;percona-xtradb-cluster-server-5.5&lt;/code&gt; package:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;apt-get -y install percona-xtradb-cluster-server-5.5
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that &lt;code&gt;percona-xtradb-cluster-server-5.5&lt;/code&gt; conflicts with the
standard Debian &lt;code&gt;mysql-server&lt;/code&gt; packages, so if you have any of those
installed, they will be removed in the process of installing XtraDB
Cluster.&lt;/p&gt;
&lt;p&gt;Stop the MySQL server services for the time being:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;service mysql stop
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Also required is the pacemaker package (and its dependencies) from
squeeze-backports:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;apt-get -t squeeze-backports install pacemaker
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And finally rsync is required for one of the supported Snapshot State
Transfer (SST) methods for Galera:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;apt-get install rsync
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, all required packages are installed and you're ready to configure
XtraDB Cluster.&lt;/p&gt;</content><category term="Galera"></category><category term="MySQL"></category><category term="Pacemaker"></category></entry><entry><title>Recovering from full cluster shutdown</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-recovering-full-cluster-shutdown/index.html" rel="alternate"></link><published>2012-12-04T10:53:27+01:00</published><updated>2012-12-04T10:53:27+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-12-04:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-recovering-full-cluster-shutdown/index.html</id><summary type="html">&lt;p&gt;If at any time &lt;em&gt;all&lt;/em&gt; of the nodes in your cluster have been taken
down, it is necessary to re-initialize the Galera replication
state. In effect, this is identical to bootstrapping the cluster.&lt;/p&gt;
&lt;p&gt;Start by manually bringing up the cluster IP on one of your nodes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ip address add &lt;span class="m"&gt;192 …&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;If at any time &lt;em&gt;all&lt;/em&gt; of the nodes in your cluster have been taken
down, it is necessary to re-initialize the Galera replication
state. In effect, this is identical to bootstrapping the cluster.&lt;/p&gt;
&lt;p&gt;Start by manually bringing up the cluster IP on one of your nodes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ip address add &lt;span class="m"&gt;192&lt;/span&gt;.168.122.99/24 dev eth1 label eth1:galera
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Re-initialize the Galera cluster:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mysqld --wsrep_cluster_address&lt;span class="o"&gt;=&lt;/span&gt;gcomm:// &lt;span class="p"&gt;&amp;amp;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note the empty &lt;code&gt;gcomm://&lt;/code&gt; address.&lt;/p&gt;
&lt;p&gt;Finally, clear your resource state with &lt;code&gt;crm resource cleanup
cl_mysql&lt;/code&gt;. Pacemaker will leave the running IP address and MySQL
instance untouched, and bring up the additional MySQL instances.&lt;/p&gt;</content><category term="Galera"></category><category term="MySQL"></category><category term="Pacemaker"></category></entry><entry><title>Setting Galera-specific MySQL options</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-setting-galera-specific-mysql-options/index.html" rel="alternate"></link><published>2012-12-04T10:53:27+01:00</published><updated>2012-12-04T10:53:27+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-12-04:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-setting-galera-specific-mysql-options/index.html</id><summary type="html">&lt;p&gt;Now you can proceed with setting Galera specifics in your MySQL
configurations.&lt;/p&gt;
&lt;p&gt;Create a configuration file, &lt;strong&gt;identical on all cluster nodes,&lt;/strong&gt; named
&lt;code&gt;/etc/mysql/conf.d/galera.cnf&lt;/code&gt; with the following content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;[mysqld]&lt;/span&gt;
&lt;span class="na"&gt;bind_address&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;0.0.0.0&lt;/span&gt;
&lt;span class="na"&gt;binlog_format&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;ROW&lt;/span&gt;
&lt;span class="na"&gt;default_storage_engine&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;InnoDB&lt;/span&gt;
&lt;span class="na"&gt;innodb_autoinc_lock_mode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;2&lt;/span&gt;
&lt;span class="na"&gt;innodb_locks_unsafe_for_binlog&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Create another configuration file …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Now you can proceed with setting Galera specifics in your MySQL
configurations.&lt;/p&gt;
&lt;p&gt;Create a configuration file, &lt;strong&gt;identical on all cluster nodes,&lt;/strong&gt; named
&lt;code&gt;/etc/mysql/conf.d/galera.cnf&lt;/code&gt; with the following content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;[mysqld]&lt;/span&gt;
&lt;span class="na"&gt;bind_address&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;0.0.0.0&lt;/span&gt;
&lt;span class="na"&gt;binlog_format&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;ROW&lt;/span&gt;
&lt;span class="na"&gt;default_storage_engine&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;InnoDB&lt;/span&gt;
&lt;span class="na"&gt;innodb_autoinc_lock_mode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;2&lt;/span&gt;
&lt;span class="na"&gt;innodb_locks_unsafe_for_binlog&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Create another configuration file, &lt;strong&gt;specific to each cluster node,&lt;/strong&gt;
named &lt;code&gt;/etc/mysql/conf.d/wsrep.cnf&lt;/code&gt; with the following content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;[mysqld]&lt;/span&gt;
&lt;span class="c1"&gt;# node alice has address 192.168.122.111&lt;/span&gt;
&lt;span class="na"&gt;wsrep_node_address&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;192.168.122.111&lt;/span&gt;
&lt;span class="na"&gt;wsrep_provider&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;/usr/lib/libgalera_smm.so&lt;/span&gt;
&lt;span class="na"&gt;wsrep_slave_threads&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;8&lt;/span&gt;
&lt;span class="na"&gt;wsrep_sst_method&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;rsync&lt;/span&gt;
&lt;span class="na"&gt;wsrep_cluster_address&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;gcomm://192.168.122.99&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;[mysqld]&lt;/span&gt;
&lt;span class="c1"&gt;# node bob has address 192.168.122.112&lt;/span&gt;
&lt;span class="na"&gt;wsrep_node_address&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;192.168.122.112&lt;/span&gt;
&lt;span class="na"&gt;wsrep_provider&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;/usr/lib/libgalera_smm.so&lt;/span&gt;
&lt;span class="na"&gt;wsrep_slave_threads&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;8&lt;/span&gt;
&lt;span class="na"&gt;wsrep_sst_method&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;rsync&lt;/span&gt;
&lt;span class="na"&gt;wsrep_cluster_address&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;gcomm://192.168.122.99&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;[mysqld]&lt;/span&gt;
&lt;span class="c1"&gt;# node charlie has address 192.168.122.111&lt;/span&gt;
&lt;span class="na"&gt;wsrep_node_address&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;192.168.122.113&lt;/span&gt;
&lt;span class="na"&gt;wsrep_provider&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;/usr/lib/libgalera_smm.so&lt;/span&gt;
&lt;span class="na"&gt;wsrep_slave_threads&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;8&lt;/span&gt;
&lt;span class="na"&gt;wsrep_sst_method&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;rsync&lt;/span&gt;
&lt;span class="na"&gt;wsrep_cluster_address&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;gcomm://192.168.122.99&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can now proceed with bootstrapping your cluster.&lt;/p&gt;</content><category term="Galera"></category><category term="MySQL"></category><category term="Pacemaker"></category></entry><entry><title>Starting Pacemaker</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-starting-pacemaker/index.html" rel="alternate"></link><published>2012-12-04T10:53:27+01:00</published><updated>2012-12-04T10:53:27+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-12-04:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-starting-pacemaker/index.html</id><summary type="html">&lt;p&gt;Once Corosync is running, you are able to start the Pacemaker cluster
resource manager on all cluster nodes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;service pacemaker start
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once cluster startup is completed, you should see output similar to
the following when invoking the &lt;code&gt;crm_mon&lt;/code&gt; utility:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;============
Last updated: Mon Dec  3 15:37:59 2012
Last change …&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;Once Corosync is running, you are able to start the Pacemaker cluster
resource manager on all cluster nodes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;service pacemaker start
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once cluster startup is completed, you should see output similar to
the following when invoking the &lt;code&gt;crm_mon&lt;/code&gt; utility:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;============
Last updated: Mon Dec  3 15:37:59 2012
Last change: Mon Dec  3 15:37:58 2012 via crmd on alice
Stack: openais
Current DC: alice - partition with quorum
Version: 1.1.7-ee0730e13d124c3d58f00016c3376a1de5323cff
3 Nodes configured, 3 expected votes
0 Resources configured.
============

Online: [ bob alice charlie ]
&lt;/pre&gt;&lt;/div&gt;</content><category term="Galera"></category><category term="MySQL"></category><category term="Pacemaker"></category></entry><entry><title>Testing resource recovery</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-testing-resource-recovery/index.html" rel="alternate"></link><published>2012-12-04T10:53:27+01:00</published><updated>2012-12-04T10:53:27+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-12-04:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-testing-resource-recovery/index.html</id><summary type="html">&lt;p&gt;If MySQL happens to die in your cluster, Pacemaker will automatically
recover the service in place. To test this, select any node on your
cluster and send the &lt;code&gt;mysqld&lt;/code&gt; process a &lt;code&gt;KILL&lt;/code&gt; signal:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;killall -KILL mysqld
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then, monitor your cluster status with &lt;code&gt;crm_mon -rf&lt;/code&gt;. After a few
seconds, you should …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If MySQL happens to die in your cluster, Pacemaker will automatically
recover the service in place. To test this, select any node on your
cluster and send the &lt;code&gt;mysqld&lt;/code&gt; process a &lt;code&gt;KILL&lt;/code&gt; signal:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;killall -KILL mysqld
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then, monitor your cluster status with &lt;code&gt;crm_mon -rf&lt;/code&gt;. After a few
seconds, you should see one of your &lt;code&gt;p_mysql&lt;/code&gt; clones entering the
&lt;code&gt;FAILED&lt;/code&gt; state:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;============&lt;/span&gt;
&lt;span class="n"&gt;Last&lt;/span&gt; &lt;span class="nl"&gt;updated&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Mon&lt;/span&gt; &lt;span class="n"&gt;Dec&lt;/span&gt;  &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;19&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mo"&gt;03&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt; &lt;span class="mi"&gt;2012&lt;/span&gt;
&lt;span class="n"&gt;Last&lt;/span&gt; &lt;span class="nl"&gt;change&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Mon&lt;/span&gt; &lt;span class="n"&gt;Dec&lt;/span&gt;  &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;54&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt; &lt;span class="mi"&gt;2012&lt;/span&gt; &lt;span class="n"&gt;via&lt;/span&gt; &lt;span class="n"&gt;crmd&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="n"&gt;bob&lt;/span&gt;
&lt;span class="nl"&gt;Stack&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;openais&lt;/span&gt;
&lt;span class="n"&gt;Current&lt;/span&gt; &lt;span class="nl"&gt;DC&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;charlie&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;partition&lt;/span&gt; &lt;span class="n"&gt;with&lt;/span&gt; &lt;span class="n"&gt;quorum&lt;/span&gt;
&lt;span class="nl"&gt;Version&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.1.7&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ee0730e13d124c3d58f00016c3376a1de5323cff&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="n"&gt;Nodes&lt;/span&gt; &lt;span class="n"&gt;configured&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="n"&gt;expected&lt;/span&gt; &lt;span class="n"&gt;votes&lt;/span&gt;
&lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="n"&gt;Resources&lt;/span&gt; &lt;span class="n"&gt;configured&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="o"&gt;============&lt;/span&gt;

&lt;span class="nl"&gt;Online&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;bob&lt;/span&gt; &lt;span class="n"&gt;alice&lt;/span&gt; &lt;span class="n"&gt;charlie&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;Full&lt;/span&gt; &lt;span class="n"&gt;list&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="nl"&gt;resources&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

 &lt;span class="n"&gt;p_ip_mysql_galera&lt;/span&gt;  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ocf&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nl"&gt;heartbeat&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;IPaddr2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;   &lt;span class="n"&gt;Started&lt;/span&gt; &lt;span class="n"&gt;alice&lt;/span&gt;
 &lt;span class="n"&gt;Clone&lt;/span&gt; &lt;span class="nl"&gt;Set&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;cl_mysql&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;p_mysql&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
     &lt;span class="nl"&gt;p_mysql&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ocf&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nl"&gt;heartbeat&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;mysql&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Started&lt;/span&gt; &lt;span class="n"&gt;bob&lt;/span&gt; &lt;span class="n"&gt;FAILED&lt;/span&gt;
     &lt;span class="nl"&gt;Started&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;alice&lt;/span&gt; &lt;span class="n"&gt;charlie&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;Migration&lt;/span&gt; &lt;span class="nl"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;Node&lt;/span&gt; &lt;span class="nl"&gt;alice&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;Node&lt;/span&gt; &lt;span class="nl"&gt;bob&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;Node&lt;/span&gt; &lt;span class="nl"&gt;charlie&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 

&lt;span class="n"&gt;Failed&lt;/span&gt; &lt;span class="nl"&gt;actions&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nl"&gt;p_mysql&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;_monitor_30000&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;bob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;call&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;status&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;complete&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;not&lt;/span&gt; &lt;span class="n"&gt;running&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then, after a few seconds, the resource will automatically recover:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;============
Last updated: Mon Dec  3 19:03:35 2012
Last change: Mon Dec  3 18:54:44 2012 via crmd on bob
Stack: openais
Current DC: charlie - partition with quorum
Version: 1.1.7-ee0730e13d124c3d58f00016c3376a1de5323cff
3 Nodes configured, 3 expected votes
4 Resources configured.
============

Online: [ bob alice charlie ]

Full list of resources:

 p_ip_mysql_galera  (ocf::heartbeat:IPaddr2):   Started alice
 Clone Set: cl_mysql [p_mysql]
     Started: [ alice bob charlie ]

Migration summary:
* Node alice: 
* Node bob: 
   p_mysql:1: migration-threshold=1000000 fail-count=1
* Node charlie: 

Failed actions:
    p_mysql:1_monitor_30000 (node=bob, call=30, rc=7, status=complete): not running
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To subsequently get rid of the entry in the &lt;code&gt;Failed actions&lt;/code&gt; list, use
&lt;code&gt;crm resource cleanup cl_mysql&lt;/code&gt;.&lt;/p&gt;</content><category term="Galera"></category><category term="MySQL"></category><category term="Pacemaker"></category></entry><entry><title>MySQL High Availability Deep Dive</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/mysql-high-availability-deep-dive/index.html" rel="alternate"></link><published>2012-12-03T13:50:00+00:00</published><updated>2012-12-03T13:50:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-12-03:fghaas.github.io/xahteiwi.eu/resources/presentations/mysql-high-availability-deep-dive/index.html</id><summary type="html">&lt;p&gt;This is a tutorial that Florian and Yves Trudeau presented at the
Percona Live UK 2012 conference in London. It covers Pacemaker
integration with DRBD, MySQL Replication, and Galera.&lt;/p&gt;
&lt;!--break--&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="https://docs.google.com/presentation/d/12CzmvBOUpbIOrS2CGbG4PU6v74g99C0gGlh0Zf0Qh7g/embed"&gt;&lt;/iframe&gt;
&lt;/div&gt;</summary><content type="html">&lt;p&gt;This is a tutorial that Florian and Yves Trudeau presented at the
Percona Live UK 2012 conference in London. It covers Pacemaker
integration with DRBD, MySQL Replication, and Galera.&lt;/p&gt;
&lt;!--break--&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="https://docs.google.com/presentation/d/12CzmvBOUpbIOrS2CGbG4PU6v74g99C0gGlh0Zf0Qh7g/embed"&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><category term="Conference"></category><category term="MySQL"></category><category term="Pacemaker"></category><category term="Galera"></category></entry><entry><title>GlusterFS in High Availability Clusters</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/glusterfs-high-availability-clusters/index.html" rel="alternate"></link><published>2012-11-08T08:15:00+00:00</published><updated>2012-11-08T08:15:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-11-08:fghaas.github.io/xahteiwi.eu/resources/presentations/glusterfs-high-availability-clusters/index.html</id><summary type="html">&lt;p&gt;Florian's Pacemaker presentation from the GlusterFS Workshop at
LinuxCon Europe 2012. Presented in Barcelona in November of 2012, this
is a overview of integrating GlusterFS with the Pacemaker cluster
stack.&lt;/p&gt;
&lt;p&gt;This tutorial gives an overview of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Pacemaker stack,&lt;/li&gt;
&lt;li&gt;Using GlusterFS for Pacemaker storage,&lt;/li&gt;
&lt;li&gt;Managing GlusterFS volumes from Pacemaker.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Florian's …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Florian's Pacemaker presentation from the GlusterFS Workshop at
LinuxCon Europe 2012. Presented in Barcelona in November of 2012, this
is a overview of integrating GlusterFS with the Pacemaker cluster
stack.&lt;/p&gt;
&lt;p&gt;This tutorial gives an overview of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Pacemaker stack,&lt;/li&gt;
&lt;li&gt;Using GlusterFS for Pacemaker storage,&lt;/li&gt;
&lt;li&gt;Managing GlusterFS volumes from Pacemaker.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Florian's original presentation included several live demos. In this
version, they have been replaced by placeholders.&lt;/p&gt;
&lt;p&gt;Use the PgUp/PgDown keys to navigate through the presentation, or just
advance by hitting the spacebar.&lt;/p&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//fghaas.github.io/lceu2012/glusterfs.html"&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><category term="Conference"></category><category term="GlusterFS"></category></entry><entry><title>Hands-On With Ceph</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/hands-ceph/index.html" rel="alternate"></link><published>2012-11-08T08:15:00+00:00</published><updated>2012-11-08T08:15:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-11-08:fghaas.github.io/xahteiwi.eu/resources/presentations/hands-ceph/index.html</id><summary type="html">&lt;p&gt;Florian's Ceph tutorial from LinuxCon Europe 2012. Presented in
Barcelona in November of 2012, this is a dense summary of the features
of the &lt;a href="https://www.hastexo.com/knowledge/storage-io/ceph"&gt;Ceph&lt;/a&gt;
distributed storage stack.&lt;/p&gt;
&lt;p&gt;This tutorial gives an overview of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Native RADOS object storage,&lt;/li&gt;
&lt;li&gt;The RBD block device,&lt;/li&gt;
&lt;li&gt;ReSTful object storage with radosgw,&lt;/li&gt;
&lt;li&gt;the Ceph distributed …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;Florian's Ceph tutorial from LinuxCon Europe 2012. Presented in
Barcelona in November of 2012, this is a dense summary of the features
of the &lt;a href="https://www.hastexo.com/knowledge/storage-io/ceph"&gt;Ceph&lt;/a&gt;
distributed storage stack.&lt;/p&gt;
&lt;p&gt;This tutorial gives an overview of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Native RADOS object storage,&lt;/li&gt;
&lt;li&gt;The RBD block device,&lt;/li&gt;
&lt;li&gt;ReSTful object storage with radosgw,&lt;/li&gt;
&lt;li&gt;the Ceph distributed filesystem.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Florian's original presentation included several live demos. In this
version, they have been replaced by placeholders.&lt;/p&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//fghaas.github.io/lceu2012/ceph.html"&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><category term="Ceph"></category><category term="Conference"></category></entry><entry><title>Talking Ceph and GlusterFS at LinuxCon Europe</title><link href="fghaas.github.io/xahteiwi.eu/blog/2012/10/24/talking-ceph-and-glusterfs-at-linuxcon-europe/index.html" rel="alternate"></link><published>2012-10-24T12:55:00+00:00</published><updated>2012-10-24T12:55:00+00:00</updated><author><name>florian</name></author><id>tag:None,2012-10-24:fghaas.github.io/xahteiwi.eu/blog/2012/10/24/talking-ceph-and-glusterfs-at-linuxcon-europe/index.html</id><summary type="html">&lt;p&gt;Early next month, I'll be off to Barcelona for speaking at LinuxCon
Europe. Here's an overview of my talks.&lt;/p&gt;
&lt;p&gt;November 5-7, the &lt;a href="http://www.linuxfoundation.org/"&gt;Linux Foundation&lt;/a&gt; is
holding the annual &lt;a href="http://events.linuxfoundation.org/events/linuxcon-europe"&gt;LinuxCon
Europe&lt;/a&gt; in one
of Europe's most beautiful cities — some say &lt;em&gt;the&lt;/em&gt; most beautiful —
Barcelona. I will be attending the full conference …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Early next month, I'll be off to Barcelona for speaking at LinuxCon
Europe. Here's an overview of my talks.&lt;/p&gt;
&lt;p&gt;November 5-7, the &lt;a href="http://www.linuxfoundation.org/"&gt;Linux Foundation&lt;/a&gt; is
holding the annual &lt;a href="http://events.linuxfoundation.org/events/linuxcon-europe"&gt;LinuxCon
Europe&lt;/a&gt; in one
of Europe's most beautiful cities — some say &lt;em&gt;the&lt;/em&gt; most beautiful —
Barcelona. I will be attending the full conference, and presenting two
talks.&lt;/p&gt;
&lt;p&gt;Wednesday, November 7, is a day full of tutorials at LinuxCon Europe. I
am presenting &lt;a href="http://linuxconeurope2012.sched.org/event/83ef77ad003a026246f37e639cd562db"&gt;Hands-On with Ceph: Object Storage, Block Storage,
Filesystem &amp;amp;
More&lt;/a&gt;,
a deep dive into the Ceph stack. This is a double-slot tutorial,
scheduled for 2:45 - 4:25pm in the Verdi room.&lt;/p&gt;
&lt;p&gt;Then on Thursday, the GlusterFS community team has invited me to speak
at the &lt;a href="http://linuxconeurope2012.sched.org/overview/type/gluster+workshop"&gt;Gluster
Workshop.&lt;/a&gt; This
workshop is complimentary to &lt;a href="http://events.linuxfoundation.org/events/linuxcon-europe/attend/register"&gt;registered LinuxCon Europe
attendees&lt;/a&gt;,
but you can also &lt;a href="https://www.regonline.com/Register/Checkin.aspx?EventID=1109147"&gt;register
separately&lt;/a&gt;
just for the workshop. In that talk, I'll speak about &lt;a href="http://linuxconeurope2012.sched.org/event/2b898583721726bd6a8d8e15af2084d8"&gt;GlusterFS in High
Availability Clusters: Integration with the Pacemaker HA
Stack&lt;/a&gt;.
It's also a 1-hour slot, from 10 - 11am in Vivaldi.&lt;/p&gt;
&lt;p&gt;I'm pretty excited about this trip: it's close to home, it's a great
conference, and I've never been to Barcelona before. So, if you're
headed there, please &lt;a href="https://www.hastexo.com/users/florian/contact"&gt;drop me a
note&lt;/a&gt; and let me know so
we can catch up. Thanks — see you there!&lt;/p&gt;</content><category term="Ceph"></category><category term="Conference"></category><category term="GlusterFS"></category></entry><entry><title>Migrating virtual machines from block-based storage to RADOS/Ceph</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/migrating-virtual-machines-block-based-storage-radosceph/index.html" rel="alternate"></link><published>2012-10-22T15:31:23+01:00</published><updated>2012-10-22T15:31:23+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-10-22:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/migrating-virtual-machines-block-based-storage-radosceph/index.html</id><summary type="html">&lt;p&gt;Ceph allows you to replace existing SAN storage (or SAN drop-in
substitutes) with a flexible storage solution with real scale-out
capabilities. Here is how you migrate existing virtual machines
managed by libvirt from block-based storage to a Ceph based storage
solution.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;What you'll need in order to successfully manage …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Ceph allows you to replace existing SAN storage (or SAN drop-in
substitutes) with a flexible storage solution with real scale-out
capabilities. Here is how you migrate existing virtual machines
managed by libvirt from block-based storage to a Ceph based storage
solution.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;What you'll need in order to successfully manage the migration from
block-based storage to a working Ceph cluster is this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A working Ceph cluster. You probably guessed this one. More
  specifically, you should have&lt;/li&gt;
&lt;li&gt;access to the client.admin key of your RADOS
    installation. Usually, the key will be stored in /etc/ceph/keyring
    on nodes running RADOS.&lt;/li&gt;
&lt;li&gt;a RADOS pool in which you can create RBD images. You can either
    use the standard rbd pool or create your own pool. We'll use the
    libvirt pool throughout the following example.&lt;/li&gt;
&lt;li&gt;a set of credentials for a client to connect to the cluster and
    create and use RBD devices. If you use a libvirt version &amp;lt; 0.9.7,
    you will have to use the default client.admin credentials for this
    purpose. If you run libvirt 0.9.7 or later, you should use a
    separate set of credentials (i.e. create a user called
    e.g. client.rbd and use that one). That user should have at least
    the allow r permission on your mons, and allow rw on your osds
    (the latter you can restrict to the rbd pool used if you wish).&lt;/li&gt;
&lt;li&gt;qemu in version 0.14 or higher&lt;/li&gt;
&lt;li&gt;libvirt in version 0.8.7 or higher (0.9.7 or higher if you want to
  use a separate user for this)&lt;/li&gt;
&lt;li&gt;Ceph 0.48 ("argonaut") or higher&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Getting Started&lt;/h2&gt;
&lt;p&gt;When migrating a VM from block-based storage to a Ceph cluster, you
unfortunately can't avoid a period of downtime (after all, you won't
be able to reliably copy a filesystem from place A to B while it's
still changing on the go). So the first thing to do is shut down a
currently running virtual machine, like we will do with the
ubuntu-amd64-alice VM in this example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;virsh shutdown ubuntu-amd64-alice
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then you need to create an RBD image within that pool. Suppose you
would like to create one that is 100GB in size (recall, all RBD images
are thin-provisioned, so it won't actually use 100GB in the Ceph
cluster right from the start).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;qemu-img create -f rbd rbd:libvirt/ubuntu-amd64-alice 100G
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This means you are connecting to the Ceph mon servers (defined in the
default configuration file, /etc/ceph/ceph.conf) using the
client.admin identity, whose authentication key should be stored in
/etc/ceph/keyring. The nominal image size is 102400MB, it's part of
the libvirt pool and its name is a hardly creative ubuntu-amd64-alice.&lt;/p&gt;
&lt;p&gt;You can run this command from any node inside or outside your Ceph
cluster, as long as the configuration file and authentication
credentials are stored in the appropriate location. The next step,
however, is one that you must complete on the node where you can
currently access your block-based storage. This could either be the
machine that you have your VM's device currently connected to via
iSCSI or - if you are using a SAN drop-in replacement based on DRBD -
the machine that currently has the VM's DRBD resource in Primary mode.&lt;/p&gt;
&lt;p&gt;If you are unsure what your VM's block device is, take a look at the
VM's configuration with&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;virsh dumpxml ubuntu-amd64-alice
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;to find out the actual device name (look out for paragraphs including
a &lt;disk&gt; statement). In our case, the actual device is
/dev/drbd/by-res/vm-ubuntu-amd64-alice. Now let's go ahead and do the
actual conversion. Please note: For the following command to work, you
need a properly populated /etc/ceph directory because that is where
qemu-img gets its information from. This is the command that initiates
the conversion:&lt;/disk&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;qemu-img convert -f raw -O rbd \
  /dev/drbd/by-res/vm-ubuntu-amd64-alice \
  rbd:libvirt/ubuntu-amd64-alice
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once the qemu-img command has completed, the actual conversion of your
data is already done. That was easy, wasn't it? The final step is to
change your libvirt VM configuration file to reflect the changes.&lt;/p&gt;
&lt;h2&gt;Adapting the VM's libvirt configuration (libvirt &amp;lt; 0.9.7)&lt;/h2&gt;
&lt;p&gt;If we want our VM to run on top of a Ceph object store, we need to
tell libvirt how to start the VM appropriately. Luckily, current
versions of libvirt support Ceph-based RBD backing devices out of the
box. Please note: All following steps assume that you have your
/etc/ceph set up properly. This means that a working ceph.conf and a
keyring file containing the authentication key for client.admin is
present.&lt;/p&gt;
&lt;p&gt;Open up your VM's configuration for editing with&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;virsh edit ubuntu-amd64-alice
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and scroll down to the VM's disk definition. In our example, that part of the configuration looks like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;disk&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;'block'&lt;/span&gt; &lt;span class="na"&gt;device=&lt;/span&gt;&lt;span class="s"&gt;'disk'&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;driver&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;'qemu'&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;'raw'&lt;/span&gt; &lt;span class="na"&gt;cache=&lt;/span&gt;&lt;span class="s"&gt;'none'&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;source&lt;/span&gt; &lt;span class="na"&gt;dev=&lt;/span&gt;&lt;span class="s"&gt;'/dev/drbd/by-res/vm-ubuntu-amd64-alice'&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;target&lt;/span&gt; &lt;span class="na"&gt;dev=&lt;/span&gt;&lt;span class="s"&gt;'vda'&lt;/span&gt; &lt;span class="na"&gt;bus=&lt;/span&gt;&lt;span class="s"&gt;'virtio'&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;address&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;'pci'&lt;/span&gt; &lt;span class="na"&gt;domain=&lt;/span&gt;&lt;span class="s"&gt;'0x0000'&lt;/span&gt; &lt;span class="na"&gt;bus=&lt;/span&gt;&lt;span class="s"&gt;'0x00'&lt;/span&gt; &lt;span class="na"&gt;slot=&lt;/span&gt;&lt;span class="s"&gt;'0x05'&lt;/span&gt; &lt;span class="na"&gt;function=&lt;/span&gt;&lt;span class="s"&gt;'0x0'&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/disk&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Replace it with an entry using our RBD image:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;disk&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;'network'&lt;/span&gt; &lt;span class="na"&gt;device=&lt;/span&gt;&lt;span class="s"&gt;'disk'&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;driver&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;'qemu'&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;'raw'&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;source&lt;/span&gt; &lt;span class="na"&gt;protocol=&lt;/span&gt;&lt;span class="s"&gt;'rbd'&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;'libvirt/ubuntu-amd64-alice'&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;host&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;'192.168.133.111'&lt;/span&gt; &lt;span class="na"&gt;port=&lt;/span&gt;&lt;span class="s"&gt;'6789'&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;host&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;'192.168.133.112'&lt;/span&gt; &lt;span class="na"&gt;port=&lt;/span&gt;&lt;span class="s"&gt;'6789'&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;host&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;'192.168.133.113'&lt;/span&gt; &lt;span class="na"&gt;port=&lt;/span&gt;&lt;span class="s"&gt;'6789'&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;/source&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;target&lt;/span&gt; &lt;span class="na"&gt;dev=&lt;/span&gt;&lt;span class="s"&gt;'vda'&lt;/span&gt; &lt;span class="na"&gt;bus=&lt;/span&gt;&lt;span class="s"&gt;'virtio'&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;address&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;'pci'&lt;/span&gt; &lt;span class="na"&gt;domain=&lt;/span&gt;&lt;span class="s"&gt;'0x0000'&lt;/span&gt; &lt;span class="na"&gt;bus=&lt;/span&gt;&lt;span class="s"&gt;'0x00'&lt;/span&gt; &lt;span class="na"&gt;slot=&lt;/span&gt;&lt;span class="s"&gt;'0x05'&lt;/span&gt; &lt;span class="na"&gt;function=&lt;/span&gt;&lt;span class="s"&gt;'0x0'&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/disk&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Be sure to replace the three IPs in the above example with the actual
IPs of your MON servers.&lt;/p&gt;
&lt;p&gt;Finally, start your virtual machine:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;virsh start ubuntu-amd64-alice
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Adapting the VM's libvirt configuration (libvirt &amp;gt;= 0.9.7)&lt;/h2&gt;
&lt;p&gt;Starting with libvirt 0.9.7, you can use a user other than
client.admin to access RBD images via libvirt. We recommend to do
this. Creating such a setup works very similar to the one without a
separate user; the main difference is that it requires you to define a
secret in libvirt for the VM. First of all, figure out what user you
will be using from within libvirt and where that user's authentication
key is stored. For this example, we will assume that the user is
called client.rbd and that this user's key is stored in
/etc/ceph/keyring.client.rbd. Now, create a new UUID by calling&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;uuidgen
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;on the command line. The UUID for our example will be
5cddc503-9c29-4aa8-943a-c097f87677cf.  Then, open
/etc/libvirt/secrets/ubuntu-amd64-alice.xml and define a secret block
in there:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;secret&lt;/span&gt; &lt;span class="na"&gt;ephemeral=&lt;/span&gt;&lt;span class="s"&gt;"no"&lt;/span&gt; &lt;span class="na"&gt;private=&lt;/span&gt;&lt;span class="s"&gt;"no"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;uuid&amp;gt;&lt;/span&gt;5cddc503-9c29-4aa8-943a-c097f87677cf&lt;span class="nt"&gt;&amp;lt;/uuid&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;usage&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;"ceph"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;client.rbd secret&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/usage&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/secret&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Be sure to replace the example's UUID with your own, self-generated
value. Make libvirt add this secret to its internal keyring:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;virsh secret-define \
  /etc/libvirt/secrets/ubuntu-amd64-alice.xml
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now find out your user's secret key. Do&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ceph auth get-or-create client.rbd
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and take note of the key. In our example,
AQB0Q4ZQYDB2MBAAYzWmHvpg7t1MzV1E0jkBww== is the key that will allow us
access as client.rbd. Then define the actual password for our secret
definition:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;virsh secret-set-value \
  5cddc503-9c29-4aa8-943a-c097f87677cf \
  AQB0Q4ZQYDB2MBAAYzWmHvpg7t1MzV1E0jkBww==
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Again, be sure to use your self-generated UUID instead of the one in
this example. Also replace the example key with your real
key. Finally, go ahead and adapt your VM settings. Open your VM
configuration with&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;virsh edit ubuntu-amd64-alice
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and scroll down to the VM's disk definition. In our example, that part of the configuration looks like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;disk&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;'block'&lt;/span&gt; &lt;span class="na"&gt;device=&lt;/span&gt;&lt;span class="s"&gt;'disk'&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;driver&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;'qemu'&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;'raw'&lt;/span&gt; &lt;span class="na"&gt;cache=&lt;/span&gt;&lt;span class="s"&gt;'none'&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;source&lt;/span&gt; &lt;span class="na"&gt;dev=&lt;/span&gt;&lt;span class="s"&gt;'/dev/drbd/by-res/vm-ubuntu-amd64-alice'&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;target&lt;/span&gt; &lt;span class="na"&gt;dev=&lt;/span&gt;&lt;span class="s"&gt;'vda'&lt;/span&gt; &lt;span class="na"&gt;bus=&lt;/span&gt;&lt;span class="s"&gt;'virtio'&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;address&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;'pci'&lt;/span&gt; &lt;span class="na"&gt;domain=&lt;/span&gt;&lt;span class="s"&gt;'0x0000'&lt;/span&gt; &lt;span class="na"&gt;bus=&lt;/span&gt;&lt;span class="s"&gt;'0x00'&lt;/span&gt; &lt;span class="na"&gt;slot=&lt;/span&gt;&lt;span class="s"&gt;'0x05'&lt;/span&gt; &lt;span class="na"&gt;function=&lt;/span&gt;&lt;span class="s"&gt;'0x0'&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/disk&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Replace it with an entry using our RBD image:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;disk&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;'network'&lt;/span&gt; &lt;span class="na"&gt;device=&lt;/span&gt;&lt;span class="s"&gt;'disk'&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;driver&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;'qemu'&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;'raw'&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;auth&lt;/span&gt; &lt;span class="na"&gt;username=&lt;/span&gt;&lt;span class="s"&gt;'rbd'&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;secret&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;'ceph'&lt;/span&gt; &lt;span class="na"&gt;usage=&lt;/span&gt;&lt;span class="s"&gt;'client.rbd secret'&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;/auth&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;source&lt;/span&gt; &lt;span class="na"&gt;protocol=&lt;/span&gt;&lt;span class="s"&gt;'rbd'&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;'libvirt/ubuntu-amd64-alice'&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;host&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;'192.168.133.111'&lt;/span&gt; &lt;span class="na"&gt;port=&lt;/span&gt;&lt;span class="s"&gt;'6789'&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;host&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;'192.168.133.112'&lt;/span&gt; &lt;span class="na"&gt;port=&lt;/span&gt;&lt;span class="s"&gt;'6789'&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;host&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;'192.168.133.113'&lt;/span&gt; &lt;span class="na"&gt;port=&lt;/span&gt;&lt;span class="s"&gt;'6789'&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;/source&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;target&lt;/span&gt; &lt;span class="na"&gt;dev=&lt;/span&gt;&lt;span class="s"&gt;'vda'&lt;/span&gt; &lt;span class="na"&gt;bus=&lt;/span&gt;&lt;span class="s"&gt;'virtio'&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;address&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;'pci'&lt;/span&gt; &lt;span class="na"&gt;domain=&lt;/span&gt;&lt;span class="s"&gt;'0x0000'&lt;/span&gt; &lt;span class="na"&gt;bus=&lt;/span&gt;&lt;span class="s"&gt;'0x00'&lt;/span&gt; &lt;span class="na"&gt;slot=&lt;/span&gt;&lt;span class="s"&gt;'0x05'&lt;/span&gt; &lt;span class="na"&gt;function=&lt;/span&gt;&lt;span class="s"&gt;'0x0'&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/disk&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Be sure to replace the three IPs in the above example with the actual
IPs of your MON servers.&lt;/p&gt;
&lt;p&gt;Finally, start your virtual machine:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;virsh start ubuntu-amd64-alice
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That's it. Your VM should now boot up and use its RBD image from Ceph
instead of its original block-based storage backing device.&lt;/p&gt;</content><category term="Ceph"></category><category term="libvirt"></category></entry><entry><title>Pacemaker and the recent GitHub service interruption</title><link href="fghaas.github.io/xahteiwi.eu/blog/2012/09/26/pacemaker-and-the-recent-github-service-interruption/index.html" rel="alternate"></link><published>2012-09-26T11:32:00+00:00</published><updated>2012-09-26T11:32:00+00:00</updated><author><name>florian</name></author><id>tag:None,2012-09-26:fghaas.github.io/xahteiwi.eu/blog/2012/09/26/pacemaker-and-the-recent-github-service-interruption/index.html</id><summary type="html">&lt;p&gt;It never fails. Someone manages to break their Pacemaker cluster, and
&lt;a href="http://openlife.cc/author"&gt;Henrik&lt;/a&gt; starts preaching &lt;a href="http://openlife.cc/blogs/2012/september/failover-evil"&gt;his usual sermon
of why Pacemaker is
terrible&lt;/a&gt; and why
you should never-ever use it. And when that someone is
&lt;a href="https://github.com/"&gt;GitHub&lt;/a&gt;, which we all know, use and love, then
that sermon gets a bit of excess …&lt;/p&gt;</summary><content type="html">&lt;p&gt;It never fails. Someone manages to break their Pacemaker cluster, and
&lt;a href="http://openlife.cc/author"&gt;Henrik&lt;/a&gt; starts preaching &lt;a href="http://openlife.cc/blogs/2012/september/failover-evil"&gt;his usual sermon
of why Pacemaker is
terrible&lt;/a&gt; and why
you should never-ever use it. And when that someone is
&lt;a href="https://github.com/"&gt;GitHub&lt;/a&gt;, which we all know, use and love, then
that sermon gets a bit of excess attention. Let's take a quick look at
the facts.&lt;/p&gt;
&lt;!--break--&gt;
&lt;!--break--&gt;
&lt;p&gt;The week of September 10, GitHub suffered a couple of outages which
caused a total downtime of 1 hour and 46 minutes, as
&lt;a href="https://github.com/jnewland"&gt;Jesse&lt;/a&gt; precisely pointed out &lt;a href="https://github.com/blog/1261-github-availability-this-week"&gt;in a blog
post&lt;/a&gt;.
Exhibiting the excellent transparency that GitHub always offers at any
time its infrastructure is affected by issues (remember &lt;a href="https://github.com/blog/1068-public-key-security-vulnerability-and-mitigation"&gt;their
role-model behavior in an SSH security
incident&lt;/a&gt;
a few months back), Jesse explains, in a very detailed way, what
happened on one of their Pacemaker clusters.&lt;/p&gt;
&lt;p&gt;Now, all of what follows is based exclusively on the information in that
blog post of Jesse's. I have no inside knowledge of the incident, so my
picture may be incomplete or skewed. But here's my take on it anyway. I
do encourage you to read Jesse's post full-length, as the rest of this
post otherwise won't make much sense. I'll just quote certain pieces of
it and comment on them here.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Please note:&lt;/strong&gt; nothing in this post should be construed as a put-down
of GitHub's excellent staff. They run a fantastic service and do an
awesome job. It's just that their post-mortem seems to have created some
misconceptions in the MySQL community about the Pacemaker stack as a
whole, and those I'd like to help rectify. Also, I'm posting this in the
hope that it provides useful insight to both the GitHub folks, and to
anyone else facing similar issues.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Enable Maintenance Mode when you should&lt;/h2&gt;
&lt;p&gt;From the original post:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Monday's migration caused higher load on the database than our
operations team has previously seen during these sorts of migrations.
So high, in fact, that they caused Percona Replication Manager's
health checks to fail on the master. In response to the failed master
health check, Percona Replication manager moved the 'active' role and
the master database to another server in the cluster and stopped MySQL
on the node it perceived as failed.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
At the time of this failover, the new database selected for the
'active' role had a cold InnoDB buffer pool and performed rather
poorly. The system load generated by the site's query load on a cold
cache soon caused Percona Replication Manager's health checks to fail
again, and the 'active' role failed back to the server it was on
originally.&lt;/blockquote&gt;
&lt;p&gt;&lt;/p&gt;
At this point, I decided to disable all health checks by enabling
Pacemaker's &lt;code&gt;maintenance-mode&lt;/code&gt;; an operating mode in which no health
checks or automatic failover actions are performed. Performance on the
site slowly recovered as the buffer pool slowly reached normal levels.
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Now there's actually several issues in there even in this early stage.
Maintenance mode is generally the right thing to do here, but you enable
it &lt;em&gt;before&lt;/em&gt; making large changes to the configuration, and you disable
it when done. If you're uncomfortable with the cluster manager taking
its hands off the entire cluster, and you know what you're doing, you
could also just disable cluster management and monitoring on a specific
resource. Both approaches are explained
&lt;a href="https://www.hastexo.com/resources/hints-and-kinks/maintenance-active-pacemaker-clusters"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Also, as far as "health checks failing" on the master is concerned,
pretty much the only thing that is likely to cause such a failure in
this instance is a timeout, and you can adjust those even on a
per-operation basis in Pacemaker. But even that is unnecessary if you
enable maintenance mode at the right time.&lt;/p&gt;
&lt;h2&gt;"Maintenance mode" really means maintenance mode&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;The following morning, our operations team was notified by a developer
of incorrect query results returning from the node providing the
'standby' role. I investigated the situation and determined that when
the cluster was placed into maintenance-mode the day before, actions
that should have caused the node elected to serve the 'standby' role
to change its replication master and start replicating were prevented
from occurring.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Well, of course. In maintenance mode, Pacemaker takes its hands off your
resources. If you're enabling maintenance mode right in the middle of a
failover, then that's not exactly a stellar idea. If you do, then it's
your job to complete those actions manually.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I determined that the best course of action was to
disable &lt;code&gt;maintenance-mode&lt;/code&gt; to allow Pacemaker and the Percona
Replication Manager to rectify the situation.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;"Best" might be an exaggeration, if I may say so.&lt;/p&gt;
&lt;h2&gt;A segfault and rejected cluster messages&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Upon attempting to disable &lt;code&gt;maintenance-mode&lt;/code&gt;, a Pacemaker segfault
occurred that resulted in a cluster state partition.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;OK, that's bad, but what exactly segfaulted? crmd? attrd? pengine? Or
the master Heartbeat process? But the next piece of information would
have me believe that the segfault really isn't the root cause of the
cluster partition:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;After this update, two nodes (I'll call them 'a' and 'b') rejected
most messages from the third node ('c'), while the third node rejected
most messages from the other two.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now it's a pity that we don't have any version information and logs, but
this looks very much like the "not in our membership" issue present up
to Pacemaker 1.1.6. This is a known issue, the fix is to update to a
more recent version (&lt;a href="https://github.com/ClusterLabs/pacemaker/commit/03f6105592281901cc10550b8ad19af4beb5f72f"&gt;here's the
commit,&lt;/a&gt; on
GitHub of course), and the workaround is to just restart the Pacemaker
services on the affected node(s) while in maintenance mode.&lt;/p&gt;
&lt;h2&gt;A non-quorate partition running MySQL?&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Despite having configured the cluster to require a majority of
machines to agree on the state of the cluster before taking action,
two simultaneous master election decisions were attempted without
proper coordination. In the first cluster, master election was
interrupted by messages from the second cluster and MySQL was stopped.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now this is an example of me being tempted to say, "logs or it didn't
happen." If you've got the default no-quorum-policy of "block", and
you're getting a non-quorate partition, and you don't have any resources
with operations &lt;em&gt;explicitly&lt;/em&gt; configured to ignore quorum, then "two
simultaneous master election decisions" can only refer to the Designated
Coordinator (DC) election, which has no bearing whatsoever on MySQL
master status. Luckily, Pacemaker allows us to take a meaningful
snapshot of all cluster logs and status after the fact with crm_report.
It would be quite interesting to see a tarball from that.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In the second, single-node cluster, node 'c' was elected at 8:19 AM,
and any subsequent messages from the other two-node cluster were
discarded. As luck would have it, the 'c' node was the node that our
operations team previously determined to be out of date. We detected
this fact and powered off this out-of-date node at 8:26 AM to end the
partition and prevent further data drift, taking down all production
database access and thus all access to github.com.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That's obviously a bummer, but really, if that partition is non-quorate,
and Pacemaker hasn't explicitly been configured to ignore that, no
cluster resources would start there. Needless to say a working fencing
configuration would have helped oodles, too.&lt;/p&gt;
&lt;h2&gt;Your cluster has no crystal ball, but it does have a command line&lt;/h2&gt;
&lt;p&gt;I'll skip over most of the rest of the GitHub post, because it's an
explanation of how these backend issues affected GitHub users. I'll just
hop on down to this piece:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The automated failover of our main production database could be
described as the root cause of both of these downtime events. In each
situation in which that occurred, if any member of our operations team
had been asked if the failover should have been performed, the answer
would have been a resounding no.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Well, you could have told your Pacemaker of that fact beforehand. Enable
maintenance mode and you're good to go.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There are many situations in which automated failover is an excellent
strategy for ensuring the availability of a service. After careful
consideration, we've determined that ensuring the availability of our
primary production database is not one of these situations. To this
end, we've made changes to our Pacemaker configuration to ensure
failover of the 'active' database role will only occur when initiated
by a member of our operations team.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That splash you just heard was the bath water. The scream was the baby
being tossed out with it.&lt;/p&gt;
&lt;p&gt;Automated failover is a pretty poor strategy &lt;em&gt;in the middle of a large
configuration change.&lt;/em&gt; And Pacemaker gives you a simple and easy
interface to disable it, by changing a single cluster property. Failure
to do so may result in problems, and in this case it did.&lt;/p&gt;
&lt;p&gt;When you put a baby seat on the passenger side of your car, you disable
the air bag to prevent major injury. But if you take that baby seat out
and an adult passenger rides with you, are you seriously saying you're
going to manually initiate the air bag in case of a crash? I hope you're
not.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Finally, our operations team is performing a full audit of our
Pacemaker and Heartbeat stack focusing on the code path that triggered
the segfault on Tuesday.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That's probably a really good idea. For anyone planning to do the same,
&lt;a href="https://www.hastexo.com/services/checkup"&gt;we can help.&lt;/a&gt;&lt;/p&gt;</content><category term="MySQL"></category><category term="Pacemaker"></category></entry><entry><title>Maintenance in active Pacemaker clusters</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/maintenance-active-pacemaker-clusters/index.html" rel="alternate"></link><published>2012-09-24T19:49:31+01:00</published><updated>2012-09-24T19:49:31+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-09-24:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/maintenance-active-pacemaker-clusters/index.html</id><summary type="html">&lt;p&gt;In a Pacemaker cluster, as in a standalone system, operators must
complete maintenance tasks such as software upgrades and configuration
changes. Here's what you need to keep Pacemaker's built-in monitoring
features from creating unwanted side effects.&lt;/p&gt;
&lt;h2&gt;Maintenance mode&lt;/h2&gt;
&lt;p&gt;This is quite possibly Pacemaker's single most useful feature for
cluster maintenance …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In a Pacemaker cluster, as in a standalone system, operators must
complete maintenance tasks such as software upgrades and configuration
changes. Here's what you need to keep Pacemaker's built-in monitoring
features from creating unwanted side effects.&lt;/p&gt;
&lt;h2&gt;Maintenance mode&lt;/h2&gt;
&lt;p&gt;This is quite possibly Pacemaker's single most useful feature for
cluster maintenance. In maintenance mode, Pacemaker essentially takes
a "hands-off" approach to your cluster. Enabling Pacemaker maintenance
mode is very easy using the Pacemaker &lt;code&gt;crm&lt;/code&gt; shell:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;crm configure property maintenance-mode&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;true&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In maintenance mode, you can stop or restart cluster resources at
will. Pacemaker will not attempt to restart them. All resources
automatically become unmanaged, that is, Pacemaker will cease
monitoring them and hence be oblivious about their status. You can
even stop all Pacemaker services on a node, and all the daemons and
processes originally started as Pacemaker managed cluster resources
will continue to run.&lt;/p&gt;
&lt;p&gt;You should know that when you start Pacemaker services on a node while
the cluster in maintenance mode, Pacemaker will initiate a single
one-shot monitor operation (a "probe") for every resource just so it
has an understanding of what resources are currently running on that
node. It will, however, take no further action other than determining
the resources' status.&lt;/p&gt;
&lt;p&gt;You disable maintenance mode with the crm shell, as well:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;crm configure property maintenance-mode&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;false&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Maintenance mode is something you enable before running other
maintenance actions, not when you're already half-way through
them. And unless you're very well versed in the interdependencies of
resources running on the cluster you're working on, it's usually the
very safest option.&lt;/p&gt;
&lt;p&gt;In short: when doing maintenance on your Pacemaker cluster, by
default, enable maintenance mode before you start, and disable it
after you're done.&lt;/p&gt;
&lt;h2&gt;Disabling monitoring and error recovery on specific resources&lt;/h2&gt;
&lt;p&gt;For any configuration changes that take no more than a few minutes,
involving an admin that is potentially watching a console window the
whole time, maintenance mode is highly recommended. However, enabling
maintenance mode can be a bit hard to argue for large configuration
changes lasting, say, several hours. Think of a massive database
rebuild, for example. In such a case, you may want to put only your
database resource in something like maintenance mode, and have
Pacemaker continue to monitor other resources like normal.&lt;/p&gt;
&lt;p&gt;You can do so by switching the resource to unmanaged mode and disable
its monitor operation:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;crm configure edit p_database
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then change the &lt;code&gt;is-managed&lt;/code&gt; meta  attribute and disable the &lt;code&gt;monitor&lt;/code&gt;
operation:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;meta is-managed=false
op monitor interval=&amp;lt;interval&amp;gt; enabled=false
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once you've done that, you'll effectively have enabled something akin
to maintenance mode for a single resource. You can reverse this as you
would expect:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;crm configure edit p_database
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then change the &lt;code&gt;is-managed&lt;/code&gt; meta attribute and re-enable the
&lt;code&gt;monitor&lt;/code&gt; operation:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;meta is-managed=true
op monitor interval=&amp;lt;interval&amp;gt; enabled=true
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When using this approach, all other resources will be monitored and
automatically recovered as they normally would. Thus, you'll have to
be acutely aware of any side effects your maintenance activities have
on other resources. If you're unsure, you should use the global
maintenance mode instead.&lt;/p&gt;</content><category term="Pacemaker"></category></entry><entry><title>High Availability in OpenStack</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/high-availability-openstack/index.html" rel="alternate"></link><published>2012-08-30T15:48:00+00:00</published><updated>2012-08-30T15:48:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-08-30:fghaas.github.io/xahteiwi.eu/resources/presentations/high-availability-openstack/index.html</id><summary type="html">&lt;p&gt;An update on high-availability development during the
&lt;a href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; Folsom development cycle. This
presentation was delivered August 30, 2012 in San Diego, California. It
was part of the inaugural
&lt;a href="http://events.linuxfoundation.org/events/cloudopen-north-america"&gt;CloudOpen&lt;/a&gt;
conference hosted by the &lt;a href="http://www.linuxfoundation.org"&gt;Linux
Foundation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Following up on his earlier talks at OpenStack Summit and OSCON, Florian
summarizes the high-availability …&lt;/p&gt;</summary><content type="html">&lt;p&gt;An update on high-availability development during the
&lt;a href="http://www.openstack.org"&gt;OpenStack&lt;/a&gt; Folsom development cycle. This
presentation was delivered August 30, 2012 in San Diego, California. It
was part of the inaugural
&lt;a href="http://events.linuxfoundation.org/events/cloudopen-north-america"&gt;CloudOpen&lt;/a&gt;
conference hosted by the &lt;a href="http://www.linuxfoundation.org"&gt;Linux
Foundation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Following up on his earlier talks at OpenStack Summit and OSCON, Florian
summarizes the high-availability features OpenStack gained during the
Folsom development cycle.&lt;/p&gt;
&lt;p&gt;Florian's full presentation is available below.&lt;/p&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//prezi.com/embed/xaclzhzpjmau/"&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><category term="Conference"></category><category term="OpenStack"></category></entry><entry><title>Speaking and BoFing at CloudOpen in San Diego!</title><link href="fghaas.github.io/xahteiwi.eu/blog/2012/08/20/speaking-and-bofing-at-cloudopen-in-san-diego/index.html" rel="alternate"></link><published>2012-08-20T07:26:00+00:00</published><updated>2012-08-20T07:26:00+00:00</updated><author><name>florian</name></author><id>tag:None,2012-08-20:fghaas.github.io/xahteiwi.eu/blog/2012/08/20/speaking-and-bofing-at-cloudopen-in-san-diego/index.html</id><summary type="html">&lt;p&gt;Next week, I will be speaking at the inaugural CloudOpen conference in
San Diego. This is your chance to learn about
&lt;a href="https://www.hastexo.com/knowledge/openstack"&gt;OpenStack&lt;/a&gt; high
availability and
&lt;a href="https://www.hastexo.com/knowledge/storage-io/ceph"&gt;Ceph!&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;August 29-31, San Diego hosts the
first &lt;a href="http://events.linuxfoundation.org/events/cloudopen-north-america"&gt;CloudOpen&lt;/a&gt;
conference, colocated with &lt;a href="http://events.linuxfoundation.org/events/linuxcon-north-america"&gt;LinuxCon North
America&lt;/a&gt;.
CloudOpen is the &lt;a href="http://www.linuxfoundation.org/"&gt;Linux Foundation&lt;/a&gt;'s
brand new, stack-agnostic cloud …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Next week, I will be speaking at the inaugural CloudOpen conference in
San Diego. This is your chance to learn about
&lt;a href="https://www.hastexo.com/knowledge/openstack"&gt;OpenStack&lt;/a&gt; high
availability and
&lt;a href="https://www.hastexo.com/knowledge/storage-io/ceph"&gt;Ceph!&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;August 29-31, San Diego hosts the
first &lt;a href="http://events.linuxfoundation.org/events/cloudopen-north-america"&gt;CloudOpen&lt;/a&gt;
conference, colocated with &lt;a href="http://events.linuxfoundation.org/events/linuxcon-north-america"&gt;LinuxCon North
America&lt;/a&gt;.
CloudOpen is the &lt;a href="http://www.linuxfoundation.org/"&gt;Linux Foundation&lt;/a&gt;'s
brand new, stack-agnostic cloud conference where OpenStackers can mingle
with CloudStackers and Eucalyptus folks to discuss open-source cloud
solutions.&lt;/p&gt;
&lt;p&gt;It's also the conference where I will be giving my fourth (and likely
last, at least for the time being) incarnation of the High Availability
for OpenStack talk I first delivered at the Folsom design summit back in
April. Since then, we've had a lot of community involvement for HA in
OpenStack, and have made some excellent progress, and I will be more
than happy to report on that. This presentation is on &lt;a href="http://cloudopen2012.sched.org/event/06939ee7fd5fe48bf202525bbd7e506d#.UDIX9hXwh2M"&gt;Thursday,
2:25-3:10pm in Executsoemive Center Room
2&lt;/a&gt;,
in the &lt;em&gt;Operations&lt;/em&gt; track.&lt;/p&gt;
&lt;p&gt;Also, &lt;a href="http://ceph.com/community/people-profile/sage-weil/"&gt;Sage Weil&lt;/a&gt;
of Ceph fame is joining me for an birds-of-a-feather (BoF) session on
Ceph. &lt;a href="http://rtrk.us/"&gt;Ross Turk&lt;/a&gt; and I had such an excellent turnout
(and a great time) in the Ceph BoF at OSCON that we just had to do
another. And Sage agreed to take part, which is excellent. He has &lt;a href="http://cloudopen2012.sched.org/event/f3e84388068b1855c5a705a97c917f44#.UDIZeBXwh2M"&gt;a
talk on Ceph in the main conference
track&lt;/a&gt;
as well.&lt;/p&gt;
&lt;p&gt;The conference organizers do not announce BoF sessions ahead of time on
the CloudOpen web site, so I've simply &lt;a href="https://plus.google.com/events/cq28o7cvj9dg1ki1om3clfo8700/110443614427234590648"&gt;set up a Google+
event&lt;/a&gt;
for you to check in on. The exact location is still TBD (we will be
assigned a room based on availability), but we will definitely be in the
conference area at the Sheraton in San Diego. If you're attending
CloudOpen and you want to learn more about Ceph, you're more than
welcome to join us!&lt;/p&gt;
&lt;p&gt;My personal CloudOpen schedule is available
&lt;a href="http://cloudopen2012.sched.org/fghaas"&gt;here&lt;/a&gt;, by the way. Feel free to
grab me at a talk, or in the hallway. See you in San Diego!&lt;/p&gt;</content><category term="Ceph"></category><category term="Conference"></category><category term="high availability"></category><category term="OpenStack"></category></entry><entry><title>Highly Available Cloud: Pacemaker integration with OpenStack</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/highly-available-cloud-pacemaker-integration-openstack/index.html" rel="alternate"></link><published>2012-07-17T21:12:00+00:00</published><updated>2012-07-17T21:12:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-07-17:fghaas.github.io/xahteiwi.eu/resources/presentations/highly-available-cloud-pacemaker-integration-openstack/index.html</id><summary type="html">&lt;p&gt;Delivered in a refreshing, unusual presentation style, Florian explains
the high availability features in the upcoming OpenStack Folsom release
at the OSCON 2012. This presentation was delivered July 17, 2012 in
Portland, Oregon.&lt;/p&gt;
&lt;p&gt;Florian summarizes high availability in OpenStack Folsom, particularly
OpenStack integration with the Pacemaker high availability cluster
stack …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Delivered in a refreshing, unusual presentation style, Florian explains
the high availability features in the upcoming OpenStack Folsom release
at the OSCON 2012. This presentation was delivered July 17, 2012 in
Portland, Oregon.&lt;/p&gt;
&lt;p&gt;Florian summarizes high availability in OpenStack Folsom, particularly
OpenStack integration with the Pacemaker high availability cluster
stack.&lt;/p&gt;
&lt;p&gt;Florian talks about high availability shortcomings in OpenStack Essex,
comparing OpenStack to some of its important competitors. He then
explains how these shortcomings are being addressed in Folsom, and gives
an overview of the current progress in view of current OpenStack Folsom
development.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you attended this presentation in person, &lt;a href="http://www.oscon.com/oscon2012/public/schedule/detail/23566"&gt;please rate
it&lt;/a&gt; on the
OSCON web site!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Florian's full presentation is below.&lt;/p&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" id="iframe_container" src="//prezi.com/embed/p4jstawrfqwh/?bgcolor=ffffff&amp;amp;lock_to_path=0&amp;amp;autoplay=0&amp;amp;autohide_ctrls=0&amp;amp;landing_data=bHVZZmNaNDBIWnNjdEVENDRhZDFNZGNIUE43MHdLNWpsdFJLb2ZHanI0VTVyS1JLQkNJUEdyNmxnWml1SVp0b2tnPT0&amp;amp;landing_sign=_6w4R86xWOeSoIzfhN5F33xtS_DEFMCOceoUbfxdimk"&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><category term="Conference"></category><category term="OpenStack"></category><category term="Pacemaker"></category></entry><entry><title>Configuring radosgw to behave like Amazon S3</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/configuring-radosgw-behave-amazon-s3/index.html" rel="alternate"></link><published>2012-07-09T08:15:57+01:00</published><updated>2012-07-09T08:15:57+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-07-09:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/configuring-radosgw-behave-amazon-s3/index.html</id><summary type="html">&lt;p&gt;If you've heard of Ceph, you've surely heard of radosgw, a RESTful
gateway interface to the RADOS object store. You've probably also
heard that it provides a front-end interface that is compatible with
Amazon's S3 API.&lt;/p&gt;
&lt;p&gt;The question remains, if you have an S3 client that always assumes it
can …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you've heard of Ceph, you've surely heard of radosgw, a RESTful
gateway interface to the RADOS object store. You've probably also
heard that it provides a front-end interface that is compatible with
Amazon's S3 API.&lt;/p&gt;
&lt;p&gt;The question remains, if you have an S3 client that always assumes it
can find objects at http://bucket.s3.amazonaws.com, how can you use
such a client to interact, unmodified, with your radosgw host (or
hosts)?&lt;/p&gt;
&lt;p&gt;Pulling this off is actually remarkably simple, if you can control
what nameserver your clients use to resolve DNS names. Which should be
a given in the private cloud space.&lt;/p&gt;
&lt;p&gt;First, of course, you'll need an installed and configured Ceph cluster
with one or several radosgw nodes. The Ceph documentation is an
excellent reference for setting up radosgw.&lt;/p&gt;
&lt;h2&gt;Configuring radosgw to support virtual hosts&lt;/h2&gt;
&lt;p&gt;Then, you make sure you have the following entry in your Ceph configuration (normally in /etc/ceph/ceph.conf):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;[client.radosgw.charlie]&lt;/span&gt;
  &lt;span class="na"&gt;rgw dns name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;s3.amazonaws.com&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Substitute charlie with whatever name you want to use for your radosgw
client when you interact with Ceph. What the rgw dns name option
specifies is that radosgw will answer queries also for URLs like
http://bucket.hostname/object, as opposed to just
http://hostname/bucket/object.&lt;/p&gt;
&lt;h2&gt;Configuring Apache to respond to S3 host names&lt;/h2&gt;
&lt;p&gt;Also, add a wildcard record to the ServerAlias directive in the web server configuration for your radosgw host. For example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;VirtualHost&lt;/span&gt; &lt;span class="s"&gt;*:80&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="nb"&gt;ServerName&lt;/span&gt; radosgw.example.com
    &lt;span class="nb"&gt;ServerAlias&lt;/span&gt; s3.amazonaws.com
    &lt;span class="nb"&gt;ServerAlias&lt;/span&gt; *.amazonaws.com
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Configuring your DNS server&lt;/h2&gt;
&lt;p&gt;Then, set up your DNS server with a wildcard record in the
s3.amazonaws.com zone, and have nameserver respond to requests in that
zone. The zone file (for BIND9, in this case) could look like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$TTL    604800
@   IN  SOA alice.example.com. root.alice.example.com. (
                  2     ; Serial
             604800     ; Refresh
              86400     ; Retry
            2419200     ; Expire
             604800 )   ; Negative Cache TTL
;
@   IN  NS  alice.example.com.
@   IN  A   192.168.122.113
*   IN  CNAME   @
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In this zone, the A record s3.amazonaws.com resolves
to 192.168.122.113, and any sub-domain (like
mybucket.s3.amazonaws.com) also resolves to that same address via a
CNAME record.&lt;/p&gt;
&lt;h2&gt;Using your RADOS store with S3 clients&lt;/h2&gt;
&lt;p&gt;And then you just configure your client hosts to resolve DNS names via
that nameserver, and use your preferred client application to interact
with it.&lt;/p&gt;
&lt;p&gt;For example, for a user that you've created with radosgw-admin, which
uses the access key 12345 with a secret of 67890, and Mark Atwood's
popular &lt;code&gt;Net::Amazon::S3::Tools&lt;/code&gt; toolkit, here's how you can interact
with your RADOS objects:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# export AWS_ACCESS_KEY_ID=12345
# export AWS_ACCESS_KEY_SECRET=67890
# s3mkbucket mymostawesomebucket
# s3ls
mymostawesomebucket
# s3put mymostawesomebucket/foobar &amp;lt;&amp;lt;&amp;lt; "hello world"
# s3ls mymostawesomebucket
foobar
# s3get mymostawesomebucket/foobar
hello world
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Simple enough. You can add one more nifty feature.&lt;/p&gt;
&lt;h2&gt;Adding load balancing&lt;/h2&gt;
&lt;p&gt;radosgw can scale horizontally, and all you need to do to make this
work is to duplicate your radosgw and Apache configuration onto a
different host, and then add a second record to your DNS zone:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$TTL    604800
@   IN  SOA alice.example.com. root.alice.example.com. (
                  3     ; Serial
             604800     ; Refresh
              86400     ; Retry
            2419200     ; Expire
             604800 )   ; Negative Cache TTL
;
@   IN  NS  alice.example.com.
@   IN  A   192.168.122.112
@   IN  A   192.168.122.113
*   IN  CNAME   @
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then, as you access more buckets, you'll hit the A records in a
round-robin fashion, meaning your requests will be balanced across the
servers. Add as many as you like.&lt;/p&gt;
&lt;h2&gt;HTTPS support&lt;/h2&gt;
&lt;p&gt;Obviously, the above steps will not work for HTTPS connections to the
REST API. And really, making that work would amount to some pretty
terrible SSL certificate authority and client trust hackery, so just
don't do it.&lt;/p&gt;</content><category term="Ceph"></category></entry><entry><title>Fencing in VMware virtualized Pacemaker nodes</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/fencing-vmware-virtualized-pacemaker-nodes/index.html" rel="alternate"></link><published>2012-05-18T09:43:28+01:00</published><updated>2012-05-18T09:43:28+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-05-18:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/fencing-vmware-virtualized-pacemaker-nodes/index.html</id><summary type="html">&lt;p&gt;For users of VMware virtualization, it's becoming increasingly common
to deploy Pacemaker clusters within the virtual infrastructure. Doing
this requires that you set up fencing via ESX Server or, more
commonly, vCenter. Here's how to do that.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;cluster-glue&lt;/code&gt; package contains node Pacemaker's fencing (STONITH)
plugins, one of which is …&lt;/p&gt;</summary><content type="html">&lt;p&gt;For users of VMware virtualization, it's becoming increasingly common
to deploy Pacemaker clusters within the virtual infrastructure. Doing
this requires that you set up fencing via ESX Server or, more
commonly, vCenter. Here's how to do that.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;cluster-glue&lt;/code&gt; package contains node Pacemaker's fencing (STONITH)
plugins, one of which is the &lt;code&gt;external/vcenter&lt;/code&gt; plugin. It enables
Pacemaker to interface with an ESX Server host or vCenter server. When
a Pacemaker node needs to be fenced, the fencing node contacts the
vCenter host and instructs it to knock out the offending node.&lt;/p&gt;
&lt;p&gt;For this to work, your configuration needs to satisfy a couple of prerequisites:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Your setup needs a reasonably recent cluster-glue package (the one
  that ships in Debian squeeze-backports and Ubuntu precise is fine).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You need to install the &lt;a href="http://www.vmware.com/support/developer/vc-sdk/"&gt;vSphere Web Services
  SDK&lt;/a&gt; on your
  nodes. This itself has a number of Perl prerequisites. On
  Debian/Ubuntu systems, you should be able to install them with:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;aptitude install libarchive-zip-perl libcrypt-ssleay-perl \
  libclass-methodmaker-perl libuuid-perl \
  libsoap-lite-perl libxml-libxml-perl
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, create a set of vCenter credentials with the &lt;code&gt;credstore_admin.pl&lt;/code&gt;
utility that comes bundled with the SDK:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/usr/lib/vmware-vcli/apps/general/credstore_admin.pl \
  -s &amp;lt;vCenter server IP or hostname&amp;gt; \
  -u &amp;lt;vCenter username&amp;gt; \
  -p &amp;lt;vCenter password&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This creates a credentials file in
&lt;code&gt;.vmware/credstore/vicredentials.xml&lt;/code&gt; relative to your home
directory. Copy this file into a location where Pacemaker can find it,
say &lt;code&gt;/etc/vicredentials.xml&lt;/code&gt;, and make sure it gets 0600
permissions. Also, remember to copy it to all your cluster nodes. Once
your credentials are properly set up, you can test the STONITH agent's
functionality by invoking it directly, like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  &lt;span class="nv"&gt;VI_SERVER&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&amp;lt;vCenter server IP or hostname&amp;gt; &lt;span class="se"&gt;\&lt;/span&gt;
  &lt;span class="nv"&gt;VI_CREDSTORE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/etc/vicredentials.xml &lt;span class="se"&gt;\&lt;/span&gt;
  &lt;span class="nv"&gt;HOSTLIST&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"&amp;lt;pacemaker hostname&amp;gt;=&amp;lt;vCenter virtual machine name&amp;gt;"&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
  &lt;span class="nv"&gt;RESETPOWERON&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
  /usr/lib/stonith/plugins/external/vcenter gethosts
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;pacemaker hostname=""&gt; is the name of one of your cluster nodes as per
uname -n, and &lt;vcenter machine="" name="" virtual=""&gt; is the corresponding
machine name in your vCenter inventory. If everything is working fine,
the gethosts command should return the Pacemaker hostname again.&lt;/vcenter&gt;&lt;/pacemaker&gt;&lt;/p&gt;
&lt;p&gt;Now, on to adding this to the Pacemaker configuration. The example
below is for two hosts named alice and bob, which in the inventory
happen to be listed by their FQDN in the example.com domain:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;primitive p_fence_alice stonith:external/vcenter \
  params VI_SERVER="vcenter.example.com" \
    VI_CREDSTORE="/etc/vicredentials.xml" \
    HOSTLIST="alice=alice.example.com" \
    RESETPOWERON="0" \
    pcmk_host_check="static-list" \
    pcmk_host_list="alice" \
  op monitor interval="60"
primitive p_fence_bob stonith:external/vcenter \
  params VI_SERVER="vcenter.example.com" \
    VI_CREDSTORE="/etc/vicredentials.xml" \
    HOSTLIST="bob=bob.example.com" \
    RESETPOWERON="0" \
    pcmk_host_check="static-list" \
    pcmk_host_list="bob" \
  op monitor interval="60"
location l_fence_alice p_fence_alice -inf: alice
location l_fence_bob p_fence_bob -inf: bob
property stonith-enabled="true"
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;At this point you should be able to test fencing with &lt;code&gt;stonith_admin
-F&lt;/code&gt; or &lt;code&gt;crm node fence&lt;/code&gt;. Or simulate a node problem with &lt;code&gt;killall -9
corosync&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Special thanks for this goes to Nhan Ngo Dinh both for writing the
plugin in the first place, and for providing an excellent and
straightforward README file for it.&lt;/p&gt;</content><category term="Pacemaker"></category></entry><entry><title>An exciting day for the Ceph community</title><link href="fghaas.github.io/xahteiwi.eu/blog/2012/05/03/an-exciting-day-for-the-ceph-community/index.html" rel="alternate"></link><published>2012-05-03T11:10:00+00:00</published><updated>2012-05-03T11:10:00+00:00</updated><author><name>florian</name></author><id>tag:None,2012-05-03:fghaas.github.io/xahteiwi.eu/blog/2012/05/03/an-exciting-day-for-the-ceph-community/index.html</id><summary type="html">&lt;p&gt;Today, as you've probably noticed if you're following the development of
the &lt;a href="https://www.hastexo.com/knowledge/storage-io/ceph"&gt;Ceph&lt;/a&gt; stack,
something mighty cool has been happening. The
&lt;a href="http://ceph.com/"&gt;ceph.com&lt;/a&gt; web site received a major makeover with a
slick new design, and the people behind Ceph have &lt;a href="http://www.marketwired.com/press-release/new-startup-inktank-delivers-the-future-of-storage-with-ceph-1652261.htm"&gt;announced the launch
of a brand new
company&lt;/a&gt;
to drive …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today, as you've probably noticed if you're following the development of
the &lt;a href="https://www.hastexo.com/knowledge/storage-io/ceph"&gt;Ceph&lt;/a&gt; stack,
something mighty cool has been happening. The
&lt;a href="http://ceph.com/"&gt;ceph.com&lt;/a&gt; web site received a major makeover with a
slick new design, and the people behind Ceph have &lt;a href="http://www.marketwired.com/press-release/new-startup-inktank-delivers-the-future-of-storage-with-ceph-1652261.htm"&gt;announced the launch
of a brand new
company&lt;/a&gt;
to drive the Ceph stack, &lt;a href="http://www.inktank.com"&gt;Inktank&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As I've previously blogged here, Ceph is &lt;a href="https://www.hastexo.com/blogs/florian/2012/03/08/ceph-tickling-my-geek-genes"&gt;one of the most interesting
storage
technologies&lt;/a&gt;
out on the market today – and this includes both open-source and
commercial offerings. It's exceptionally well designed, extremely
scalable, and useful for a frighteningly diverse set of usage scenarios.
Up to this point, Ceph development has been driven and funded by &lt;a href="http://newdream.net/"&gt;New
Dream Network&lt;/a&gt;, a long-standing hosting provider
operating out of Southern California since 1997 under the
&lt;a href="http://www.dreamhost.com/"&gt;DreamHost&lt;/a&gt; brand. Now, it's being launched
into its own company.&lt;/p&gt;
&lt;p&gt;Inktank is about to offer professional services and training around the
Ceph stack. I've had the pleasure to meet with Inktank President &amp;amp; COO
Bryan Bogensberger and others at the OpenStack conference in San
Francisco. Indeed, meeting with them was one of my motivations for being
there – besides &lt;a href="https://www.hastexo.com/resources/presentations/reliable-redundant-resilient-high-availability-openstack"&gt;high availability in
OpenStack&lt;/a&gt;,
of course.&lt;/p&gt;
&lt;p&gt;What Inktank enables us to do is to remain involved in the Ceph
community even more than we previously were. We're already offering Ceph
instruction as part of our &lt;a href="https://www.hastexo.com/services/training/hastexo-high-availability-expert"&gt;High Availability
Expert&lt;/a&gt;
and &lt;a href="https://www.hastexo.com/services/training/cloud-bootcamp"&gt;Cloud Bootcamp for
OpenStack&lt;/a&gt;
training classes. &lt;a href="https://www.hastexo.com/who/martin"&gt;Martin&lt;/a&gt; has
&lt;a href="https://www.hastexo.com/resources/presentations/glusterfs-und-ceph-skalierbares-storage-ohne-wenn-und-aber"&gt;presented Ceph at
CeBIT&lt;/a&gt;
in Germany this year. He has also just published a well-received
&lt;a href="http://www.admin-magazine.com/HPC/Articles/The-RADOS-Object-Store-and-Ceph-Filesystem"&gt;article on Ceph in the U.S. edition of ADMIN
magazine&lt;/a&gt;,
and I have another one coming up in next month's Issue 218 of &lt;a href="http://www.linuxjournal.com"&gt;Linux
Journal&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So, we're excited for Inktank and wish them the best – even though we
sadly can't be at their &lt;a href="https://www.eventbrite.com/e/always-bet-on-ink-tickets-3311680325"&gt;launch party in Las Vegas on May
8&lt;/a&gt;.
We appreciate the invitation, guys – have fun!&lt;/p&gt;</content><category term="Ceph"></category></entry><entry><title>A look back at my first OpenStack Design Summit &amp; Conference</title><link href="fghaas.github.io/xahteiwi.eu/blog/2012/04/24/a-look-back-at-my-first-openstack-design-summit-conference/index.html" rel="alternate"></link><published>2012-04-24T09:35:00+00:00</published><updated>2012-04-24T09:35:00+00:00</updated><author><name>florian</name></author><id>tag:None,2012-04-24:fghaas.github.io/xahteiwi.eu/blog/2012/04/24/a-look-back-at-my-first-openstack-design-summit-conference/index.html</id><summary type="html">&lt;p&gt;I've just returned from the &lt;a href="http://www.openstack.org/conference/san-francisco-2012/"&gt;OpenStack Folsom Design Summit and Spring
2012
Conference&lt;/a&gt;,
and am finally getting rid of my jet lag. Here's a summary of what's
been a mind-blowing conference experience for me.&lt;/p&gt;
&lt;!--break--&gt;
&lt;!--break--&gt;
&lt;p&gt;This was my first OpenStack Design Summit and Conference. And as anyone
who's in open source …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've just returned from the &lt;a href="http://www.openstack.org/conference/san-francisco-2012/"&gt;OpenStack Folsom Design Summit and Spring
2012
Conference&lt;/a&gt;,
and am finally getting rid of my jet lag. Here's a summary of what's
been a mind-blowing conference experience for me.&lt;/p&gt;
&lt;!--break--&gt;
&lt;!--break--&gt;
&lt;p&gt;This was my first OpenStack Design Summit and Conference. And as anyone
who's in open source is acutely aware, some communities can be reluctant
to accept newcomers. Some may even seem outright hostile to the timid.
Not the &lt;a href="http://www.openstack.org/community"&gt;OpenStack&lt;/a&gt; community.&lt;/p&gt;
&lt;p&gt;The minute I sat down in the opening session of the Design Summit on
Monday, I felt instantly welcome and at home. Even as a relative
OpenStack newbie (who was invited to the Design Summit to provide some
insights and guidance on high availability), I immediately got the
impression that I was in the right place at the right time. I've rarely
seen a developer community on such a positive vibe. Sure, we'll blast
each other on technical disagreements, but all in a good-natured, fun
way.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://folsomdesignsummit2012.sched.org/event/fa2a5803a4b4ba857db57c84a1e1d3bc"&gt;My own Design Summit
session&lt;/a&gt;
clearly wasn't without such disagreements, and expectedly so. But I
think we came to some excellent conclusions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Infrastructure high availability will be an overarching design goal
    in the upcoming OpenStack Folsom release.&lt;/li&gt;
&lt;li&gt;We will shoot for providing HA solutions for all OpenStack
    infrastructure services. This includes MySQL, RabbitMQ, Glance,
    Keystone, Nova and Horizon (Swift already has HA built in). hastexo
    will play a very active role in this.&lt;/li&gt;
&lt;li&gt;We will not reinvent the wheel, and instead rely on &lt;a href="http://clusterlabs.org/"&gt;the Pacemaker
    stack&lt;/a&gt; wherever possible.&lt;/li&gt;
&lt;li&gt;Most of the challenge is really in the documentation and in the
    development of reference solutions that deployment solutions
    (&lt;a href="https://jujucharms.com/"&gt;Juju&lt;/a&gt;, &lt;a href="https://www.chef.io/chef/"&gt;Chef&lt;/a&gt;,
    &lt;a href="https://puppetlabs.com/"&gt;Puppet&lt;/a&gt;) can then build on. We will take a
    lot of responsibility in that effort, as well.&lt;/li&gt;
&lt;li&gt;Some services still require some work to become fully HA capable.
    Cinder (the volume service that's being factored out of Nova for
    Folsom) is one example, Quantum is another. This work will be
    tackled.&lt;/li&gt;
&lt;li&gt;We're currently planning to stop short of providing monitoring and
    HA for Nova instances (a.k.a. &lt;em&gt;guest HA&lt;/em&gt;). This is on the list for
    the next release past Folsom.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With those issues discussed, voted on and on the record, I had the honor
of &lt;a href="http://openstackconferencespring2012.sched.org/event/a6d940d2ebd11e37c6ac389f7d4d2125"&gt;presenting them to a larger audience at the main
conference&lt;/a&gt;.
It seems to have hit home pretty well, based on feedback from attendees
given in-person and on Twitter. &lt;span style="text-decoration: line-through;"&gt;I'm hoping the conference
organizers will make a video recording available shortly. Meanwhile, my
presentation is already available
&lt;a href="https://prezi.com/gxaohiwl46z2/high-availability-in-openstack/"&gt;here&lt;/a&gt;.&lt;/span&gt; &lt;a href="https://www.hastexo.com/resources/presentations/reliable-redundant-resilient-high-availability-openstack"&gt;It's
now available here in the &lt;em&gt;Presentations&lt;/em&gt;
section.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Overall, this has been a wonderful and very well organized conference,
and I'm very much looking forward to coming back next time around.&lt;/p&gt;
&lt;p&gt;We're also continuing to spread the OpenStack message. Just this week,
&lt;a href="https://www.hastexo.com/who/martin"&gt;Martin&lt;/a&gt; is headed for the &lt;a href="http://www.netways.de/en/osdc/y2012/"&gt;Open
Source Data Center Conference&lt;/a&gt; in
Nuremberg, Germany. His talk, &lt;em&gt;&lt;a href="http://www.netways.de/index.php?id=300&amp;amp;L=1"&gt;Die eigene Cloud mit OpenStack
Essex&lt;/a&gt;&lt;/em&gt; (in German) is about
deploying private clouds based on the OpenStack Essex release. If you're
there, grab him! He'll be happy to answer your OpenStack questions.&lt;/p&gt;</content><category term="Conference"></category><category term="OpenStack"></category></entry><entry><title>Reliable, Redundant, Resilient: High Availability in OpenStack</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/reliable-redundant-resilient-high-availability-in-openstack/index.html" rel="alternate"></link><published>2012-04-20T15:36:00+00:00</published><updated>2012-04-20T15:36:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-04-20:fghaas.github.io/xahteiwi.eu/resources/presentations/reliable-redundant-resilient-high-availability-in-openstack/index.html</id><summary type="html">&lt;p&gt;Delivered in a refreshing, unusual presentation style, Florian explains
the high availability features in the upcoming OpenStack Folsom release
at the OpenStack Conference Spring 2012. This presentation was delivered
April 21, 2012 in San Francisco, California.&lt;/p&gt;
&lt;!--break--&gt;
&lt;p&gt;Drawing on the results of an extremely productive OpenStack Design
Summit session he led …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Delivered in a refreshing, unusual presentation style, Florian explains
the high availability features in the upcoming OpenStack Folsom release
at the OpenStack Conference Spring 2012. This presentation was delivered
April 21, 2012 in San Francisco, California.&lt;/p&gt;
&lt;!--break--&gt;
&lt;p&gt;Drawing on the results of an extremely productive OpenStack Design
Summit session he led two days earlier, Florian summarizes high
availability in Folsom.&lt;/p&gt;
&lt;p&gt;Florian talks about high availability shortcomings in OpenStack Essex,
comparing OpenStack to some of its important competitors. He then
explains how these shortcomings are being addressed in Folsom, and also
touches upon hastexo's involvement in the process as part of the global
OpenStack community.&lt;/p&gt;
&lt;p&gt;Florian's full presentation is available below.&lt;/p&gt;
&lt;div class="embed-responsive embed-responsive-16by9"&gt;
&lt;iframe class="embed-responsive-item" src="//prezi.com/embed/gxaohiwl46z2/"&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><category term="OpenStack"></category></entry><entry><title>Speaking at OSCON 2012</title><link href="fghaas.github.io/xahteiwi.eu/blog/2012/04/03/speaking-at-oscon-2012/index.html" rel="alternate"></link><published>2012-04-03T09:24:00+00:00</published><updated>2012-04-03T09:24:00+00:00</updated><author><name>florian</name></author><id>tag:None,2012-04-03:fghaas.github.io/xahteiwi.eu/blog/2012/04/03/speaking-at-oscon-2012/index.html</id><summary type="html">&lt;p&gt;I'll be speaking at &lt;a href="http://www.oscon.com/oscon2012"&gt;OSCON 2012&lt;/a&gt; in
Portland, on high availability in
&lt;a href="https://www.hastexo.com/knowledge/openstack"&gt;OpenStack&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I learned from O'Reilly yesterday that my presentation proposal for this
year's &lt;a href="http://www.oscon.com/oscon2012"&gt;OSCON&lt;/a&gt;, which takes place July
16-20, 2012 in Portland, Oregon, has been accepted. As this is my first
OSCON speaking slot (actually, it's my first …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I'll be speaking at &lt;a href="http://www.oscon.com/oscon2012"&gt;OSCON 2012&lt;/a&gt; in
Portland, on high availability in
&lt;a href="https://www.hastexo.com/knowledge/openstack"&gt;OpenStack&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I learned from O'Reilly yesterday that my presentation proposal for this
year's &lt;a href="http://www.oscon.com/oscon2012"&gt;OSCON&lt;/a&gt;, which takes place July
16-20, 2012 in Portland, Oregon, has been accepted. As this is my first
OSCON speaking slot (actually, it's my first OSCON altogether), this is
a thrilling speaking opportunity for me.&lt;/p&gt;
&lt;p&gt;My talk, &lt;em&gt;Highly Available Cloud: OpenStack integration with Pacemaker&lt;/em&gt;
is currently (tentatively, I suppose) scheduled for 11:30 on July 18.&lt;/p&gt;</content><category term="Conference"></category><category term="OpenStack"></category><category term="OSCON"></category></entry><entry><title>Presentation accepted for OpenStack Spring 2012 Conference</title><link href="fghaas.github.io/xahteiwi.eu/blog/2012/03/28/presentation-accepted-for-openstack-spring-2012-conference/index.html" rel="alternate"></link><published>2012-03-28T19:52:00+00:00</published><updated>2012-03-28T19:52:00+00:00</updated><author><name>florian</name></author><id>tag:None,2012-03-28:fghaas.github.io/xahteiwi.eu/blog/2012/03/28/presentation-accepted-for-openstack-spring-2012-conference/index.html</id><summary type="html">&lt;p&gt;I just learned that my presentation is going ahead at the &lt;a href="http://www.openstack.org/conference/san-francisco-2012/"&gt;OpenStack
Spring 2012
Conference&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;My presentation, &lt;em&gt;Reliable, Redundant: High Availability in
OpenStack,&lt;/em&gt; has been accepted for the main conference track. The
official schedule isn't yet up pending confirmation from all speakers,
but I've been tentatively informed that it's on …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I just learned that my presentation is going ahead at the &lt;a href="http://www.openstack.org/conference/san-francisco-2012/"&gt;OpenStack
Spring 2012
Conference&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;My presentation, &lt;em&gt;Reliable, Redundant: High Availability in
OpenStack,&lt;/em&gt; has been accepted for the main conference track. The
official schedule isn't yet up pending confirmation from all speakers,
but I've been tentatively informed that it's on at 11:30 am on Friday,
April 20.&lt;/p&gt;
&lt;p&gt;This of course means that you should totally &lt;a href="http://www.openstack.org/conference/san-francisco-2012/register/"&gt;register for the
conference&lt;/a&gt;
if you haven't already done so, and I'll be happy to chat with anyone
interested in OpenStack HA. See you in San Francisco!&lt;/p&gt;</content><category term="Conference"></category><category term="OpenStack"></category></entry><entry><title>Mandatory and advisory ordering in Pacemaker</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/mandatory-and-advisory-ordering-pacemaker/index.html" rel="alternate"></link><published>2012-03-22T15:02:14+01:00</published><updated>2012-03-22T15:02:14+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-03-22:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/mandatory-and-advisory-ordering-pacemaker/index.html</id><summary type="html">&lt;p&gt;Ever wonder what's the difference between &lt;code&gt;order &amp;lt;name&amp;gt; inf:
&amp;lt;first-resource&amp;gt; &amp;lt;second-resource&amp;gt;&lt;/code&gt; and a score of something other
than &lt;code&gt;inf&lt;/code&gt;? We'll explain.&lt;/p&gt;
&lt;p&gt;If you specify an order constraint score of &lt;code&gt;INFINITY&lt;/code&gt; (&lt;code&gt;inf&lt;/code&gt; or the
keyword &lt;code&gt;mandatory&lt;/code&gt; in crm shell syntax), then the order constraint is
considered mandatory. If you specify &lt;code&gt;0 …&lt;/code&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Ever wonder what's the difference between &lt;code&gt;order &amp;lt;name&amp;gt; inf:
&amp;lt;first-resource&amp;gt; &amp;lt;second-resource&amp;gt;&lt;/code&gt; and a score of something other
than &lt;code&gt;inf&lt;/code&gt;? We'll explain.&lt;/p&gt;
&lt;p&gt;If you specify an order constraint score of &lt;code&gt;INFINITY&lt;/code&gt; (&lt;code&gt;inf&lt;/code&gt; or the
keyword &lt;code&gt;mandatory&lt;/code&gt; in crm shell syntax), then the order constraint is
considered mandatory. If you specify &lt;code&gt;0&lt;/code&gt;, or the keyword &lt;code&gt;advisory&lt;/code&gt;
then it's advisory. What does that mean?&lt;/p&gt;
&lt;p&gt;Firstly, anytime two resources are started in the same cluster
transition, order constraints do apply regardless of whether they're
mandatory or advisory. So for the two constraints shown here:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;order o_foo_before_bar inf: foo bar
order o_foo_before_bar 0: foo bar
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;... if &lt;code&gt;foo&lt;/code&gt; and &lt;code&gt;bar&lt;/code&gt; are just starting, &lt;code&gt;foo&lt;/code&gt; starts first, and
&lt;code&gt;bar&lt;/code&gt; starts only when &lt;code&gt;foo&lt;/code&gt;'s start operation is completed. So what's
the difference, really?&lt;/p&gt;
&lt;h2&gt;Mandatory ordering&lt;/h2&gt;
&lt;p&gt;In a &lt;strong&gt;mandatory&lt;/strong&gt; order constraint, the order is enforced under all
circumstances. Consider the following example (primitive definitions
omitted to keep this short):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;order o_foo_before_bar inf: foo bar
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Suppose &lt;code&gt;foo&lt;/code&gt; fails. Now &lt;code&gt;foo&lt;/code&gt; must be recovered, but before that,
&lt;code&gt;bar&lt;/code&gt; must also stop. So the sequence of events is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;foo&lt;/code&gt; fails&lt;/li&gt;
&lt;li&gt;Pacemaker attempts to stop &lt;code&gt;foo&lt;/code&gt; again (to make sure it's cleaned
   up).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bar&lt;/code&gt; stops.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;foo&lt;/code&gt; starts&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bar&lt;/code&gt; starts.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If &lt;code&gt;foo&lt;/code&gt; fails to start back up, then &lt;code&gt;bar&lt;/code&gt; will remain stopped. Based
on the start-failure-is-fatal and migration-threshold settings both
resources can now potentially migrate to other nodes, but if &lt;code&gt;foo&lt;/code&gt;
can't be started anywhere, &lt;code&gt;bar&lt;/code&gt; also remains stopped.&lt;/p&gt;
&lt;h2&gt;Advisory ordering&lt;/h2&gt;
&lt;p&gt;In an &lt;strong&gt;advisory&lt;/strong&gt; order constraint, the order is enforced only if
both resources start in the same transition. Otherwise, it's
ignored. Consider the following example (primitive definitions again
omitted):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;order o_foo_before_bar 0: foo bar
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Again, suppose &lt;code&gt;foo&lt;/code&gt; fails. &lt;code&gt;foo&lt;/code&gt; must be recovered, but now &lt;code&gt;bar&lt;/code&gt; can
keep running as it's not being started in the same transition. Thus:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;foo&lt;/code&gt; fails&lt;/li&gt;
&lt;li&gt;Pacemaker attempts to stop &lt;code&gt;foo&lt;/code&gt; again (to make sure it's cleaned
   up).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;foo&lt;/code&gt; starts&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If &lt;code&gt;foo&lt;/code&gt; fails to start back up, then &lt;code&gt;bar&lt;/code&gt; can continue to
run. Still, based on the &lt;code&gt;start-failure-is-fatal&lt;/code&gt; and
&lt;code&gt;migration-threshold&lt;/code&gt; settings applying to &lt;code&gt;foo&lt;/code&gt;, either it or both
resources (depending on colocation constraints) can potentially
migrate to other nodes.&lt;/p&gt;
&lt;h2&gt;So when do I use which?&lt;/h2&gt;
&lt;p&gt;Advisory ordering is good for when your dependent resource can recover
from a brief interruption in the resource it depends on. For example,
you'll want to fire up your libvirt daemon before you start your
Pacemaker-managed virtual machines, but if libvirtd were ever to crash
you can restart it without needing to restart VMs.&lt;/p&gt;
&lt;p&gt;Mandatory ordering is for stricter dependencies. Filesystems mounted
from an iSCSI device will probably want to be remounted if the iSCSI
initator has reported an error. Likewise, you'll probably also want to
restart the applications working with that filesystem.&lt;/p&gt;</content><category term="Pacemaker"></category></entry><entry><title>High Availability in OpenStack</title><link href="fghaas.github.io/xahteiwi.eu/blog/2012/03/21/high-availability-in-openstack/index.html" rel="alternate"></link><published>2012-03-21T13:08:00+00:00</published><updated>2012-03-21T13:08:00+00:00</updated><author><name>florian</name></author><id>tag:None,2012-03-21:fghaas.github.io/xahteiwi.eu/blog/2012/03/21/high-availability-in-openstack/index.html</id><summary type="html">&lt;p&gt;A few thoughts on high availability features (or the current absence
thereof) in OpenStack.&lt;/p&gt;
&lt;p&gt;I've just proposed a session for the &lt;a href="http://wiki.openstack.org/Summit/Folsom"&gt;OpenStack Folsom design
summit&lt;/a&gt; which &lt;a href="http://www.joinfu.com/"&gt;Jay
Pipes&lt;/a&gt; was nice enough to invite me to
(thanks!), and I thought I'd write up a few thoughts of mine ahead of
time …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A few thoughts on high availability features (or the current absence
thereof) in OpenStack.&lt;/p&gt;
&lt;p&gt;I've just proposed a session for the &lt;a href="http://wiki.openstack.org/Summit/Folsom"&gt;OpenStack Folsom design
summit&lt;/a&gt; which &lt;a href="http://www.joinfu.com/"&gt;Jay
Pipes&lt;/a&gt; was nice enough to invite me to
(thanks!), and I thought I'd write up a few thoughts of mine ahead of
time to get the discussion started.&lt;/p&gt;
&lt;p&gt;A little while back, Tristan van Bokkem &lt;a href="http://www.mail-archive.com/openstack@lists.launchpad.net/msg07495.html"&gt;started a discussion on high
availability for
Nova&lt;/a&gt;
on the OpenStack mailing list. So in Nova specifically, there are a few
components where high availability is readily available; you just have
to use it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MySQL. That's a no-brainer. &lt;a href="https://www.hastexo.com/resources/presentations/zen-pacemaker"&gt;MySQL HA with
    Pacemaker&lt;/a&gt;
    has been done so many times that I won't rehash it here. What's nice
    in this regard is that
    &lt;a href="http://galeracluster.com/products/mysql_galera"&gt;Galera&lt;/a&gt; (included
    in &lt;a href="http://www.percona.com/software/percona-xtradb-cluster"&gt;Percona XtraDB
    Cluster&lt;/a&gt;)
    now promises to do away with the limitations of both
    &lt;a href="https://www.hastexo.com/drbd"&gt;DRBD&lt;/a&gt; and traditional &lt;a href="http://dev.mysql.com/doc/refman/5.6/en/replication.html"&gt;MySQL
    replication&lt;/a&gt;,
    and provide multiple-node, multiple-master &lt;em&gt;synchronous&lt;/em&gt; replication
    for MySQL. As I'm sure you're aware, classic MySQL replication isn't
    synchronous, and DRBD can't do multi-node master-master, but the
    Galera based solution looks promising, &lt;a href="http://www.percona.com/blog/2012/01/09/announcement-of-percona-xtradb-cluster-alpha-release/"&gt;if not as mature as the
    other
    two&lt;/a&gt;.
    Of course, I don't understand why the Galera folks had to reinvent
    not only replication (which makes sense) but also cluster membership
    and management (which doesn't), but that's a different discussion to
    be had altogether.&lt;/li&gt;
&lt;li&gt;RabbitMQ. Has somewhat similar HA considerations as MySQL. A
    Pacemaker/DRBD-based solution &lt;a href="http://www.rabbitmq.com/pacemaker.html"&gt;exists, but is considered deprecated
    by the RabbitMQ
    maintainers&lt;/a&gt;. Enter
    &lt;a href="http://www.rabbitmq.com/ha.html"&gt;mirrored queues,&lt;/a&gt; where again the
    developers seemingly threw out the baby with the bath water and
    rather than just reimplementing replication (sensible), they came up
    with their own cluster manager (questionable). Their mirrored queues
    would probably have played very nicely with master/slave sets in
    Pacemaker.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As Tom Ellis &lt;a href="http://www.mail-archive.com/openstack@lists.launchpad.net/msg07595.html"&gt;pointed out in another
email&lt;/a&gt;
the previously mentioned thread, there are more HA considerations for
services in Nova proper.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;nova-volume still has a lot of work to do. It has an iSCSI
    &lt;a href="http://nova.openstack.org/api/nova.volume.driver.html"&gt;driver&lt;/a&gt;
    which can of course be used as an iSCSI proxy pointed at a highly
    available, potentially DRBD-backed, software iSCSI target. Or at an
    iSCSI based hardware solution that has HA built-in, such as HP
    LeftHand. Alternatively, we could just operate on RBD volumes (part
    of
    &lt;a href="https://www.hastexo.com/blogs/florian/2012/03/08/ceph-tickling-my-geek-genes"&gt;Ceph&lt;/a&gt;)
    which will also take care of redundancy for us, and add seamless
    scaleout and remirroring. That being said, there is currently no
    real HA provision for the nova-volume service itself, and that's
    something that will be required.&lt;/li&gt;
&lt;li&gt;Compute nodes can all run their own instance of nova-api.&lt;/li&gt;
&lt;li&gt;Front-end API servers can all run nova-scheduler, with a load
    balancer in front of them.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The
&lt;a href="https://www.hastexo.com/knowledge/high-availability/pacemaker"&gt;Pacemaker&lt;/a&gt;
stack has the potential of being a nice fit for most of the above. It
comes with &lt;a href="http://linux-ha.org/doc/man-pages/re-ra-iSCSITarget.html"&gt;iSCSI target
support&lt;/a&gt; (RBD
doesn't need Pacemaker on the server end, as Ceph takes care of its own
HA). Pacemaker also ties in directly with upstart, so any upstart job
can be monitored as a Pacemaker service. And Pacemaker's &lt;a href="http://clusterlabs.org/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained/s-resource-clone.html" title="Don't balk at the XML! That's Pacemaker's reference documentation; any sane person would use the crm shell to manage Pacemaker resources in real life."&gt;clone
facility&lt;/a&gt;
makes it easy to run multiple instances of inherently stateless services
with minimal configuration. What's more, Pacemaker comes with full
integration for the &lt;a href="http://horms.net/projects/ldirectord/"&gt;ldirectord&lt;/a&gt;
load-balancing service. Of course, Pacemaker adds a reliable
communications layer
(&lt;a href="https://www.hastexo.com/knowledge/high-availability/corosync"&gt;Corosync&lt;/a&gt;)
and a multi-master, self-replicating configuration facility.&lt;/p&gt;
&lt;p&gt;As for non-Nova Openstack services, Glance could use some Pacemaker
integration (not hard to do; it's just that someone has to do it).&lt;/p&gt;
&lt;p&gt;Ceph, in my opinion, has the very interesting potential of being a
redundant, scalable storage one-stop shop for OpenStack. It serves the
purposes of both volume/block storage (with RBD) and object storage
(with RADOS/radosgw). And, as already pointed out, it comes with HA,
replication, and scalability built-in.&lt;/p&gt;
&lt;p&gt;Comments and feedback on the above are much appreciated. For OpenStack
developers who visit this blog for the first time: you need to login to
post comments in our effort to combat comment spam – but you can simply
use your Launchpad OpenID to do so.&lt;/p&gt;</content></entry><entry><title>On my (ex-)maintainership of the DRBD User's Guide</title><link href="fghaas.github.io/xahteiwi.eu/blog/2012/03/20/on-my-ex-maintainership-of-the-drbd-users-guide/index.html" rel="alternate"></link><published>2012-03-20T11:01:00+00:00</published><updated>2012-03-20T11:01:00+00:00</updated><author><name>florian</name></author><id>tag:None,2012-03-20:fghaas.github.io/xahteiwi.eu/blog/2012/03/20/on-my-ex-maintainership-of-the-drbd-users-guide/index.html</id><summary type="html">&lt;p&gt;Here's a quick summary of my past and current relationship with the DRBD
User's Guide.&lt;/p&gt;
&lt;p&gt;As you probably know, I created the original &lt;a href="http://drbd.linbit.com/users-guide/"&gt;DRBD User's
Guide&lt;/a&gt; several years back, and I
maintained it throughout my time at Linbit. &lt;a href="https://fghaas.wordpress.com/2011/09/05/on-to-new-endeavors/"&gt;When I left last
year&lt;/a&gt;, it
was originally mutually understood (or so …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Here's a quick summary of my past and current relationship with the DRBD
User's Guide.&lt;/p&gt;
&lt;p&gt;As you probably know, I created the original &lt;a href="http://drbd.linbit.com/users-guide/"&gt;DRBD User's
Guide&lt;/a&gt; several years back, and I
maintained it throughout my time at Linbit. &lt;a href="https://fghaas.wordpress.com/2011/09/05/on-to-new-endeavors/"&gt;When I left last
year&lt;/a&gt;, it
was originally mutually understood (or so I thought) that I could
continue to maintain it – as a non-employee, in a community capacity,
without compensation, just as it's common in many other open source
projects. I tend to enjoy technical writing, and it was something I
certainly was looking forward to. And things got off to a promising
start, the first (trivial) patch to the documentation which I
&lt;a href="http://lists.linbit.com/pipermail/drbd-dev/2011-September/001684.html"&gt;submitted&lt;/a&gt;
in my new life &lt;a href="http://git.drbd.org/gitweb.cgi?p=drbd-documentation.git;a=commit;h=35d41237aea064686ee21621b9fb1b9111a4424c"&gt;was quickly merged without
issue&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When I made my
&lt;a href="http://lists.linbit.com/pipermail/drbd-dev/2011-November/001987.html"&gt;second&lt;/a&gt;
and
&lt;a href="http://lists.linbit.com/pipermail/drbd-dev/2011-November/001999.html"&gt;third&lt;/a&gt;
submission to the documentation, the latter of which was a a bit more
elaborate, things got a bit strange. This was after hastexo went
operational, although whether that is at all related to the sequence of
events I don't know. At any rate, I was being served with a
"Documentation Contributor License Agreement". Which wasn't considered
necessary in my earlier patch. Which involved copyright assignment.
Which I balked at. I don't necessarily object to copyright assignment if
I write on a contract, as I occasionally do for technical magazines --
but what I wrote definitely hadn't been contracted out to me. I had
simply submitted it unsolicited in the mere hope it was going to be
useful, and I wasn't interested in contract work, either. In addition,
the documentation was (and is) under a &lt;a href="http://creativecommons.org/licenses/by-sa/3.0/"&gt;liberal CC-BY-SA
license&lt;/a&gt; which made any
copyright assignment unnecessary for a simple contribution. In my humble
opinion, that is.&lt;/p&gt;
&lt;p&gt;So I raised these points, and my concerns were rejected, and my patches
didn't make it in. Note, I have no quarrel with this at all – it may
well be a perfectly sane business decision. But that's none of my
business anymore, and I respect their decision just fine. They have
stuck to their decision, and that is just fine too.&lt;/p&gt;
&lt;p&gt;It only means that I don't maintain the User's Guide anymore, and I'm
evidently also unable to contribute patches, corrections or improvements
unless I consent to copyright assignment, which I disagree with in this
instance. So unless the policy changes at some point in the future, I
won't be contributing to the User's Guide any longer.&lt;/p&gt;
&lt;p&gt;My name will remain in the authors list pretty much indefinitely (unless
someone publishes a complete rewrite) as that is required by law, but
you should interpret that as my being the original author – technically
a co-author, as I always made a point of crediting Lars' and Phil's
earlier work that the User's Guide was based on. My co-authorship
doesn't imply, however, that I'm a currently active author or
maintainer.&lt;/p&gt;</content><category term="DRBD"></category></entry><entry><title>Managing cron jobs with Pacemaker</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/managing-cron-jobs-pacemaker/index.html" rel="alternate"></link><published>2012-03-19T16:42:40+01:00</published><updated>2012-03-19T16:42:40+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-03-19:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/managing-cron-jobs-pacemaker/index.html</id><summary type="html">&lt;p&gt;It's not uncommon in Pacemaker clusters to run specific cron jobs only
on a node that currently runs a particular resource. The
&lt;code&gt;ocf:heartbeat:symlink&lt;/code&gt; resource agent can be exceptionally helpful in
this situation. Here's how to use it.&lt;/p&gt;
&lt;p&gt;Suppose you've got a cron job for Postfix whose definition normally …&lt;/p&gt;</summary><content type="html">&lt;p&gt;It's not uncommon in Pacemaker clusters to run specific cron jobs only
on a node that currently runs a particular resource. The
&lt;code&gt;ocf:heartbeat:symlink&lt;/code&gt; resource agent can be exceptionally helpful in
this situation. Here's how to use it.&lt;/p&gt;
&lt;p&gt;Suppose you've got a cron job for Postfix whose definition normally
lives in &lt;code&gt;/etc/cron.d/postfix&lt;/code&gt;. All your Postfix related data is in a
mountpoint &lt;code&gt;/srv/postfix&lt;/code&gt; (that filesystem could live on iSCSI, or DRBD,
or it could be a GlusterFS mount – that's irrelevant for the purposes
of this discussion). And as such, you've moved your cron definition to
&lt;code&gt;/srv/postfix/cron&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now you want that cron job to execute only on the node that also is
currently the active Postfix host. That's not hard at all:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;primitive p_postfix ocf:heartbeat:postfix \
  params config_dir="/etc/postfix" \
  op monitor interval="10"
primitive p_symlink ocf:heartbeat:symlink \
  params target="/srv/postfix/cron" \
    link="/etc/cron.d/postfix" \
    backup_suffix=".disabled" \
  op monitor interval="10"
primitive p_cron lsb:cron \
  op monitor interval=10
order o_symlink_before_cron inf: p_symlink p_cron
colocation c_cron_on_symlink inf: p_cron p_symlink
colocation c_symlink_on_postfix inf: p_symlink p_postfix
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;What this will do for you is this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Check whether a file named &lt;code&gt;postfix&lt;/code&gt; already exists in &lt;code&gt;/etc/cron.d&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If it does, rename it to &lt;code&gt;postfix.disabled&lt;/code&gt; (remember, cron ignores
  job definitions with dots in the filename)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(Re-)Create the postfix job definition as a symlink to
  &lt;code&gt;/srv/postfix/cron&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Restart &lt;code&gt;cron&lt;/code&gt; when it's done.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;c_symlink_on_postfix&lt;/code&gt; colocation ensures that all of this happens
on the node where the &lt;code&gt;p_postfix&lt;/code&gt; resource is also active.&lt;/p&gt;</content><category term="Pacemaker"></category></entry><entry><title>Storage Replication in High-Performance High-Availability Environments</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/storage-replication-high-performance-high-availability-environments/index.html" rel="alternate"></link><published>2012-03-19T10:54:00+00:00</published><updated>2012-03-19T10:54:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-03-19:fghaas.github.io/xahteiwi.eu/resources/presentations/storage-replication-high-performance-high-availability-environments/index.html</id><summary type="html">&lt;p&gt;At linux.conf.au 2012, Florian gave this presentation on the integration
of &lt;a href="https://www.hastexo.com/drbd"&gt;DRBD&lt;/a&gt;, Flashcache and
&lt;a href="https://www.hastexo.com/knowledge/high-availability/pacemaker"&gt;Pacemaker&lt;/a&gt;
in the High Availability and Distributed Storage miniconf.&lt;/p&gt;
&lt;p&gt;In this 30-minute presentation, Florian explores the benefits of
&lt;a href="https://github.com/facebook/flashcache"&gt;Flashcache&lt;/a&gt; for replicated
storage. Flashcache, &lt;a href="https://www.facebook.com/note.php?note_id=388112370932"&gt;originally developed at
Facebook,&lt;/a&gt; is a
general purpose, &lt;a href="https://github.com/facebook/flashcache/blob/master/doc/flashcache-doc.txt"&gt;block level cache …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;At linux.conf.au 2012, Florian gave this presentation on the integration
of &lt;a href="https://www.hastexo.com/drbd"&gt;DRBD&lt;/a&gt;, Flashcache and
&lt;a href="https://www.hastexo.com/knowledge/high-availability/pacemaker"&gt;Pacemaker&lt;/a&gt;
in the High Availability and Distributed Storage miniconf.&lt;/p&gt;
&lt;p&gt;In this 30-minute presentation, Florian explores the benefits of
&lt;a href="https://github.com/facebook/flashcache"&gt;Flashcache&lt;/a&gt; for replicated
storage. Flashcache, &lt;a href="https://www.facebook.com/note.php?note_id=388112370932"&gt;originally developed at
Facebook,&lt;/a&gt; is a
general purpose, &lt;a href="https://github.com/facebook/flashcache/blob/master/doc/flashcache-doc.txt"&gt;block level cache device implemented in the Linux
device-mapper
framework.&lt;/a&gt;
You can use flashcache in two distinct ways in Pacemaker
high-availability clusters, which Florian both explains in his talk.&lt;/p&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//www.youtube.com/embed/l910kiEuHOM"&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><category term="DRBD"></category><category term="Flashcache"></category><category term="Pacemaker"></category><category term="Performance"></category></entry><entry><title>Roll Your Own Cloud</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/roll-your-own-cloud/index.html" rel="alternate"></link><published>2012-03-19T08:27:00+00:00</published><updated>2012-03-19T08:27:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-03-19:fghaas.github.io/xahteiwi.eu/resources/presentations/roll-your-own-cloud/index.html</id><summary type="html">&lt;p&gt;In this talk (with a highly unusual presentation style!), Florian and
Tim Serong explore the capabilities of KVM, iSCSI, DRBD and Pacemaker to
create a fully open-source enterprise cloud.&lt;/p&gt;
&lt;p&gt;Shot at linux.conf.au 2011 in Brisbane, this is a video of an
entertaining talk featuring Florian speaking and Tim …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this talk (with a highly unusual presentation style!), Florian and
Tim Serong explore the capabilities of KVM, iSCSI, DRBD and Pacemaker to
create a fully open-source enterprise cloud.&lt;/p&gt;
&lt;p&gt;Shot at linux.conf.au 2011 in Brisbane, this is a video of an
entertaining talk featuring Florian speaking and Tim live-cartooning to
the delight of the audience.&lt;/p&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//www.youtube.com/embed/NyHJ8Uf03qg"&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><category term="DRBD"></category><category term="KVM"></category><category term="libvirt"></category><category term="Pacemaker"></category></entry><entry><title>What's a Totem "Retransmit List" all about in Corosync?</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/whats-totem-retransmit-list-all-about-corosync/index.html" rel="alternate"></link><published>2012-03-15T09:11:34+01:00</published><updated>2012-03-15T09:11:34+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-03-15:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/whats-totem-retransmit-list-all-about-corosync/index.html</id><summary type="html">&lt;p&gt;Occasionally, you may see errors similar to this in your system logs:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;corosync [TOTEM ] Retransmit List: e4 e5 e7 e8 ea eb ed ee
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here's what causes them, and what you can do to fix the issue.&lt;/p&gt;
&lt;p&gt;Corosync, more specifically its Totem protocol implementation, defines
a maximum number of cluster …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Occasionally, you may see errors similar to this in your system logs:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;corosync [TOTEM ] Retransmit List: e4 e5 e7 e8 ea eb ed ee
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here's what causes them, and what you can do to fix the issue.&lt;/p&gt;
&lt;p&gt;Corosync, more specifically its Totem protocol implementation, defines
a maximum number of cluster messages that can be sent during one token
rotation. By default, that number is 50, but you may modify this value
by setting the &lt;code&gt;window_size&lt;/code&gt; parameter in your &lt;code&gt;corosync.conf&lt;/code&gt;
configuration file.&lt;/p&gt;
&lt;p&gt;When among several fast cluster nodes ("processors" in Totem speak)
there are one or few slow ones, the kernel receive buffers can't cope,
messages get lost, and they then need to be retransmitted. This is
what causes the Retransmit List notifications in the syslogs. This
doesn't mean you're losing any messages or data. But it does mean that
your cluster performance degrades when this happens, and thus you
should really fix that problem.&lt;/p&gt;
&lt;p&gt;There are a few considerations that apply to tuning Corosync's
&lt;code&gt;window_size&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If you have a small cluster (say, 8 nodes or less), and they all can
  be expected to perform equally well because they have identical or
  nearly-identical hardware, then setting a large &lt;code&gt;window_size&lt;/code&gt; of up
  to 300 should be fine.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If your cluster is rather heterogeneous, then you should probably
  stick with the default of 50. Definitely don't go higher than
  256000/MTU, where MTU is that of the network interface(s) Corosync
  communicates over. For a standard Ethernet interface the default MTU
  is 1500, which would make for a maximum &lt;code&gt;window_size&lt;/code&gt; of 170.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you're running on the generally safe default of 50, and you're
  still getting Retransmit List notifications, then one of your nodes
  is most likely significantly slower than the others, and you had
  better find the cause of that and fix it. The node could be under
  constant excessive load, or have a problem with its network driver,
  or may be plugged into an incorrectly-configured switch port.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="Corosync"></category></entry><entry><title>The Zen of Pacemaker</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/zen-pacemaker/index.html" rel="alternate"></link><published>2012-03-13T13:37:00+00:00</published><updated>2012-03-13T13:37:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-03-13:fghaas.github.io/xahteiwi.eu/resources/presentations/zen-pacemaker/index.html</id><summary type="html">&lt;p&gt;&lt;a href="https://www.hastexo.com/who/florian"&gt;Florian&lt;/a&gt; teams up with Tim Serong
and Andrew Beekhof for a highly acclaimed tutorial at linux.conf.au
2012.&lt;/p&gt;
&lt;!--break--&gt;
&lt;p&gt;At linux.conf.au 2012, Florian co-presented this 90-minute tutorial with
Tim Serong of SUSE.
&lt;a href="https://www.hastexo.com/knowledge/high-availability/pacemaker"&gt;Pacemaker&lt;/a&gt;
author Andrew Beekhof dropped in as well, to field questions and provide
additional insight into …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://www.hastexo.com/who/florian"&gt;Florian&lt;/a&gt; teams up with Tim Serong
and Andrew Beekhof for a highly acclaimed tutorial at linux.conf.au
2012.&lt;/p&gt;
&lt;!--break--&gt;
&lt;p&gt;At linux.conf.au 2012, Florian co-presented this 90-minute tutorial with
Tim Serong of SUSE.
&lt;a href="https://www.hastexo.com/knowledge/high-availability/pacemaker"&gt;Pacemaker&lt;/a&gt;
author Andrew Beekhof dropped in as well, to field questions and provide
additional insight into Pacemaker development.&lt;/p&gt;
&lt;p&gt;The tutorial covers the configuration of a MySQL 2-node high
availability cluster front to back, diving into the configuration of
&lt;a href="https://www.hastexo.com/drbd"&gt;replicated storage&lt;/a&gt;, the &lt;a href="https://www.hastexo.com/knowledge/high-availability/corosync"&gt;cluster
communications
infrastructure&lt;/a&gt;,
&lt;a href="https://www.hastexo.com/knowledge/high-availability/pacemaker"&gt;cluster resource
management&lt;/a&gt;,
and of course the MySQL database itself.&lt;/p&gt;
&lt;p&gt;Reviews for this tutorial were rather enthusiastic, with bloggers
comparing the experience to &lt;a href="http://www.anchor.com.au/blog/2012/01/lca-day-3-high-availability/"&gt;an enlightenment session from the Jedi
grand
masters&lt;/a&gt;
of high availability. Take a look!&lt;/p&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//www.youtube.com/embed/3GoT36cK6os"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//www.slideshare.net/slideshow/embed_code/key/jIo0ZxOl7znyYd"&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><category term="Conference"></category></entry><entry><title>Finding out which OSDs currently store a specific RADOS object</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/which-osd-stores-specific-rados-object/index.html" rel="alternate"></link><published>2012-03-09T22:55:06+01:00</published><updated>2012-03-09T22:55:06+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-03-09:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/which-osd-stores-specific-rados-object/index.html</id><summary type="html">&lt;p&gt;Ever wanted to know just which of your OSDs a RADOS object is
currently stored in? Here's how.&lt;/p&gt;
&lt;p&gt;Suppose you've got an RBD device, named &lt;code&gt;test&lt;/code&gt;. Then you can use the
&lt;code&gt;rbd info&lt;/code&gt; command to display which name prefix is used by the RADOS
objects that make up the RBD …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Ever wanted to know just which of your OSDs a RADOS object is
currently stored in? Here's how.&lt;/p&gt;
&lt;p&gt;Suppose you've got an RBD device, named &lt;code&gt;test&lt;/code&gt;. Then you can use the
&lt;code&gt;rbd info&lt;/code&gt; command to display which name prefix is used by the RADOS
objects that make up the RBD:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ceph04:~ # rbd info test
rbd image 'test':
    size 1024 MB in 256 objects
    order 22 (4096 KB objects)
    block_name_prefix: rb.0.0
    parent:  (pool -1)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In this example, the prefix we're looking for is &lt;code&gt;rb.0.0&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;What's the RBD currently made of?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ceph04:~ # rados -p rbd ls | grep "^rb.0.0."
rb.0.0.000000000000
rb.0.0.000000000020
rb.0.0.000000000021
rb.0.0.000000000040
rb.0.0.000000000042
rb.0.0.000000000060
rb.0.0.000000000063
rb.0.0.000000000080
rb.0.0.000000000081
rb.0.0.000000000082
rb.0.0.000000000083
rb.0.0.000000000084
rb.0.0.000000000085
rb.0.0.000000000086
rb.0.0.000000000087
rb.0.0.000000000088
rb.0.0.0000000000a0
rb.0.0.0000000000a5
rb.0.0.0000000000c0
rb.0.0.0000000000c6
rb.0.0.0000000000e0
rb.0.0.0000000000e7
rb.0.0.0000000000ff
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now suppose you're interested in where &lt;code&gt;rb.0.0.0000000000a5&lt;/code&gt; is.&lt;/p&gt;
&lt;p&gt;You first grab an OSD map:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ceph04&lt;span class="o"&gt;:~&lt;/span&gt; &lt;span class="c1"&gt;# ceph osd getmap -o /tmp/osdmap&lt;/span&gt;
&lt;span class="m"&gt;2012-03-09&lt;/span&gt; &lt;span class="m"&gt;21&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;31&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;47.055376&lt;/span&gt; mon &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;osd&lt;span class="p"&gt;,&lt;/span&gt;getmap&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="m"&gt;2012-03-09&lt;/span&gt; &lt;span class="m"&gt;21&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;31&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;47.056624&lt;/span&gt; mon.1 &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="s"&gt;'got osdmap epoch 187'&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
wrote &lt;span class="m"&gt;2273&lt;/span&gt; byte payload to &lt;span class="o"&gt;/&lt;/span&gt;tmp&lt;span class="o"&gt;/&lt;/span&gt;osdmap
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And now you can use &lt;code&gt;osdmaptool&lt;/code&gt; to test an object name against the
mapfile:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ceph04:~ # osdmaptool --test-map-object rb.0.0.0000000000a5 /tmp/osdmap 
osdmaptool: osdmap file '/tmp/osdmap'
 object 'rb.0.0.0000000000a5' -&amp;gt; 0.7ea1 -&amp;gt; [2,0]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;... meaning the object lives in Placement Group &lt;code&gt;0.7ea1&lt;/code&gt;, of which
replicas currently exist in OSDs 2 and 0.&lt;/p&gt;
&lt;p&gt;Why do you want to know this? Normally, really, you don't. All the
replication and distribution happens under the covers without your
intervention. But you can use this rather neatly if you want to watch
your data being redistributed as you take out OSDs temporarily, and
put them back in.&lt;/p&gt;</content><category term="Ceph"></category></entry><entry><title>Ceph: tickling my geek genes</title><link href="fghaas.github.io/xahteiwi.eu/blog/2012/03/08/ceph-tickling-my-geek-genes/index.html" rel="alternate"></link><published>2012-03-08T20:12:00+00:00</published><updated>2012-03-08T20:12:00+00:00</updated><author><name>florian</name></author><id>tag:None,2012-03-08:fghaas.github.io/xahteiwi.eu/blog/2012/03/08/ceph-tickling-my-geek-genes/index.html</id><summary type="html">&lt;p&gt;Haven't heard of &lt;a href="http://ceph.com/"&gt;Ceph&lt;/a&gt;, the open-source distributed
petascale storage stack? Well, you've really been missing out. It's not
just a filesystem. It's a filesystem, and a striped/replicated block
device provider, and a virtualization storage backend, and a cloud
object store, and then some.&lt;/p&gt;
&lt;p&gt;Most of you will, by now …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Haven't heard of &lt;a href="http://ceph.com/"&gt;Ceph&lt;/a&gt;, the open-source distributed
petascale storage stack? Well, you've really been missing out. It's not
just a filesystem. It's a filesystem, and a striped/replicated block
device provider, and a virtualization storage backend, and a cloud
object store, and then some.&lt;/p&gt;
&lt;p&gt;Most of you will, by now, probably have heard of the Ceph filesystem, a
distributed, replicated, extremely scaleable filesystem that &lt;a href="http://kernelnewbies.org/Linux_2_6_34#head-87b23f85b5bdd35c0ab58c1ebfdcbd48d1658eef"&gt;went
upstream with the 2.6.34 kernel
release.&lt;/a&gt; But
that filesystem is really just a client to something that happens server
side, which is much more than just file storage.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://ceph.com/category/rados/"&gt;RADOS&lt;/a&gt;, the reliable autonomic
distributed object store is a massively distributed, replicating,
rack-aware object store. It organizes storage in objects, where each
object has an identifier, a payload, and a number of attributes.&lt;/p&gt;
&lt;p&gt;Objects are allocated to a Placement Group (PG), and each PG maps to one
or several Object Storage Devices or OSDs. OSDs are managed by a
userspace daemon – everything server-side in Ceph is in userspace,
really – and locally map to a simple directory. For local storage,
objects simply map to flat files, so OSDs don't need to muck around with
local block storage. And they can take advantage of lots of useful
features built into advanced filesystems, like extended attributes,
clones/reflinks, copy-on-write (with btrfs). Extra points for the effort
to &lt;em&gt;not&lt;/em&gt; reinvent wheels.&lt;/p&gt;
&lt;p&gt;The entire object store uses a deterministic placement algorithm, CRUSH
(Controlled Replication Under Scaleable Hashing). There's never a
central instance to ask on every access, instead, everything can work
out where objects are. That means the store scales out seamlessly, and
can expand and contract on the admin's whim.&lt;/p&gt;
&lt;p&gt;And based on that basic architecture, there's a number of entry points
and deployment scenarios for the stack:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;radosgw&lt;/strong&gt; provides a RESTful API for dynamic cloud storage. And it
    includes an S3 and Swift frontend to act as object storage for
    AWS/Eucalyptus and OpenStack clouds, respectively.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Qemu-RBD&lt;/strong&gt; is a storage driver for the Qemu/KVM hypervisor (fully
    integrated with libvirt) that allows the hypervisor to access
    replicated block devices that are also striped across the object
    store – with a configurable number of replicas, of course.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RBD&lt;/strong&gt; is a Linux block device that, again, is striped and
    replicated over the object store.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;librados&lt;/strong&gt; (C) and &lt;strong&gt;libradospp&lt;/strong&gt; (C++) are APIs to access the
    object store programmatically, and come with a number of scripting
    language bindings. As you've probably guessed, Qemu-RBD builds on
    librados.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ceph&lt;/strong&gt; (the filesystem) exposes POSIX filesystem semantics built
    on top of RADOS, where all POSIX-related metadata is again stored in
    the object store. This is a remarkably thin client layer at just
    17,000 LOC (compare to GFS2 at 26,000 and OCFS2 at 68,000).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In short: it's cool stuff. And it's 100% open source, it's all under the
LGPL 2.1, and the developers have made a point of not creating any
closed-source "enterprise" features – in short, they're not shipping
crippleware.&lt;/p&gt;
&lt;p&gt;We've recently started contributing to the Ceph project to improve its
high-availability cluster integration: we've submitted
&lt;a href="https://www.hastexo.com/knowledge/high-availability/pacemaker"&gt;Pacemaker&lt;/a&gt;
agents &lt;a href="https://github.com/ceph/ceph/commit/92cfad42030889d52911814faa717bebbd4dd22f"&gt;to monitor the Ceph daemons
proper&lt;/a&gt;
(a pretty trivial wrapper for a script that ships with Ceph, for now).
And we've also contributed &lt;a href="https://github.com/ceph/ceph/commit/c31b86963ab3c51b5c6d17f6e3222fe164ef3ee9"&gt;a resource agent to manage an RBD device as
a Pacemaker
resource&lt;/a&gt;.
The latter gives Pacemaker users the ability to use RBD devices as a
drop-in replacement for iSCSI devices, MD devices under Pacemaker
control, or &lt;a href="https://www.hastexo.com/drbd"&gt;DRBD&lt;/a&gt;. The Ceph community has
been exceptionally welcoming and has made contributing a pleasure –
there's no copyright assignment nonsense, no CLAs, just a very positive
attitude toward outside contributions.&lt;/p&gt;
&lt;p&gt;And in case you want to use a Ceph filesystem as a generally available
file system in your Pacemaker cluster (as you would with NFS, GlusterFS,
GFS2, or OCFS2), you can &lt;a href="https://github.com/ClusterLabs/resource-agents/commit/f93668b4b60682363a686a293810e34ad4088a47"&gt;do that now,
too&lt;/a&gt;.
However, please be cautioned that that should be considered an
experimental feature: the Ceph devs have made it very clear on numerous
occasions that they're currently focusing on making RADOS and RBD rock
solid, and then they'll tackle the POSIX filesystem layer to get it out
of experimental mode.&lt;/p&gt;
&lt;p&gt;We'll publish more on Ceph on our web site over the coming weeks, but
for those of you at CeBIT in Hannover, Germany this week: my colleague
&lt;a href="https://www.hastexo.com/who/martin"&gt;Martin Loschwitz&lt;/a&gt; is &lt;a href="https://www.hastexo.com/blogs/martin/2012/02/27/hastexo-coming-cebit-2012"&gt;presenting a
talk on Ceph (and
GlusterFS)&lt;/a&gt;
at the CeBIT Open Source Forum tomorrow.&lt;/p&gt;</content><category term="Ceph"></category></entry><entry><title>Solve a DRBD split-brain in 4 steps</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/solve-drbd-split-brain-4-steps/index.html" rel="alternate"></link><published>2012-03-06T01:29:24+01:00</published><updated>2012-03-06T01:29:24+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-03-06:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/solve-drbd-split-brain-4-steps/index.html</id><summary type="html">&lt;p&gt;Whenever a DRBD setup runs into a situation where the replication
network is disconnected and fencing policy is set to &lt;code&gt;dont-care&lt;/code&gt;
(default), there is the potential risk of a split-brain. Even with
resource level fencing or STONITH setup, there are corner cases that
will end up in a split-brain.&lt;/p&gt;
&lt;p&gt;When …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Whenever a DRBD setup runs into a situation where the replication
network is disconnected and fencing policy is set to &lt;code&gt;dont-care&lt;/code&gt;
(default), there is the potential risk of a split-brain. Even with
resource level fencing or STONITH setup, there are corner cases that
will end up in a split-brain.&lt;/p&gt;
&lt;p&gt;When your DRBD resource is in a split-brain situation, don't panic!
Split-brain means that the contents of the backing devices of your
DRBD resource on both sides of your cluster started to diverge. At
some point in time, the DRBD resource on both nodes went into the
Primary role while the cluster nodes themselves were disconnected from
each other.&lt;/p&gt;
&lt;p&gt;Different writes happened to both sides of your cluster
afterwards. After reconnecting, DRBD doesn't know which set of data is
"right" and which is "wrong".&lt;/p&gt;
&lt;h2&gt;Indications of a Split-Brain&lt;/h2&gt;
&lt;p&gt;The symptoms of a split-brain are that the peers will not reconnect on
DRBD startup but stay in connection state StandAlone or
WFConnection. The latter will be shown if the remote peer detected the
split-brain earlier and was faster at shutdown its connection. In your
kernel logs you will see messages like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;block&lt;/span&gt; &lt;span class="n"&gt;drbd0&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Split&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Brain&lt;/span&gt; &lt;span class="n"&gt;detected&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dropping&lt;/span&gt; &lt;span class="n"&gt;connection&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;4 Steps to solve the Split-Brain&lt;/h2&gt;
&lt;h3&gt;Step 1&lt;/h3&gt;
&lt;p&gt;Manually choose a node which data modifications will be discarded.&lt;/p&gt;
&lt;p&gt;We call it the split brain victim. Choose wisely, all modifications
will be lost! When in doubt run a backup of the victim's data before
you continue.&lt;/p&gt;
&lt;p&gt;When running a Pacemaker cluster, you can enable maintenance mode. If
the split brain victim is in Primary role, bring down all applications
using this resource. Now switch the victim to Secondary role:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;victim# drbdadm secondary resource
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Step 2&lt;/h3&gt;
&lt;p&gt;Disconnect the resource if it's in connection state &lt;code&gt;WFConnection&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;victim# drbdadm disconnect resource
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Step 3&lt;/h3&gt;
&lt;p&gt;Force discard of all modifications on the split brain victim:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;victim# drbdadm -- --discard-my-data connect resource
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;for DRBD 8.4.x:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;victim# drbdadm connect --discard-my-data resource
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Step 4&lt;/h3&gt;
&lt;p&gt;Resync will start automatically if the survivor was in
&lt;code&gt;WFConnection&lt;/code&gt; network state. If the split brain survivor is still in
&lt;code&gt;Standalone&lt;/code&gt; connection state, reconnect it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;survivor# drbdadm connect resource
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;At the latest now the resynchronization from the survivor
(&lt;code&gt;SyncSource&lt;/code&gt;) to the victim (&lt;code&gt;SyncTarget&lt;/code&gt;) starts immediately. There
is no full sync initiated but all modifications on the victim will be
overwritten by the survivor's data and modifications on the survivor
will be applied to the victim.&lt;/p&gt;
&lt;h2&gt;Background: What happens?&lt;/h2&gt;
&lt;p&gt;With the default after-split-brain policies of disconnect this will
happen always in dual primary setups. It can happen in single primary
setups if one peer changes at least once its role from Secondary to
Primary while disconnected from the previous (before network
interruption) Primary.&lt;/p&gt;
&lt;p&gt;There are a variety of automatic policies to solve a split brain but
some of them will overwrite (potentially valid) data without further
inquiry. Even with theses policies in place a unresolvable split-brain
can occur.&lt;/p&gt;
&lt;p&gt;The split-brain is detected once the peers reconnect and do their DRBD
protocol handshake which also includes exchanging of the Generation
Identifiers (GIs).&lt;/p&gt;</content><category term="DRBD"></category></entry><entry><title>Checking Corosync cluster membership</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/checking-corosync-cluster-membership/index.html" rel="alternate"></link><published>2012-03-04T23:42:35+01:00</published><updated>2012-03-04T23:42:35+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-03-04:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/checking-corosync-cluster-membership/index.html</id><summary type="html">&lt;p&gt;It's simple and easy to get Pacemaker's view of the status of members
in a cluster – just invoke &lt;code&gt;crm_mon&lt;/code&gt;. But what if you want to check on
the cluster membership when Pacemaker is not running, or you want to
make sure whether Corosync's view of the cluster is identical to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;It's simple and easy to get Pacemaker's view of the status of members
in a cluster – just invoke &lt;code&gt;crm_mon&lt;/code&gt;. But what if you want to check on
the cluster membership when Pacemaker is not running, or you want to
make sure whether Corosync's view of the cluster is identical to
Pacemaker's? Here's how.&lt;/p&gt;
&lt;h2&gt;Checking ring status with &lt;code&gt;corosync-cfgtool&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;corosync-cfgtool&lt;/code&gt; utility displays the cluster connectivity status
when invoked with the &lt;code&gt;-s&lt;/code&gt; flag:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# corosync-cfgtool -s
Printing ring status.
Local node ID 303938909
RING ID 0
    id  = 10.0.1.1
    status  = ring 0 active with no faults
RING ID 1
    id  = 192.168.42.1
    status  = ring 1 active with no faults
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The above is the status of two healthy rings; a failed ring (one
affected by a network interruption, for example) would show a &lt;code&gt;FAULTY&lt;/code&gt;
status.&lt;/p&gt;
&lt;p&gt;There's a catch. In a two-node cluster, if both nodes were to start
while all cluster communication links are down, then Corosync would
form &lt;em&gt;two&lt;/em&gt; memberships with healthy, one-member rings. Both of the
nodes would show a ring status similar to the above, but your cluster
still wouldn't be communicating. So, you can't rely on
&lt;code&gt;corosync-cfgtool -s&lt;/code&gt; alone. You must also check Corosync's member
list.&lt;/p&gt;
&lt;h2&gt;Querying the member list with &lt;code&gt;corosync-cmapctl&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;We can examine Corosync's cluster member list with the &lt;code&gt;corosync-cmapctl&lt;/code&gt; command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# corosync-cmapctl | grep member
runtime.totem.pg.mrp.srp.members.303938909.ip=r(0) ip(10.0.1.1) r(1) ip(192.168.42.1) 
runtime.totem.pg.mrp.srp.members.303938909.join_count=1
runtime.totem.pg.mrp.srp.members.303938909.status=joined
runtime.totem.pg.mrp.srp.members.320716125.ip=r(0) ip(10.0.1.2) r(1) ip(192.168.42.2) 
runtime.totem.pg.mrp.srp.members.320716125.join_count=1
runtime.totem.pg.mrp.srp.members.320716125.status=joined
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In this example, we have two nodes (with node IDs &lt;code&gt;303938909&lt;/code&gt; and
&lt;code&gt;320716125&lt;/code&gt;). They are both configured to use two communication rings,
&lt;code&gt;r(0)&lt;/code&gt; and &lt;code&gt;r(1)&lt;/code&gt;, and both of them have successfully joined the
cluster.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; In earlier Corosync releases (pre-2.0), the
&lt;code&gt;corosync-cmapctl&lt;/code&gt; tool was called &lt;code&gt;corosync-objctl&lt;/code&gt;. Its command
syntax for querying the member list was identical.&lt;/p&gt;</content><category term="Corosync"></category></entry><entry><title>Fencing in Libvirt/KVM virtualized cluster nodes</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/fencing-libvirtkvm-virtualized-cluster-nodes/index.html" rel="alternate"></link><published>2012-02-29T13:56:42+01:00</published><updated>2012-02-29T13:56:42+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-02-29:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/fencing-libvirtkvm-virtualized-cluster-nodes/index.html</id><summary type="html">&lt;p&gt;Often, people deploy the Pacemaker stack in virtual environments for
purposes of testing and evaluation. In such environments, it's easy to
test Pacemaker's fencing capabilities by tying in with the hypervisor.&lt;/p&gt;
&lt;p&gt;This quick howto illustrates how to configure fencing for two virtual
cluster nodes hosted on a libvirt/KVM hypervisor …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Often, people deploy the Pacemaker stack in virtual environments for
purposes of testing and evaluation. In such environments, it's easy to
test Pacemaker's fencing capabilities by tying in with the hypervisor.&lt;/p&gt;
&lt;p&gt;This quick howto illustrates how to configure fencing for two virtual
cluster nodes hosted on a libvirt/KVM hypervisor host.&lt;/p&gt;
&lt;h2&gt;libvirt configuration (hypervisor)&lt;/h2&gt;
&lt;p&gt;In order to do libvirt fencing, your hypervisor should have its
libvirtd daemon listen on a network socket. libvirtd is capable of
doing this, both on an encrypted TLS socket, and on a regular,
unencrypted TCP port. Needless to say, for production use you should
only use TLS, but for testing and evaluation – and for that purpose
only – TCP is fine.&lt;/p&gt;
&lt;p&gt;In order for your hypervisor to listen on an unauthenticated,
insecure, unencrypted network socket (did we mention that's unsuitable
for production?), add the following lines to your libvirtd
configuration file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="na"&gt;listen_tls&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;0&lt;/span&gt;
&lt;span class="na"&gt;listen_tcp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;1&lt;/span&gt;
&lt;span class="na"&gt;tcp_port&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"16509"&lt;/span&gt;
&lt;span class="na"&gt;auth_tcp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"none"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can also set the &lt;code&gt;listen_addr&lt;/code&gt; parameter, for example to have
libvirtd listen only on the network that your virtual machines run
in. If you don't set listen_addr, libvirtd will simply listen on the
wildcard address.&lt;/p&gt;
&lt;p&gt;You'll also have to add the &lt;code&gt;-l&lt;/code&gt; or &lt;code&gt;--listen&lt;/code&gt; flag to your libvirtd
invocation. On Debian/Ubuntu platforms, you can do so by editing the
&lt;code&gt;/etc/default/libvirt-bin&lt;/code&gt; configuration file.&lt;/p&gt;
&lt;p&gt;Once you've done that, you can use &lt;code&gt;netstat -ltp&lt;/code&gt; to check whether
libvirtd is in fact listening on its configured port, 16509/tcp. Also,
make sure that you don't have a firewall blocking that port.&lt;/p&gt;
&lt;h2&gt;libvirt configuration (virtual machines)&lt;/h2&gt;
&lt;p&gt;Inside your virtual machines, you'll also have to install the libvirt
client binaries – the fencing mechanism uses the virsh utility under
the covers. Some platforms provide a &lt;code&gt;libvirt-client&lt;/code&gt; package for that
purpose; for other's, you'll simply have to install the full &lt;code&gt;libvirt&lt;/code&gt;
package.&lt;/p&gt;
&lt;p&gt;Once that is set up, you should be able to run this command from
inside your virtual machines:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;virsh --connect&lt;span class="o"&gt;=&lt;/span&gt;qemu+tcp://&amp;lt;IP of your hypervisor&amp;gt;/system &lt;span class="se"&gt;\&lt;/span&gt;
  list --all
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;... and that command should list all the domains running on that host,
including the one you're connecting from.&lt;/p&gt;
&lt;h2&gt;Pacemaker configuration&lt;/h2&gt;
&lt;p&gt;In one of your virtual machines, you can now set up your fencing
configuration.&lt;/p&gt;
&lt;p&gt;This example assumes that you have two nodes named alice and bob, that
their corresponding virtual machine domain names are also alice and
bob, and that they can reach their hypervisor by TCP at 192.168.0.1:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;primitive p_fence_alice stonith:external/libvirt \
  params hostlist="alice" \
   hypervisor_uri="qemu+tcp://192.168.0.1/system" \
  op monitor interval="60"
primitive p_fence_bob stonith:external/libvirt \
  params hostlist="bob" \
    hypervisor_uri="qemu+tcp://192.168.0.1/system" \
  op monitor interval="60"
location l_fence_alice p_fence_alice -inf: alice
location l_fence_bob p_fence_bob -inf: bob
property stonith-enabled=true
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now you can test fencing to the best of your abilities.&lt;/p&gt;</content><category term="Pacemaker"></category></entry><entry><title>Network connectivity check in Pacemaker</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/network-connectivity-check-pacemaker/index.html" rel="alternate"></link><published>2012-02-27T17:45:19+01:00</published><updated>2012-02-27T17:45:19+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-02-27:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/network-connectivity-check-pacemaker/index.html</id><summary type="html">&lt;p&gt;If you want a Pacemaker cluster to move resources on changes on the
network connectivity of an individual node, there are two major steps
involved:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Let Pacemaker monitor connectivity;&lt;/li&gt;
&lt;li&gt;Configure constraints to react on connectivity changes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;Be sure to run at least Pacemaker 1.0.11 or 1.1 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you want a Pacemaker cluster to move resources on changes on the
network connectivity of an individual node, there are two major steps
involved:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Let Pacemaker monitor connectivity;&lt;/li&gt;
&lt;li&gt;Configure constraints to react on connectivity changes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;Be sure to run at least Pacemaker 1.0.11 or 1.1.6 to include some
important fixes affecting the &lt;code&gt;ocf:pacemaker:ping&lt;/code&gt; resource agent.&lt;/p&gt;
&lt;p&gt;Preferably, choose more than one reliable ping targets in your network
(like a highly available gateway router, a core switch, or DNS
server).&lt;/p&gt;
&lt;h2&gt;Pacemaker configuration&lt;/h2&gt;
&lt;p&gt;The following crm shell code snippet configures a cloned ping resource
including constraints to run Dummy resources on any node that has
connectivity at all. Please note, that the first constraint forbids to
run &lt;code&gt;p_dummy1&lt;/code&gt; if all nodes lose connectivity. The second constraint
places &lt;code&gt;p_dummy2&lt;/code&gt; on the node that has the best connectivity:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;primitive p_ping ocf:pacemaker:ping \
   params host_list="dns.example.com router.example.com" \
   multiplier="1000" dampen="60s"\
   op monitor interval="10s"
clone cl_ping p_ping

primitive p_dummy1 ocf:pacemaker:Dummy
primitive p_dummy2 ocf:pacemaker:Dummy

location l_dummy1_needs_connectivity p_dummy1 \
  rule -inf: not_defined pingd or pingd lte 0
location l_dummy2_likes_best_connectivity p_dummy2 \
  rule pingd: defined pingd
&lt;/pre&gt;&lt;/div&gt;</content><category term="Pacemaker"></category></entry><entry><title>Speaking at the 2012 Percona Live MySQL Conference</title><link href="fghaas.github.io/xahteiwi.eu/blog/2012/02/27/speaking-2012-percona-live-mysql-conference/index.html" rel="alternate"></link><published>2012-02-27T13:39:00+00:00</published><updated>2012-02-27T13:39:00+00:00</updated><author><name>florian</name></author><id>tag:None,2012-02-27:fghaas.github.io/xahteiwi.eu/blog/2012/02/27/speaking-2012-percona-live-mysql-conference/index.html</id><summary type="html">&lt;p&gt;This year, I have the pleasure of returning to the MySQL Conference &amp;amp;
Expo as a speaker. Percona have picked up the torch that O'Reilly had
held as the conference organizers, and they're putting together a 3-day
conference this year. I am co-presenting a tutorial with Yves Trudeau
from Percona.&lt;/p&gt;
&lt;p&gt;Our …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This year, I have the pleasure of returning to the MySQL Conference &amp;amp;
Expo as a speaker. Percona have picked up the torch that O'Reilly had
held as the conference organizers, and they're putting together a 3-day
conference this year. I am co-presenting a tutorial with Yves Trudeau
from Percona.&lt;/p&gt;
&lt;p&gt;Our tutorial is called &lt;a href="http://www.percona.com/live/mysql-conference-2012/sessions/mysql-high-availability-deep-dive-pacemaker-drbd-mysql-replication-and-more"&gt;High Availability Deep Dive: Pacemaker, DRBD,
MySQL Replication, and
more!&lt;/a&gt; and
it's going to be the only full-day tutorial offered in this year's
conference. In it, Yves and I are going to cover&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;/p&gt;
    An overview of the Pacemaker cluster stack (the classic "this is
    Pacemaker" introduction)&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;
-   &lt;/p&gt;
DRBD-backed MySQL replication (another classic and widely deployed
scenario)
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;
-   &lt;/p&gt;
MySQL replication under Pacemaker management (a new option which
Yves has vastly improved through a big patch set to the MySQL RA).
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;


&lt;p&gt;Do I expect this talk to be controversial? Definitely. The amount of
"Pacemaker is terrible" and "Pacemaker is unsuitable for managing highly
available databases" that has been around the blogosphere lately is
pretty mind-boggling.&lt;/p&gt;
&lt;p&gt;But strangely enough, most of the things brought forward against
Pacemaker by its detractors seem like a time-warp back to about 2007.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;/p&gt;
    "We must use XML to manage Pacemaker!" Nonsense. In fact, that was
    &lt;em&gt;never&lt;/em&gt; true – the release of Pacemaker as a separate project and
    the release of the crm shell coincided. Ever since, Pacemaker
    configuration has been as text-based as MySQL itself.&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;
-   &lt;/p&gt;
"All Pacemaker can do is react to node failure!" Nothing could be
further from the truth. Pacemaker has some of the most sophisticated
resource monitoring and auto-recovery capabilities under the sun.
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;
-   &lt;/p&gt;
"OK. But all it can do to react to &lt;em&gt;resource&lt;/em&gt; failure is kill a
daemon!" Bogus again. It will happily do whatever the resource agent
specifies. Or the admin, through the configuration. 
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;


&lt;p&gt;In our tutorial, we're going to dispel a few of these myths. We
certainly make no claims as to Pacemaker being the one and only solution
for MySQL HA, but it's one that serves lots of use cases excellently.&lt;/p&gt;
&lt;p&gt;Needless to say, I'll also hang around for the conference proper, and
I'm very much looking forward to seeing lots of familiar faces. I'll
also remain in the Bay Area for some time after the MySQL conference –
more on that in a day or two.&lt;/p&gt;</content><category term="Conference"></category><category term="MySQL"></category></entry><entry><title>GFS2 in Pacemaker (Debian/Ubuntu)</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/gfs2-pacemaker-debianubuntu/index.html" rel="alternate"></link><published>2012-02-26T20:34:08+01:00</published><updated>2012-02-26T20:34:08+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-02-26:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/gfs2-pacemaker-debianubuntu/index.html</id><summary type="html">&lt;p&gt;Setting up GFS2 in Pacemaker requires configuring the Pacemaker DLM,
the Pacemaker GFS control daemon, and a GFS2 filesystem itself.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;GFS2 with Pacemaker integration is supported on Debian
(&lt;code&gt;squeeze-backports&lt;/code&gt; and up) and Ubuntu (10.04 LTS and up). You'll need
the &lt;code&gt;dlm-pcmk&lt;/code&gt;, &lt;code&gt;gfs2-tools&lt;/code&gt;, and &lt;code&gt;gfs-pcmk&lt;/code&gt; packages.&lt;/p&gt;
&lt;p&gt;Fencing is imperative …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Setting up GFS2 in Pacemaker requires configuring the Pacemaker DLM,
the Pacemaker GFS control daemon, and a GFS2 filesystem itself.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;GFS2 with Pacemaker integration is supported on Debian
(&lt;code&gt;squeeze-backports&lt;/code&gt; and up) and Ubuntu (10.04 LTS and up). You'll need
the &lt;code&gt;dlm-pcmk&lt;/code&gt;, &lt;code&gt;gfs2-tools&lt;/code&gt;, and &lt;code&gt;gfs-pcmk&lt;/code&gt; packages.&lt;/p&gt;
&lt;p&gt;Fencing is imperative. Get a proper fencing/STONITH configuration set
up and test it thoroughly.&lt;/p&gt;
&lt;h2&gt;Pacemaker configuration&lt;/h2&gt;
&lt;p&gt;The Pacemaker configuration, shown here in &lt;code&gt;crm&lt;/code&gt; shell syntax, normally
puts all the required resources into one cloned group. Have a look at
this configuration snippet:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;primitive p_dlm_controld ocf:pacemaker:controld \
  params daemon="dlm_controld.pcmk" \
  op start interval="0" timeout="90" \
  op stop interval="0" timeout="100" \
  op monitor interval="10"
primitive p_gfs_controld ocf:pacemaker:controld \
  params daemon="gfs_controld.pcmk"\
  op start interval="0" timeout="90" \
  op stop interval="0" timeout="100" \
  op monitor interval="10"
primitive p_fs_gfs2 ocf:heartbeat:Filesystem \
  params device="&amp;lt;your device path&amp;gt;" \
    directory="&amp;lt;your mount point&amp;gt;" \
    fstype="gfs2" \
  op monitor interval="10"
group g_gfs2 p_dlm_controld p_gfs_controld p_fs_gfs2
clone cl_gfs2 g_gfs2 \
  meta interleave="true"
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then when that's done, your filesystem should happily mount on all nodes.&lt;/p&gt;</content><category term="Pacemaker"></category></entry><entry><title>Interleaving in Pacemaker clones</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/interleaving-pacemaker-clones/index.html" rel="alternate"></link><published>2012-02-26T20:34:08+01:00</published><updated>2012-02-26T20:34:08+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-02-26:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/interleaving-pacemaker-clones/index.html</id><summary type="html">&lt;p&gt;Ever wonder what &lt;code&gt;meta interleave&lt;/code&gt; really means in a Pacemaker clone
definition? We'll explain.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;interleave&lt;/code&gt; meta attribute is only valid on Pacemaker clone
definitions – and their extended version of sorts, master/slave
sets. It's not available on primitives and groups. Clones are often
used in configurations involving cluster filesystems …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Ever wonder what &lt;code&gt;meta interleave&lt;/code&gt; really means in a Pacemaker clone
definition? We'll explain.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;interleave&lt;/code&gt; meta attribute is only valid on Pacemaker clone
definitions – and their extended version of sorts, master/slave
sets. It's not available on primitives and groups. Clones are often
used in configurations involving cluster filesystems, such as GFS2
(&lt;a href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/gfs2-pacemaker-debianubuntu/index.html"&gt;here's an example&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Consider the following example (primitive definitions omitted to keep
this short):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;clone cl_foo p_foo meta interleave=false
clone cl_bar p_bar meta interleave=false
order o_foo_before_bar inf: cl_foo cl_bar
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;What this means is for the &lt;code&gt;order&lt;/code&gt; constraint to be fulfilled, &lt;em&gt;all&lt;/em&gt;
instances of &lt;code&gt;cl_foo&lt;/code&gt; must start before &lt;em&gt;any&lt;/em&gt; instance of &lt;code&gt;cl_bar&lt;/code&gt;
can. Often, that's not what you want.&lt;/p&gt;
&lt;p&gt;In contrast, consider this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;clone cl_foo p_foo meta interleave=true
clone cl_bar p_bar meta interleave=true
order o_foo_before_bar inf: cl_foo cl_bar
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here, for each node, as soon as the &lt;em&gt;local&lt;/em&gt; instance of &lt;code&gt;cl_foo&lt;/code&gt; has
started, the corresponding local instance of &lt;code&gt;cl_bar&lt;/code&gt; can, too. &lt;strong&gt;This
is what's usually desired – when in doubt, allow interleaving.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One thing that often throws people is that interleaving only works
when Pacemaker is configured to run the same number of instances of
two clones on the same node. Thus,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;clone cl_foo p_foo\
  meta interleave=true \
    globally-unique=true clone-node-max=2
clone cl_bar p_bar meta interleave=false
order o_foo_before_bar inf: cl_foo cl_bar
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;... won't work, as Pacemaker is allowed to run 2 instances of &lt;code&gt;cl_foo&lt;/code&gt;
on the same node, but only one of &lt;code&gt;cl_bar&lt;/code&gt; (the default for
&lt;code&gt;clone-node-max&lt;/code&gt; is 1).&lt;/p&gt;
&lt;p&gt;Also, &lt;code&gt;globally-unique=true&lt;/code&gt; is a requirement for any
&lt;code&gt;clone-node-max&lt;/code&gt;&amp;gt;1 – which means that interleaving between a
globally-unique and a not globally-unique clone is also not supported.&lt;/p&gt;</content><category term="Pacemaker"></category></entry><entry><title>OCFS2 in Pacemaker (Debian/Ubuntu)</title><link href="fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/ocfs2-pacemaker-debianubuntu/index.html" rel="alternate"></link><published>2012-02-24T17:01:20+01:00</published><updated>2012-02-24T17:01:20+01:00</updated><author><name>Florian Haas</name></author><id>tag:None,2012-02-24:fghaas.github.io/xahteiwi.eu/resources/hints-and-kinks/ocfs2-pacemaker-debianubuntu/index.html</id><summary type="html">&lt;p&gt;Setting up OCFS2 in Pacemaker requires configuring the Pacemaker DLM,
the O2CB lock manager for OCFS2, and an OCFS2 filesystem itself.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;OCFS2 with Pacemaker integration is supported on Debian
  (&lt;code&gt;squeeze-backports&lt;/code&gt; and up) and Ubuntu (10.04 LTS and up). You'll
  need the &lt;code&gt;dlm-pcmk&lt;/code&gt;, &lt;code&gt;ocfs2-tools&lt;/code&gt;, &lt;code&gt;ocfs2-tools-pacemaker&lt;/code&gt; and
  &lt;code&gt;openais&lt;/code&gt; packages.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fencing …&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;Setting up OCFS2 in Pacemaker requires configuring the Pacemaker DLM,
the O2CB lock manager for OCFS2, and an OCFS2 filesystem itself.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;OCFS2 with Pacemaker integration is supported on Debian
  (&lt;code&gt;squeeze-backports&lt;/code&gt; and up) and Ubuntu (10.04 LTS and up). You'll
  need the &lt;code&gt;dlm-pcmk&lt;/code&gt;, &lt;code&gt;ocfs2-tools&lt;/code&gt;, &lt;code&gt;ocfs2-tools-pacemaker&lt;/code&gt; and
  &lt;code&gt;openais&lt;/code&gt; packages.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fencing is imperative. Get a proper fencing/STONITH configuration
  set up and test it thoroughly.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Running OCFS2/Pacemaker integration requires that you load Corosync
  with the &lt;code&gt;openais_ckpt&lt;/code&gt; service enabled. The service definition is in
  the file &lt;code&gt;/etc/corosync/service.d/ckpt-service&lt;/code&gt; which the &lt;code&gt;openais&lt;/code&gt;
  package installs by default. Make sure you did not accidentally
  delete or disable this file.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Pacemaker configuration&lt;/h2&gt;
&lt;p&gt;The Pacemaker configuration, shown here in crm shell syntax, normally
puts all the required resources into one cloned group. Have a look at
this configuration snippet:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;primitive p_dlm_controld ocf:pacemaker:controld \
  op start interval="0" timeout="90" \
  op stop interval="0" timeout="100" \
  op monitor interval="10"
primitive p_o2cb ocf:pacemaker:o2cb \
  op start interval="0" timeout="90" \
  op stop interval="0" timeout="100" \
  op monitor interval="10"
primitive p_fs_ocfs2 ocf:heartbeat:Filesystem \
  params device="&amp;lt;your device path&amp;gt;" \
    directory="&amp;lt;your mount point&amp;gt;" \
    fstype="ocfs2" \
  meta target-role=Stopped \
  op monitor interval="10"
group g_ocfs2 p_dlm_controld p_o2cb p_fs_ocfs2
clone cl_ocfs2 g_ocfs2 \
  meta interleave="true"
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Why keep the filesystem stopped?&lt;/h2&gt;
&lt;p&gt;Because you probably either don't have a configured OCFS2 filesystem
on your device yet, or your ran mkfs.ocfs2 when the Pacemaker stack
wasn't running. In either of those two cases, mount.ocfs2 will refuse
to mount the filesystem.&lt;/p&gt;
&lt;p&gt;Thus, fire up your DLM and the o2cb process like the above
configuration does, and then:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If you haven't got a filesystem yet, run &lt;code&gt;mkfs.ocfs2&lt;/code&gt; on your device, or&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you do already have one, run
  &lt;code&gt;tunefs.ocfs2 --update-cluster-stack &amp;lt;device&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then when that's done, run &lt;code&gt;crm resource start p_fs_ocfs2&lt;/code&gt; and your
filesystem should happily mount on all nodes.&lt;/p&gt;</content><category term="Pacemaker"></category></entry><entry><title>Fencing and Maintaining Sanity in High-Availability Clusters</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/fencing-and-maintaining-sanity-high-availability-clusters/index.html" rel="alternate"></link><published>2011-11-01T13:45:00+00:00</published><updated>2011-11-01T13:45:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2011-11-01:fghaas.github.io/xahteiwi.eu/resources/presentations/fencing-and-maintaining-sanity-high-availability-clusters/index.html</id><summary type="html">&lt;p&gt;A 45-minute talk co-presented by
&lt;a href="/who/florian"&gt;Florian&lt;/a&gt; and &lt;a href="https://alteeve.ca/w/Main_Page"&gt;Madison
Kelly&lt;/a&gt; at Linuxcon Europe 2011 in
Prague. Florian and Madison explain the purpose of fencing, options for
implementing fencing, and common pitfalls.&lt;/p&gt;
&lt;p&gt;This presentation rounded out a series of high-availability talks at
Linuxcon Europe 2011, the first Linuxcon event hosted by the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A 45-minute talk co-presented by
&lt;a href="/who/florian"&gt;Florian&lt;/a&gt; and &lt;a href="https://alteeve.ca/w/Main_Page"&gt;Madison
Kelly&lt;/a&gt; at Linuxcon Europe 2011 in
Prague. Florian and Madison explain the purpose of fencing, options for
implementing fencing, and common pitfalls.&lt;/p&gt;
&lt;p&gt;This presentation rounded out a series of high-availability talks at
Linuxcon Europe 2011, the first Linuxcon event hosted by the Linux
Foundation in Europe. Florian and Madison presented at the Clarion
Congress hotel in Prague, Czech Republic, in the main conference track.&lt;/p&gt;
&lt;p&gt;As Florian and Madison both have a strong dislike of boring "death by
Powerpoint" bullet-point slides, you will see the slides are not
entirely self-explanatory by themselves. However, both their email
addresses are given near the end of the talk, and they are both more
than happy to answer follow-up questions.&lt;/p&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//www.slideshare.net/slideshow/embed_code/key/JQdx4FrVorOzkW"&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><category term="Conference"></category></entry><entry><title>MySQL High Availability Sprint: Launch the Pacemaker!</title><link href="fghaas.github.io/xahteiwi.eu/resources/presentations/mysql-high-availability-sprint-launch-pacemaker/index.html" rel="alternate"></link><published>2011-11-01T13:45:00+00:00</published><updated>2011-11-01T13:45:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:None,2011-11-01:fghaas.github.io/xahteiwi.eu/resources/presentations/mysql-high-availability-sprint-launch-pacemaker/index.html</id><summary type="html">&lt;p&gt;This is a very dense tutorial given at Percona Live UK 2011 in London,
England. In three hours, Florian covered the MySQL HA Stack with
Pacemaker and DRBD, front to back.&lt;/p&gt;
&lt;!--break--&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//www.slideshare.net/slideshow/embed_code/key/sz9doig59uDdAC"&gt;&lt;/iframe&gt;
&lt;/div&gt;</summary><content type="html">&lt;p&gt;This is a very dense tutorial given at Percona Live UK 2011 in London,
England. In three hours, Florian covered the MySQL HA Stack with
Pacemaker and DRBD, front to back.&lt;/p&gt;
&lt;!--break--&gt;
&lt;div class="embed-responsive embed-responsive-4by3"&gt;
&lt;iframe class="embed-responsive-item" src="//www.slideshare.net/slideshow/embed_code/key/sz9doig59uDdAC"&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><category term="Conference"></category><category term="MySQL"></category><category term="Pacemaker"></category></entry></feed>